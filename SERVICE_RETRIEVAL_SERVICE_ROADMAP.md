# Retrieval Service æœåŠ¡åŠŸèƒ½æ¸…å•ä¸è¿­ä»£è®¡åˆ’

## æœåŠ¡æ¦‚è¿°

Retrieval Service æ˜¯æ··åˆæ£€ç´¢æœåŠ¡ï¼Œæä¾›å‘é‡æ£€ç´¢ã€BM25 æ£€ç´¢ã€æ··åˆæ£€ç´¢å’Œé‡æ’åºåŠŸèƒ½ã€‚

**æŠ€æœ¯æ ˆ**: FastAPI + Milvus + Elasticsearch + Sentence-Transformers

**ç«¯å£**: 8003

---

## ä¸€ã€åŠŸèƒ½å®Œæˆåº¦è¯„ä¼°

### âœ… å·²å®ŒæˆåŠŸèƒ½

#### 1. æ ¸å¿ƒæ£€ç´¢
- âœ… å‘é‡æ£€ç´¢ï¼ˆMilvusï¼‰
- âœ… BM25 æ£€ç´¢ï¼ˆç¤ºä¾‹ï¼‰
- âœ… æ··åˆæ£€ç´¢ï¼ˆRRF èåˆï¼‰
- âœ… åŸºç¡€é‡æ’åº

#### 2. API æ¥å£
- âœ… `/api/v1/retrieval/vector` - å‘é‡æ£€ç´¢
- âœ… `/api/v1/retrieval/bm25` - BM25 æ£€ç´¢
- âœ… `/api/v1/retrieval/hybrid` - æ··åˆæ£€ç´¢
- âœ… `/health` - å¥åº·æ£€æŸ¥

---

## äºŒã€å¾…å®ŒæˆåŠŸèƒ½æ¸…å•

### ğŸ”„ P0 - æ ¸å¿ƒåŠŸèƒ½ï¼ˆè¿­ä»£1ï¼š2å‘¨ï¼‰

#### 1. Elasticsearch é›†æˆ
**å½“å‰çŠ¶æ€**: æœªå®Œæ•´å®ç°

**å¾…å®ç°**:

```python
# æ–‡ä»¶: algo/retrieval-service/app/services/elasticsearch_service.py

from elasticsearch import AsyncElasticsearch
from typing import List, Dict

class ElasticsearchService:
    """Elasticsearch æœåŠ¡"""
    
    def __init__(self):
        self.client = AsyncElasticsearch(
            hosts=[{
                'host': os.getenv('ELASTICSEARCH_HOST', 'localhost'),
                'port': int(os.getenv('ELASTICSEARCH_PORT', 9200)),
                'scheme': 'http'
            }]
        )
        self.index_prefix = "voicehelper"
    
    async def create_index(
        self,
        tenant_id: str,
        kb_id: str
    ):
        """åˆ›å»ºç´¢å¼•"""
        index_name = self._get_index_name(tenant_id, kb_id)
        
        # å®šä¹‰æ˜ å°„
        mappings = {
            "properties": {
                "chunk_id": {"type": "keyword"},
                "document_id": {"type": "keyword"},
                "content": {
                    "type": "text",
                    "analyzer": "ik_max_word",  # ä¸­æ–‡åˆ†è¯
                    "search_analyzer": "ik_smart"
                },
                "title": {
                    "type": "text",
                    "analyzer": "ik_max_word"
                },
                "metadata": {"type": "object"},
                "created_at": {"type": "date"}
            }
        }
        
        # åˆ›å»ºç´¢å¼•
        await self.client.indices.create(
            index=index_name,
            body={"mappings": mappings}
        )
        
        logger.info(f"Created ES index: {index_name}")
    
    async def index_document(
        self,
        tenant_id: str,
        kb_id: str,
        chunk_id: str,
        document_id: str,
        content: str,
        metadata: Dict = None
    ):
        """ç´¢å¼•æ–‡æ¡£"""
        index_name = self._get_index_name(tenant_id, kb_id)
        
        doc = {
            "chunk_id": chunk_id,
            "document_id": document_id,
            "content": content,
            "metadata": metadata or {},
            "created_at": datetime.utcnow().isoformat()
        }
        
        await self.client.index(
            index=index_name,
            id=chunk_id,
            body=doc
        )
    
    async def search(
        self,
        tenant_id: str,
        kb_id: str,
        query: str,
        top_k: int = 50,
        filters: Dict = None
    ) -> List[Dict]:
        """BM25 æœç´¢"""
        index_name = self._get_index_name(tenant_id, kb_id)
        
        # æ„å»ºæŸ¥è¯¢
        must_clauses = [
            {
                "multi_match": {
                    "query": query,
                    "fields": ["content^2", "title^1.5"],
                    "type": "best_fields"
                }
            }
        ]
        
        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filters:
            for key, value in filters.items():
                must_clauses.append({
                    "term": {f"metadata.{key}": value}
                })
        
        # æ‰§è¡Œæœç´¢
        response = await self.client.search(
            index=index_name,
            body={
                "query": {
                    "bool": {
                        "must": must_clauses
                    }
                },
                "size": top_k,
                "_source": ["chunk_id", "document_id", "content", "metadata"]
            }
        )
        
        # æå–ç»“æœ
        results = []
        for hit in response['hits']['hits']:
            results.append({
                "chunk_id": hit['_source']['chunk_id'],
                "document_id": hit['_source']['document_id'],
                "content": hit['_source']['content'],
                "score": hit['_score'],
                "metadata": hit['_source'].get('metadata', {}),
                "source": "bm25"
            })
        
        return results
    
    async def bulk_index(
        self,
        tenant_id: str,
        kb_id: str,
        documents: List[Dict]
    ):
        """æ‰¹é‡ç´¢å¼•"""
        index_name = self._get_index_name(tenant_id, kb_id)
        
        # æ„å»ºæ‰¹é‡æ“ä½œ
        bulk_body = []
        for doc in documents:
            bulk_body.append({
                "index": {
                    "_index": index_name,
                    "_id": doc["chunk_id"]
                }
            })
            bulk_body.append({
                "chunk_id": doc["chunk_id"],
                "document_id": doc["document_id"],
                "content": doc["content"],
                "metadata": doc.get("metadata", {}),
                "created_at": datetime.utcnow().isoformat()
            })
        
        # æ‰§è¡Œæ‰¹é‡æ“ä½œ
        response = await self.client.bulk(body=bulk_body)
        
        logger.info(f"Bulk indexed {len(documents)} documents")
        
        return response
    
    def _get_index_name(self, tenant_id: str, kb_id: str) -> str:
        """ç”Ÿæˆç´¢å¼•åç§°"""
        return f"{self.index_prefix}_{tenant_id}_{kb_id}"
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] ES è¿æ¥æ­£å¸¸
- [ ] ç´¢å¼•åˆ›å»ºå’Œç®¡ç†
- [ ] BM25 æœç´¢æ­£å¸¸
- [ ] æ”¯æŒä¸­æ–‡åˆ†è¯

#### 2. å¤šæ¨¡æ€æ£€ç´¢
**å½“å‰çŠ¶æ€**: éƒ¨åˆ†å®ç°

**ä½ç½®**: `algo/retrieval-service/app/services/multimodal_retrieval_service.py`

**å¾…å®ç°**:

```python
# æ–‡ä»¶: algo/retrieval-service/app/services/multimodal_retrieval_service.py
# å½“å‰TODO: ç¬¬301ã€313è¡Œ - å®é™…çš„å›¾åƒ/æ–‡æœ¬ç¼–ç å™¨ï¼Œç¬¬383è¡Œ - å›¾åƒæè¿°æ¨¡å‹

from transformers import CLIPModel, CLIPProcessor
import torch

class MultimodalRetrievalService:
    """å¤šæ¨¡æ€æ£€ç´¢æœåŠ¡"""
    
    def __init__(self):
        # åŠ è½½ CLIP æ¨¡å‹
        self.model_name = "openai/clip-vit-base-patch32"
        self.model = CLIPModel.from_pretrained(self.model_name)
        self.processor = CLIPProcessor.from_pretrained(self.model_name)
        
        self.milvus_client = MilvusClient()
        self.collection_name = "multimodal_embeddings"
    
    async def encode_image(self, image_data: bytes) -> List[float]:
        """ç¼–ç å›¾åƒä¸ºå‘é‡"""
        # åŠ è½½å›¾åƒ
        from PIL import Image
        import io
        
        image = Image.open(io.BytesIO(image_data))
        
        # å¤„ç†å›¾åƒ
        inputs = self.processor(
            images=image,
            return_tensors="pt"
        )
        
        # ç”ŸæˆåµŒå…¥
        with torch.no_grad():
            image_features = self.model.get_image_features(**inputs)
        
        # å½’ä¸€åŒ–
        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        
        return image_features[0].tolist()
    
    async def encode_text(self, text: str) -> List[float]:
        """ç¼–ç æ–‡æœ¬ä¸ºå‘é‡"""
        # å¤„ç†æ–‡æœ¬
        inputs = self.processor(
            text=[text],
            return_tensors="pt",
            padding=True
        )
        
        # ç”ŸæˆåµŒå…¥
        with torch.no_grad():
            text_features = self.model.get_text_features(**inputs)
        
        # å½’ä¸€åŒ–
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        return text_features[0].tolist()
    
    async def search_by_image(
        self,
        image_data: bytes,
        tenant_id: str,
        kb_id: str,
        top_k: int = 10
    ) -> List[Dict]:
        """
        ä»¥å›¾æœå›¾/æ–‡
        """
        # ç¼–ç å›¾åƒ
        image_embedding = await self.encode_image(image_data)
        
        # å‘é‡æœç´¢
        results = self.milvus_client.search(
            collection_name=self.collection_name,
            data=[image_embedding],
            limit=top_k,
            filter=f"tenant_id == '{tenant_id}' && kb_id == '{kb_id}'",
            output_fields=["chunk_id", "content", "content_type", "metadata"]
        )
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        for hit in results[0]:
            formatted_results.append({
                "chunk_id": hit["chunk_id"],
                "content": hit["content"],
                "content_type": hit["content_type"],
                "score": hit["distance"],
                "metadata": hit.get("metadata", {})
            })
        
        return formatted_results
    
    async def search_by_text_for_images(
        self,
        query: str,
        tenant_id: str,
        kb_id: str,
        top_k: int = 10
    ) -> List[Dict]:
        """
        ä»¥æ–‡æœå›¾
        """
        # ç¼–ç æ–‡æœ¬
        text_embedding = await self.encode_text(query)
        
        # æœç´¢å›¾åƒ
        results = self.milvus_client.search(
            collection_name=self.collection_name,
            data=[text_embedding],
            limit=top_k,
            filter=f"tenant_id == '{tenant_id}' && kb_id == '{kb_id}' && content_type == 'image'",
            output_fields=["chunk_id", "image_url", "description", "metadata"]
        )
        
        # æ ¼å¼åŒ–
        formatted_results = []
        for hit in results[0]:
            formatted_results.append({
                "chunk_id": hit["chunk_id"],
                "image_url": hit["image_url"],
                "description": hit.get("description", ""),
                "score": hit["distance"],
                "metadata": hit.get("metadata", {})
            })
        
        return formatted_results
    
    async def index_multimodal_content(
        self,
        tenant_id: str,
        kb_id: str,
        content_id: str,
        content_type: str,  # 'text', 'image', 'mixed'
        text: Optional[str] = None,
        image_data: Optional[bytes] = None,
        metadata: Dict = None
    ):
        """
        ç´¢å¼•å¤šæ¨¡æ€å†…å®¹
        """
        embeddings = []
        
        # ç¼–ç å†…å®¹
        if text and content_type in ['text', 'mixed']:
            text_emb = await self.encode_text(text)
            embeddings.append(text_emb)
        
        if image_data and content_type in ['image', 'mixed']:
            image_emb = await self.encode_image(image_data)
            embeddings.append(image_emb)
        
        # æ··åˆæ¨¡æ€ï¼šå¹³å‡åµŒå…¥
        if len(embeddings) > 1:
            final_embedding = np.mean(embeddings, axis=0).tolist()
        else:
            final_embedding = embeddings[0]
        
        # æ’å…¥ Milvus
        data = [{
            "chunk_id": content_id,
            "tenant_id": tenant_id,
            "kb_id": kb_id,
            "embedding": final_embedding,
            "content_type": content_type,
            "content": text or "",
            "metadata": metadata or {}
        }]
        
        self.milvus_client.insert(
            collection_name=self.collection_name,
            data=data
        )
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] CLIP æ¨¡å‹é›†æˆ
- [ ] ä»¥å›¾æœå›¾åŠŸèƒ½
- [ ] ä»¥æ–‡æœå›¾åŠŸèƒ½
- [ ] æ··åˆæ¨¡æ€æ£€ç´¢

#### 3. ç´¢å¼•ä¼˜åŒ–å™¨
**å½“å‰çŠ¶æ€**: æœªå®ç°

**ä½ç½®**: `algo/retrieval-service/app/services/index_optimizer_service.py`

**å¾…å®ç°**:

```python
# æ–‡ä»¶: algo/retrieval-service/app/services/index_optimizer_service.py

class IndexOptimizerService:
    """ç´¢å¼•ä¼˜åŒ–æœåŠ¡"""
    
    def __init__(self):
        self.milvus_client = MilvusClient()
        self.redis_client = redis.Redis()
    
    async def optimize_index(
        self,
        tenant_id: str,
        kb_id: str
    ):
        """
        ä¼˜åŒ–ç´¢å¼•
        1. åˆ†ææŸ¥è¯¢æ¨¡å¼
        2. è°ƒæ•´ç´¢å¼•å‚æ•°
        3. é‡å»ºç´¢å¼•ï¼ˆå¦‚éœ€è¦ï¼‰
        """
        collection_name = f"kb_{tenant_id}_{kb_id}"
        
        # 1. åˆ†ææŸ¥è¯¢ç»Ÿè®¡
        stats = await self._analyze_query_patterns(tenant_id, kb_id)
        
        # 2. ç¡®å®šä¼˜åŒ–ç­–ç•¥
        if stats["avg_top_k"] > 100:
            # å¤§ top_k æŸ¥è¯¢å¤šï¼Œä¼˜åŒ–å¬å›
            index_params = {
                "metric_type": "IP",
                "index_type": "HNSW",
                "params": {"M": 32, "efConstruction": 512}  # æé«˜ç²¾åº¦
            }
        else:
            # å° top_k æŸ¥è¯¢å¤šï¼Œä¼˜åŒ–é€Ÿåº¦
            index_params = {
                "metric_type": "IP",
                "index_type": "IVF_FLAT",
                "params": {"nlist": 2048}  # æé«˜é€Ÿåº¦
            }
        
        # 3. é‡å»ºç´¢å¼•
        await self._rebuild_index(collection_name, index_params)
        
        logger.info(f"Optimized index for {collection_name}")
    
    async def _analyze_query_patterns(
        self,
        tenant_id: str,
        kb_id: str
    ) -> Dict:
        """åˆ†ææŸ¥è¯¢æ¨¡å¼"""
        # ä» Redis è·å–æŸ¥è¯¢ç»Ÿè®¡
        key = f"query_stats:{tenant_id}:{kb_id}"
        
        stats_data = self.redis_client.hgetall(key)
        
        return {
            "total_queries": int(stats_data.get(b"total", 0)),
            "avg_top_k": float(stats_data.get(b"avg_top_k", 10)),
            "avg_latency_ms": float(stats_data.get(b"avg_latency", 0)),
            "cache_hit_rate": float(stats_data.get(b"cache_hit_rate", 0))
        }
    
    async def _rebuild_index(
        self,
        collection_name: str,
        index_params: Dict
    ):
        """é‡å»ºç´¢å¼•"""
        # åˆ é™¤æ—§ç´¢å¼•
        self.milvus_client.drop_index(
            collection_name=collection_name,
            field_name="embedding"
        )
        
        # åˆ›å»ºæ–°ç´¢å¼•
        self.milvus_client.create_index(
            collection_name=collection_name,
            field_name="embedding",
            index_params=index_params
        )
        
        # åŠ è½½åˆ°å†…å­˜
        self.milvus_client.load_collection(collection_name)
    
    async def auto_optimize_all(self):
        """è‡ªåŠ¨ä¼˜åŒ–æ‰€æœ‰ç´¢å¼•ï¼ˆå®šæ—¶ä»»åŠ¡ï¼‰"""
        # è·å–æ‰€æœ‰é›†åˆ
        collections = self.milvus_client.list_collections()
        
        for collection in collections:
            if collection.startswith("kb_"):
                # è§£æ tenant_id å’Œ kb_id
                parts = collection.split("_")
                if len(parts) >= 3:
                    tenant_id = parts[1]
                    kb_id = parts[2]
                    
                    await self.optimize_index(tenant_id, kb_id)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æŸ¥è¯¢æ¨¡å¼åˆ†æ
- [ ] è‡ªåŠ¨ç´¢å¼•ä¼˜åŒ–
- [ ] æ€§èƒ½æå‡æ˜æ˜¾

---

### ğŸ”„ P1 - é«˜çº§åŠŸèƒ½ï¼ˆè¿­ä»£2ï¼š1å‘¨ï¼‰

#### 1. å›¾æ£€ç´¢å¢å¼º

```python
# æ–‡ä»¶: algo/retrieval-service/app/services/graph_retrieval_service.py

from neo4j import AsyncGraphDatabase

class GraphRetrievalService:
    """å›¾æ£€ç´¢æœåŠ¡"""
    
    def __init__(self):
        self.driver = AsyncGraphDatabase.driver(
            os.getenv("NEO4J_URI", "bolt://localhost:7687"),
            auth=(
                os.getenv("NEO4J_USER", "neo4j"),
                os.getenv("NEO4J_PASSWORD", "password")
            )
        )
    
    async def search_with_graph(
        self,
        query: str,
        tenant_id: str,
        kb_id: str,
        max_hops: int = 2
    ) -> List[Dict]:
        """
        åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢
        1. æå–æŸ¥è¯¢ä¸­çš„å®ä½“
        2. åœ¨å›¾ä¸­æŸ¥æ‰¾ç›¸å…³èŠ‚ç‚¹
        3. éå†å…³ç³»è·å–ä¸Šä¸‹æ–‡
        """
        # 1. æå–å®ä½“
        entities = await self._extract_entities(query)
        
        # 2. å›¾æŸ¥è¯¢
        async with self.driver.session() as session:
            results = await session.run("""
                MATCH (e:Entity)
                WHERE e.name IN $entities
                  AND e.tenant_id = $tenant_id
                  AND e.kb_id = $kb_id
                
                // æ‰©å±•å…³ç³»
                MATCH path = (e)-[*1..%d]-(related)
                
                RETURN e, related, relationships(path)
                LIMIT 20
            """ % max_hops, 
                entities=entities,
                tenant_id=tenant_id,
                kb_id=kb_id
            )
            
            # 3. èšåˆç»“æœ
            graph_results = await self._aggregate_graph_results(results)
        
        return graph_results
    
    async def hybrid_search_with_graph(
        self,
        query: str,
        tenant_id: str,
        kb_id: str,
        vector_results: List[Dict]
    ) -> List[Dict]:
        """
        æ··åˆæ£€ç´¢ï¼šå‘é‡ + å›¾
        """
        # è·å–å›¾æ£€ç´¢ç»“æœ
        graph_results = await self.search_with_graph(
            query, tenant_id, kb_id
        )
        
        # èåˆç»“æœ
        merged = self._merge_vector_and_graph(
            vector_results,
            graph_results
        )
        
        return merged
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] Neo4j é›†æˆ
- [ ] å›¾éå†æŸ¥è¯¢
- [ ] å‘é‡+å›¾æ··åˆæ£€ç´¢

#### 2. æ™ºèƒ½é‡æ’åº

```python
# æ–‡ä»¶: algo/retrieval-service/app/services/rerank_service.py

from sentence_transformers import CrossEncoder

class RerankService:
    """é‡æ’åºæœåŠ¡"""
    
    def __init__(self):
        # Cross-Encoder
        self.cross_encoder = CrossEncoder(
            'cross-encoder/ms-marco-MiniLM-L-12-v2'
        )
        
        # LLM é‡æ’åºï¼ˆå¯é€‰ï¼‰
        self.use_llm_rerank = os.getenv("USE_LLM_RERANK", "false") == "true"
    
    async def rerank(
        self,
        query: str,
        documents: List[Dict],
        top_k: int = 10,
        method: str = "cross-encoder"
    ) -> List[Dict]:
        """
        é‡æ’åºæ–‡æ¡£
        """
        if method == "cross-encoder":
            return await self._cross_encoder_rerank(query, documents, top_k)
        elif method == "llm":
            return await self._llm_rerank(query, documents, top_k)
        else:
            raise ValueError(f"Unknown rerank method: {method}")
    
    async def _cross_encoder_rerank(
        self,
        query: str,
        documents: List[Dict],
        top_k: int
    ) -> List[Dict]:
        """Cross-Encoder é‡æ’åº"""
        # å‡†å¤‡è¾“å…¥
        pairs = [
            [query, doc["content"]]
            for doc in documents
        ]
        
        # è®¡ç®—åˆ†æ•°
        scores = self.cross_encoder.predict(pairs)
        
        # æ’åº
        for i, doc in enumerate(documents):
            doc["rerank_score"] = float(scores[i])
        
        reranked = sorted(
            documents,
            key=lambda x: x["rerank_score"],
            reverse=True
        )
        
        return reranked[:top_k]
    
    async def _llm_rerank(
        self,
        query: str,
        documents: List[Dict],
        top_k: int
    ) -> List[Dict]:
        """LLM é‡æ’åº"""
        # æ„å»º prompt
        doc_list = "\n\n".join([
            f"[{i+1}] {doc['content'][:200]}..."
            for i, doc in enumerate(documents)
        ])
        
        prompt = f"""
        æŸ¥è¯¢: {query}
        
        æ–‡æ¡£åˆ—è¡¨:
        {doc_list}
        
        è¯·æ ¹æ®ç›¸å…³æ€§å¯¹æ–‡æ¡£è¿›è¡Œæ’åºï¼Œè¿”å›æ–‡æ¡£ç¼–å·åˆ—è¡¨ï¼ˆä»é«˜åˆ°ä½ï¼‰ã€‚
        åªè¿”å›ç¼–å·ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚: 3,1,5,2,4
        """
        
        # è°ƒç”¨ LLM
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{MODEL_ADAPTER_URL}/api/v1/chat/completions",
                json={
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.1
                }
            )
            result = response.json()
            
        # è§£ææ’åº
        order_str = result["choices"][0]["message"]["content"].strip()
        order = [int(x) - 1 for x in order_str.split(",")]
        
        # é‡æ’åº
        reranked = [documents[i] for i in order if i < len(documents)]
        
        return reranked[:top_k]
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] Cross-Encoder é‡æ’åº
- [ ] LLM é‡æ’åº
- [ ] æ’åºè´¨é‡æå‡

---

## ä¸‰ã€å®æ–½æ–¹æ¡ˆ

### é˜¶æ®µ1ï¼šæ ¸å¿ƒåŠŸèƒ½ï¼ˆWeek 1-2ï¼‰

#### Day 1-5: Elasticsearch é›†æˆ
1. ES è¿æ¥å’Œç´¢å¼•ç®¡ç†
2. BM25 æœç´¢å®ç°
3. ä¸­æ–‡åˆ†è¯é…ç½®
4. æµ‹è¯•

#### Day 6-10: å¤šæ¨¡æ€æ£€ç´¢
1. CLIP æ¨¡å‹é›†æˆ
2. å›¾åƒ/æ–‡æœ¬ç¼–ç 
3. è·¨æ¨¡æ€æœç´¢
4. æµ‹è¯•

#### Day 11-14: ç´¢å¼•ä¼˜åŒ–
1. æŸ¥è¯¢åˆ†æ
2. è‡ªåŠ¨ä¼˜åŒ–
3. æ€§èƒ½æµ‹è¯•

### é˜¶æ®µ2ï¼šé«˜çº§åŠŸèƒ½ï¼ˆWeek 3ï¼‰

#### Day 15-17: å›¾æ£€ç´¢
1. Neo4j é›†æˆ
2. å›¾éå†æŸ¥è¯¢
3. æ··åˆæ£€ç´¢

#### Day 18-21: æ™ºèƒ½é‡æ’åº
1. Cross-Encoder
2. LLM é‡æ’åº
3. è´¨é‡è¯„ä¼°

---

## å››ã€éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶
- [ ] ES é›†æˆå®Œæˆ
- [ ] å¤šæ¨¡æ€æ£€ç´¢æ­£å¸¸
- [ ] ç´¢å¼•ä¼˜åŒ–æœ‰æ•ˆ
- [ ] å›¾æ£€ç´¢æ­£å¸¸
- [ ] é‡æ’åºè´¨é‡é«˜

### æ€§èƒ½éªŒæ”¶
- [ ] å‘é‡æ£€ç´¢ < 50ms
- [ ] BM25 æ£€ç´¢ < 100ms
- [ ] æ··åˆæ£€ç´¢ < 150ms
- [ ] é‡æ’åº < 500ms

### è´¨é‡éªŒæ”¶
- [ ] Recall@10 > 0.9
- [ ] MRR > 0.7
- [ ] æ‰€æœ‰ TODO æ¸…ç†

---

## æ€»ç»“

Retrieval Service çš„ä¸»è¦å¾…å®ŒæˆåŠŸèƒ½ï¼š

1. **Elasticsearch é›†æˆ**: å®Œæ•´ BM25 æ£€ç´¢
2. **å¤šæ¨¡æ€æ£€ç´¢**: CLIP è·¨æ¨¡æ€æœç´¢
3. **ç´¢å¼•ä¼˜åŒ–**: è‡ªåŠ¨è°ƒä¼˜
4. **å›¾æ£€ç´¢**: çŸ¥è¯†å›¾è°±å¢å¼º
5. **æ™ºèƒ½é‡æ’åº**: Cross-Encoder å’Œ LLM

å®Œæˆåå°†æä¾›å¼ºå¤§çš„æ··åˆæ£€ç´¢èƒ½åŠ›ã€‚


