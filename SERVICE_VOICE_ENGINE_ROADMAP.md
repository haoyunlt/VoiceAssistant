# Voice Engine æœåŠ¡åŠŸèƒ½æ¸…å•ä¸è¿­ä»£è®¡åˆ’

## æœåŠ¡æ¦‚è¿°

Voice Engine æ˜¯è¯­éŸ³å¼•æ“æœåŠ¡ï¼Œæä¾› ASRï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰ã€TTSï¼ˆæ–‡æœ¬è½¬è¯­éŸ³ï¼‰ã€VADï¼ˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼‰åŠŸèƒ½ã€‚

**æŠ€æœ¯æ ˆ**: FastAPI + Whisper + Edge TTS + Silero VAD + Python 3.11+

**ç«¯å£**: 8004

---

## ä¸€ã€åŠŸèƒ½å®Œæˆåº¦è¯„ä¼°

### âœ… å·²å®ŒæˆåŠŸèƒ½

#### 1. ASR (è¯­éŸ³è¯†åˆ«)
- âœ… åŸºäº Faster-Whisper çš„æ‰¹é‡è¯†åˆ«
- âœ… æ–‡ä»¶ä¸Šä¼ è¯†åˆ«
- âœ… å¤šè¯­è¨€æ”¯æŒï¼ˆä¸­æ–‡ã€è‹±æ–‡ï¼‰
- âœ… VAD é›†æˆï¼ˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼‰
- âœ… ç½®ä¿¡åº¦è¯„åˆ†
- âœ… æ—¶é—´æˆ³å’Œåˆ†æ®µè¾“å‡º

#### 2. TTS (æ–‡æœ¬è½¬è¯­éŸ³)
- âœ… åŸºäº Edge TTS çš„è¯­éŸ³åˆæˆ
- âœ… æ‰¹é‡åˆæˆ
- âœ… æµå¼åˆæˆ
- âœ… å¤šéŸ³è‰²æ”¯æŒ
- âœ… éŸ³è‰²åˆ—è¡¨æŸ¥è¯¢
- âœ… é€Ÿåº¦å’ŒéŸ³è°ƒè°ƒèŠ‚
- âœ… TTS ç»“æœç¼“å­˜

#### 3. VAD (è¯­éŸ³æ´»åŠ¨æ£€æµ‹)
- âœ… åŸºäº Silero VAD çš„æ£€æµ‹
- âœ… æ‰¹é‡æ£€æµ‹
- âœ… æ–‡ä»¶ä¸Šä¼ æ£€æµ‹
- âœ… å¯é…ç½®é˜ˆå€¼
- âœ… è¯­éŸ³æ®µæ—¶é—´æˆ³

#### 4. API æ¥å£
- âœ… `/api/v1/asr/recognize` - ASR æ‰¹é‡è¯†åˆ«
- âœ… `/api/v1/asr/recognize/upload` - ASR æ–‡ä»¶ä¸Šä¼ 
- âœ… `/api/v1/tts/synthesize` - TTS æ‰¹é‡åˆæˆ
- âœ… `/api/v1/tts/synthesize/stream` - TTS æµå¼åˆæˆ
- âœ… `/api/v1/tts/voices` - éŸ³è‰²åˆ—è¡¨
- âœ… `/api/v1/vad/detect` - VAD æ£€æµ‹
- âœ… `/api/v1/vad/detect/upload` - VAD æ–‡ä»¶ä¸Šä¼ 
- âœ… `/health` - å¥åº·æ£€æŸ¥

#### 5. åŸºç¡€è®¾æ–½
- âœ… FastAPI åº”ç”¨æ¡†æ¶
- âœ… Pydantic æ•°æ®æ¨¡å‹
- âœ… æ—¥å¿—é…ç½®
- âœ… é…ç½®ç®¡ç†
- âœ… Docker æ”¯æŒ

---

## äºŒã€å¾…å®ŒæˆåŠŸèƒ½æ¸…å•

### ğŸ”„ P0 - æ ¸å¿ƒåŠŸèƒ½ï¼ˆè¿­ä»£1ï¼š2å‘¨ï¼‰

#### 1. ASR æµå¼è¯†åˆ«
**å½“å‰çŠ¶æ€**: æœªå®ç°

**ä½ç½®**: `algo/voice-engine/app/routers/asr.py`

**å¾…å®ç°**:

```python
# æ–‡ä»¶: algo/voice-engine/app/routers/asr.py
# å½“å‰TODO: ç¬¬98è¡Œ - å®ç°æµå¼è¯†åˆ«

@router.websocket("/api/v1/asr/recognize/stream")
async def recognize_stream(websocket: WebSocket):
    """
    WebSocket æµå¼è¯­éŸ³è¯†åˆ«
    å®¢æˆ·ç«¯æŒç»­å‘é€éŸ³é¢‘æ•°æ®ï¼ŒæœåŠ¡ç«¯å®æ—¶è¿”å›è¯†åˆ«ç»“æœ
    """
    await websocket.accept()
    
    try:
        # åˆå§‹åŒ–æµå¼è¯†åˆ«å™¨
        streaming_asr = StreamingASR(
            model=app.state.whisper_model,
            vad_model=app.state.vad_model
        )
        
        # éŸ³é¢‘ç¼“å†²åŒº
        audio_buffer = bytearray()
        
        while True:
            # æ¥æ”¶éŸ³é¢‘æ•°æ®
            message = await websocket.receive()
            
            if message["type"] == "websocket.disconnect":
                break
            
            # è§£ææ¶ˆæ¯
            data = json.loads(message["text"]) if "text" in message else message
            
            if data.get("type") == "audio":
                # æ·»åŠ åˆ°ç¼“å†²åŒº
                audio_chunk = base64.b64decode(data["audio"])
                audio_buffer.extend(audio_chunk)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿæ•°æ®è¿›è¡Œè¯†åˆ«
                if len(audio_buffer) >= streaming_asr.min_chunk_size:
                    # æ‰§è¡Œ VAD
                    speech_segments = streaming_asr.detect_speech(
                        bytes(audio_buffer)
                    )
                    
                    # å¯¹è¯­éŸ³æ®µè¿›è¡Œè¯†åˆ«
                    for segment in speech_segments:
                        result = await streaming_asr.recognize_segment(segment)
                        
                        # å‘é€éƒ¨åˆ†ç»“æœ
                        await websocket.send_json({
                            "type": "partial",
                            "text": result["text"],
                            "confidence": result["confidence"],
                            "is_final": False
                        })
                    
                    # æ¸…ç©ºå·²å¤„ç†çš„éƒ¨åˆ†
                    audio_buffer = bytearray()
            
            elif data.get("type") == "end":
                # å¤„ç†å‰©ä½™ç¼“å†²åŒº
                if len(audio_buffer) > 0:
                    final_result = await streaming_asr.recognize_segment(
                        bytes(audio_buffer)
                    )
                    
                    await websocket.send_json({
                        "type": "final",
                        "text": final_result["text"],
                        "confidence": final_result["confidence"],
                        "is_final": True
                    })
                
                break
    
    except WebSocketDisconnect:
        logger.info("Client disconnected")
    except Exception as e:
        logger.error(f"Streaming ASR error: {e}")
        await websocket.send_json({
            "type": "error",
            "message": str(e)
        })
    finally:
        await websocket.close()


class StreamingASR:
    """æµå¼ ASR è¯†åˆ«å™¨"""
    
    def __init__(self, model, vad_model):
        self.model = model
        self.vad_model = vad_model
        self.min_chunk_size = 16000 * 2  # 2ç§’éŸ³é¢‘ (16kHz)
        self.sample_rate = 16000
    
    def detect_speech(self, audio_bytes: bytes) -> List[AudioSegment]:
        """ä½¿ç”¨ VAD æ£€æµ‹è¯­éŸ³æ®µ"""
        # è½¬æ¢ä¸º numpy æ•°ç»„
        audio_np = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0
        
        # VAD æ£€æµ‹
        speech_timestamps = self.vad_model(
            audio_np,
            self.sample_rate
        )
        
        # æå–è¯­éŸ³æ®µ
        segments = []
        for ts in speech_timestamps:
            start_sample = int(ts['start'] * self.sample_rate)
            end_sample = int(ts['end'] * self.sample_rate)
            segment_audio = audio_np[start_sample:end_sample]
            
            segments.append(AudioSegment(
                audio=segment_audio,
                start_time=ts['start'],
                end_time=ts['end']
            ))
        
        return segments
    
    async def recognize_segment(self, segment: AudioSegment) -> dict:
        """è¯†åˆ«å•ä¸ªè¯­éŸ³æ®µ"""
        # ä½¿ç”¨ Whisper è¯†åˆ«
        segments, info = self.model.transcribe(
            segment.audio,
            language="zh",
            beam_size=5
        )
        
        # æå–ç»“æœ
        text = ""
        for seg in segments:
            text += seg.text
        
        return {
            "text": text.strip(),
            "confidence": getattr(info, 'avg_logprob', 0.0),
            "language": info.language
        }
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ”¯æŒ WebSocket è¿æ¥
- [ ] å®æ—¶æ¥æ”¶éŸ³é¢‘æµ
- [ ] å¢é‡è¿”å›è¯†åˆ«ç»“æœ
- [ ] VAD å®æ—¶æ£€æµ‹
- [ ] å»¶è¿Ÿ < 500ms

#### 2. Azure Speech SDK é›†æˆ
**å½“å‰çŠ¶æ€**: æœªå®ç°

**ä½ç½®**: `algo/voice-engine/app/services/asr_service.py`, `tts_service.py`, `azure_speech_service.py`

**å¾…å®ç°**:

```python
# æ–‡ä»¶: algo/voice-engine/app/services/azure_speech_service.py

import azure.cognitiveservices.speech as speechsdk
from typing import Optional

class AzureSpeechService:
    """Azure Speech SDK æœåŠ¡"""
    
    def __init__(self):
        self.speech_key = os.getenv("AZURE_SPEECH_KEY")
        self.speech_region = os.getenv("AZURE_SPEECH_REGION")
        
        if not self.speech_key or not self.speech_region:
            raise ValueError("Azure Speech credentials not configured")
        
        self.speech_config = speechsdk.SpeechConfig(
            subscription=self.speech_key,
            region=self.speech_region
        )
    
    async def recognize_from_audio(
        self,
        audio_data: bytes,
        language: str = "zh-CN"
    ) -> dict:
        """
        ä½¿ç”¨ Azure Speech SDK è¿›è¡Œ ASR
        """
        # é…ç½®è¯†åˆ«å™¨
        self.speech_config.speech_recognition_language = language
        
        # åˆ›å»ºéŸ³é¢‘æµ
        audio_stream = speechsdk.audio.PushAudioInputStream()
        audio_config = speechsdk.audio.AudioConfig(stream=audio_stream)
        
        # åˆ›å»ºè¯†åˆ«å™¨
        speech_recognizer = speechsdk.SpeechRecognizer(
            speech_config=self.speech_config,
            audio_config=audio_config
        )
        
        # å†™å…¥éŸ³é¢‘æ•°æ®
        audio_stream.write(audio_data)
        audio_stream.close()
        
        # æ‰§è¡Œè¯†åˆ«
        result = speech_recognizer.recognize_once()
        
        # å¤„ç†ç»“æœ
        if result.reason == speechsdk.ResultReason.RecognizedSpeech:
            return {
                "text": result.text,
                "confidence": result.confidence,
                "language": language
            }
        elif result.reason == speechsdk.ResultReason.NoMatch:
            return {
                "text": "",
                "confidence": 0.0,
                "error": "No speech recognized"
            }
        else:
            return {
                "text": "",
                "confidence": 0.0,
                "error": f"Recognition failed: {result.reason}"
            }
    
    async def synthesize_to_audio(
        self,
        text: str,
        voice: str = "zh-CN-XiaoxiaoNeural",
        rate: str = "+0%",
        pitch: str = "+0Hz"
    ) -> bytes:
        """
        ä½¿ç”¨ Azure Speech SDK è¿›è¡Œ TTS
        """
        # é…ç½®è¯­éŸ³
        self.speech_config.speech_synthesis_voice_name = voice
        
        # æ„å»º SSML
        ssml = f"""
        <speak version='1.0' xmlns='http://www.w3.org/2001/10/synthesis' xml:lang='zh-CN'>
            <voice name='{voice}'>
                <prosody rate='{rate}' pitch='{pitch}'>
                    {text}
                </prosody>
            </voice>
        </speak>
        """
        
        # åˆ›å»ºåˆæˆå™¨ï¼ˆè¾“å‡ºåˆ°å†…å­˜ï¼‰
        audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=False)
        speech_synthesizer = speechsdk.SpeechSynthesizer(
            speech_config=self.speech_config,
            audio_config=None  # ä½¿ç”¨å†…å­˜æµ
        )
        
        # æ‰§è¡Œåˆæˆ
        result = speech_synthesizer.speak_ssml_async(ssml).get()
        
        if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
            return result.audio_data
        else:
            raise Exception(f"TTS failed: {result.reason}")
    
    async def continuous_recognition(
        self,
        websocket: WebSocket,
        language: str = "zh-CN"
    ):
        """
        è¿ç»­è¯†åˆ«ï¼ˆç”¨äº WebSocket æµå¼ï¼‰
        """
        self.speech_config.speech_recognition_language = language
        
        # åˆ›å»ºæ¨é€æµ
        push_stream = speechsdk.audio.PushAudioInputStream()
        audio_config = speechsdk.audio.AudioConfig(stream=push_stream)
        
        # åˆ›å»ºè¯†åˆ«å™¨
        speech_recognizer = speechsdk.SpeechRecognizer(
            speech_config=self.speech_config,
            audio_config=audio_config
        )
        
        # è®¾ç½®äº‹ä»¶å¤„ç†å™¨
        def recognizing_cb(evt):
            """éƒ¨åˆ†ç»“æœ"""
            asyncio.create_task(
                websocket.send_json({
                    "type": "partial",
                    "text": evt.result.text,
                    "confidence": 0.0
                })
            )
        
        def recognized_cb(evt):
            """æœ€ç»ˆç»“æœ"""
            asyncio.create_task(
                websocket.send_json({
                    "type": "final",
                    "text": evt.result.text,
                    "confidence": evt.result.confidence
                })
            )
        
        # è¿æ¥äº‹ä»¶
        speech_recognizer.recognizing.connect(recognizing_cb)
        speech_recognizer.recognized.connect(recognized_cb)
        
        # å¼€å§‹è¿ç»­è¯†åˆ«
        speech_recognizer.start_continuous_recognition()
        
        try:
            while True:
                # æ¥æ”¶éŸ³é¢‘æ•°æ®
                message = await websocket.receive()
                
                if message["type"] == "websocket.disconnect":
                    break
                
                data = json.loads(message["text"])
                
                if data.get("type") == "audio":
                    audio_chunk = base64.b64decode(data["audio"])
                    push_stream.write(audio_chunk)
                
                elif data.get("type") == "end":
                    break
        
        finally:
            # åœæ­¢è¯†åˆ«
            speech_recognizer.stop_continuous_recognition()
            push_stream.close()
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] Azure ASR é›†æˆå®Œæˆ
- [ ] Azure TTS é›†æˆå®Œæˆ
- [ ] æ”¯æŒè¿ç»­è¯†åˆ«
- [ ] é…ç½®åˆ‡æ¢ï¼ˆWhisper/Azureï¼‰

#### 3. å…¨åŒå·¥è¯­éŸ³äº¤äº’
**å½“å‰çŠ¶æ€**: éƒ¨åˆ†å®ç°ï¼Œéœ€å®Œå–„

**ä½ç½®**: `algo/voice-engine/app/core/full_duplex_engine.py`

**å¾…å®ç°**:

```python
# æ–‡ä»¶: algo/voice-engine/app/core/full_duplex_engine.py
# å½“å‰TODO: ç¬¬205è¡Œ - å®é™…å®ç°ä¸­éœ€è¦åœæ­¢éŸ³é¢‘æ’­æ”¾

class FullDuplexEngine:
    """
    å…¨åŒå·¥è¯­éŸ³äº¤äº’å¼•æ“
    æ”¯æŒåŒæ—¶å½•éŸ³å’Œæ’­æ”¾ï¼Œå®ç°æ‰“æ–­åŠŸèƒ½
    """
    
    def __init__(self):
        self.asr_service = ASRService()
        self.tts_service = TTSService()
        self.vad_service = VADService()
        
        self.is_playing = False
        self.audio_player = None
    
    async def start_session(self, websocket: WebSocket):
        """å¯åŠ¨å…¨åŒå·¥ä¼šè¯"""
        await websocket.accept()
        
        # åˆ›å»ºä»»åŠ¡
        asr_task = asyncio.create_task(self._asr_loop(websocket))
        tts_task = asyncio.create_task(self._tts_loop(websocket))
        vad_task = asyncio.create_task(self._vad_loop(websocket))
        
        try:
            # å¹¶å‘è¿è¡Œ
            await asyncio.gather(asr_task, tts_task, vad_task)
        except Exception as e:
            logger.error(f"Full duplex error: {e}")
        finally:
            await self._cleanup()
    
    async def _asr_loop(self, websocket: WebSocket):
        """ASR å¾ªç¯"""
        audio_buffer = bytearray()
        
        while True:
            message = await websocket.receive()
            
            if message["type"] == "websocket.disconnect":
                break
            
            data = json.loads(message["text"])
            
            if data.get("type") == "audio":
                # æ·»åŠ åˆ°ç¼“å†²åŒº
                audio_chunk = base64.b64decode(data["audio"])
                audio_buffer.extend(audio_chunk)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´è¯­éŸ³
                if len(audio_buffer) >= 16000 * 2:  # 2ç§’
                    # VAD æ£€æµ‹
                    has_speech = await self.vad_service.detect(
                        bytes(audio_buffer)
                    )
                    
                    if has_speech:
                        # å¦‚æœæ­£åœ¨æ’­æ”¾ï¼Œåœæ­¢æ’­æ”¾ï¼ˆæ‰“æ–­ï¼‰
                        if self.is_playing:
                            await self._stop_playback()
                        
                        # ASR è¯†åˆ«
                        result = await self.asr_service.recognize(
                            bytes(audio_buffer)
                        )
                        
                        # å‘é€è¯†åˆ«ç»“æœ
                        await websocket.send_json({
                            "type": "asr_result",
                            "text": result["text"],
                            "confidence": result["confidence"]
                        })
                        
                        # æ¸…ç©ºç¼“å†²åŒº
                        audio_buffer = bytearray()
    
    async def _tts_loop(self, websocket: WebSocket):
        """TTS å¾ªç¯"""
        while True:
            # ç­‰å¾… TTS è¯·æ±‚
            message = await websocket.receive()
            
            if message["type"] == "websocket.disconnect":
                break
            
            data = json.loads(message["text"])
            
            if data.get("type") == "tts_request":
                # åˆæˆè¯­éŸ³
                audio = await self.tts_service.synthesize(
                    text=data["text"],
                    voice=data.get("voice", "zh-CN-XiaoxiaoNeural")
                )
                
                # æµå¼æ’­æ”¾
                await self._stream_audio(websocket, audio)
    
    async def _stream_audio(self, websocket: WebSocket, audio: bytes):
        """æµå¼æ’­æ”¾éŸ³é¢‘"""
        self.is_playing = True
        
        # åˆ†å—å‘é€
        chunk_size = 4096
        for i in range(0, len(audio), chunk_size):
            chunk = audio[i:i+chunk_size]
            
            await websocket.send_json({
                "type": "audio_chunk",
                "audio": base64.b64encode(chunk).decode(),
                "is_last": i + chunk_size >= len(audio)
            })
            
            # å°å»¶è¿Ÿï¼Œé¿å…è¿‡å¿«
            await asyncio.sleep(0.01)
        
        self.is_playing = False
    
    async def _stop_playback(self):
        """åœæ­¢éŸ³é¢‘æ’­æ”¾"""
        self.is_playing = False
        logger.info("Playback interrupted by user speech")
    
    async def _vad_loop(self, websocket: WebSocket):
        """VAD ç›‘æ§å¾ªç¯"""
        # ç›‘æ§è¯­éŸ³æ´»åŠ¨ï¼Œç”¨äºæ‰“æ–­æ£€æµ‹
        pass
    
    async def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.is_playing = False
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ”¯æŒåŒæ—¶å½•éŸ³å’Œæ’­æ”¾
- [ ] ç”¨æˆ·è¯´è¯æ—¶è‡ªåŠ¨åœæ­¢æ’­æ”¾
- [ ] ä½å»¶è¿Ÿï¼ˆ< 300msï¼‰
- [ ] ç¨³å®šçš„ WebSocket è¿æ¥

#### 4. è¯­éŸ³å…‹éš†åŠŸèƒ½
**å½“å‰çŠ¶æ€**: æœªå®ç°

**ä½ç½®**: `algo/voice-engine/app/services/voice_cloning_service.py`

**å¾…å®ç°**:

```python
# æ–‡ä»¶: algo/voice-engine/app/services/voice_cloning_service.py
# å½“å‰TODO: ç¬¬254è¡Œ - å®é™…çš„æ¨¡å‹è®­ç»ƒæµç¨‹ï¼Œç¬¬311è¡Œ - ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åˆæˆ

import torch
from TTS.api import TTS

class VoiceCloningService:
    """è¯­éŸ³å…‹éš†æœåŠ¡"""
    
    def __init__(self):
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self.model_name = "tts_models/multilingual/multi-dataset/your_tts"
        self.tts = TTS(model_name=self.model_name, gpu=False)
        
        self.cloned_voices = {}
    
    async def clone_voice(
        self,
        voice_id: str,
        audio_samples: List[bytes],
        text_samples: List[str]
    ) -> dict:
        """
        å…‹éš†å£°éŸ³
        
        Args:
            voice_id: å£°éŸ³ID
            audio_samples: éŸ³é¢‘æ ·æœ¬åˆ—è¡¨ï¼ˆè‡³å°‘3ä¸ªï¼‰
            text_samples: å¯¹åº”çš„æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å…‹éš†ç»“æœ
        """
        if len(audio_samples) < 3:
            raise ValueError("è‡³å°‘éœ€è¦3ä¸ªéŸ³é¢‘æ ·æœ¬")
        
        if len(audio_samples) != len(text_samples):
            raise ValueError("éŸ³é¢‘å’Œæ–‡æœ¬æ•°é‡å¿…é¡»ç›¸åŒ")
        
        # 1. ä¿å­˜éŸ³é¢‘æ ·æœ¬
        sample_paths = []
        for i, audio_data in enumerate(audio_samples):
            path = f"/tmp/voice_clone_{voice_id}_{i}.wav"
            with open(path, 'wb') as f:
                f.write(audio_data)
            sample_paths.append(path)
        
        # 2. æå–è¯´è¯äººåµŒå…¥ï¼ˆspeaker embeddingï¼‰
        try:
            # ä½¿ç”¨ YourTTS æå– speaker embedding
            speaker_embedding = self._extract_speaker_embedding(
                sample_paths[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬
            )
            
            # 3. å­˜å‚¨å£°éŸ³é…ç½®
            self.cloned_voices[voice_id] = {
                "speaker_embedding": speaker_embedding,
                "sample_paths": sample_paths,
                "created_at": datetime.utcnow()
            }
            
            return {
                "voice_id": voice_id,
                "status": "success",
                "sample_count": len(audio_samples)
            }
        
        except Exception as e:
            logger.error(f"Voice cloning failed: {e}")
            raise
    
    def _extract_speaker_embedding(self, audio_path: str) -> torch.Tensor:
        """æå–è¯´è¯äººåµŒå…¥"""
        # ä½¿ç”¨ TTS æ¨¡å‹æå–
        embedding = self.tts.synthesizer.tts_model.speaker_manager.encoder.compute_embedding(
            audio_path
        )
        return embedding
    
    async def synthesize_with_cloned_voice(
        self,
        voice_id: str,
        text: str,
        language: str = "zh-cn"
    ) -> bytes:
        """
        ä½¿ç”¨å…‹éš†çš„å£°éŸ³åˆæˆè¯­éŸ³
        
        Args:
            voice_id: å£°éŸ³ID
            text: è¦åˆæˆçš„æ–‡æœ¬
            language: è¯­è¨€
            
        Returns:
            éŸ³é¢‘æ•°æ®
        """
        if voice_id not in self.cloned_voices:
            raise ValueError(f"Voice {voice_id} not found")
        
        voice_data = self.cloned_voices[voice_id]
        speaker_embedding = voice_data["speaker_embedding"]
        
        # ä½¿ç”¨å…‹éš†çš„å£°éŸ³åˆæˆ
        output_path = f"/tmp/synthesized_{voice_id}_{int(time.time())}.wav"
        
        self.tts.tts_to_file(
            text=text,
            speaker_wav=voice_data["sample_paths"][0],  # å‚è€ƒéŸ³é¢‘
            language=language,
            file_path=output_path
        )
        
        # è¯»å–åˆæˆçš„éŸ³é¢‘
        with open(output_path, 'rb') as f:
            audio_data = f.read()
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(output_path)
        
        return audio_data
    
    async def list_cloned_voices(self) -> List[dict]:
        """åˆ—å‡ºæ‰€æœ‰å…‹éš†çš„å£°éŸ³"""
        return [
            {
                "voice_id": voice_id,
                "sample_count": len(data["sample_paths"]),
                "created_at": data["created_at"].isoformat()
            }
            for voice_id, data in self.cloned_voices.items()
        ]
    
    async def delete_cloned_voice(self, voice_id: str):
        """åˆ é™¤å…‹éš†çš„å£°éŸ³"""
        if voice_id in self.cloned_voices:
            # åˆ é™¤éŸ³é¢‘æ–‡ä»¶
            for path in self.cloned_voices[voice_id]["sample_paths"]:
                if os.path.exists(path):
                    os.remove(path)
            
            del self.cloned_voices[voice_id]
```

**API ç«¯ç‚¹**:
```python
# æ–‡ä»¶: algo/voice-engine/app/routers/voice_cloning.py

@router.post("/api/v1/voice/clone")
async def clone_voice(
    voice_id: str = Form(...),
    audio_files: List[UploadFile] = File(...),
    texts: List[str] = Form(...)
):
    """å…‹éš†å£°éŸ³"""
    # è¯»å–éŸ³é¢‘æ–‡ä»¶
    audio_samples = []
    for file in audio_files:
        audio_data = await file.read()
        audio_samples.append(audio_data)
    
    # å…‹éš†
    result = await voice_cloning_service.clone_voice(
        voice_id=voice_id,
        audio_samples=audio_samples,
        text_samples=texts
    )
    
    return result

@router.post("/api/v1/voice/synthesize/cloned")
async def synthesize_with_cloned_voice(request: ClonedVoiceSynthesizeRequest):
    """ä½¿ç”¨å…‹éš†çš„å£°éŸ³åˆæˆ"""
    audio = await voice_cloning_service.synthesize_with_cloned_voice(
        voice_id=request.voice_id,
        text=request.text,
        language=request.language
    )
    
    return Response(
        content=audio,
        media_type="audio/wav"
    )

@router.get("/api/v1/voice/cloned/list")
async def list_cloned_voices():
    """åˆ—å‡ºå…‹éš†çš„å£°éŸ³"""
    voices = await voice_cloning_service.list_cloned_voices()
    return {"voices": voices}
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ”¯æŒå£°éŸ³å…‹éš†
- [ ] è‡³å°‘3ä¸ªæ ·æœ¬å³å¯å…‹éš†
- [ ] åˆæˆè´¨é‡é«˜
- [ ] æä¾›å®Œæ•´ API

#### 5. éŸ³é¢‘å¤„ç†å¢å¼º
**å½“å‰çŠ¶æ€**: æœªå®ç°

**ä½ç½®**: `algo/voice-engine/main.py`

**å¾…å®ç°**:

```python
# æ–‡ä»¶: algo/voice-engine/app/services/audio_processing_service.py

import noisereduce as nr
import librosa
import soundfile as sf

class AudioProcessingService:
    """éŸ³é¢‘å¤„ç†æœåŠ¡"""
    
    def __init__(self):
        self.sample_rate = 16000
    
    async def denoise(
        self,
        audio_data: bytes,
        noise_profile: Optional[bytes] = None
    ) -> bytes:
        """
        é™å™ªå¤„ç†
        
        Args:
            audio_data: åŸå§‹éŸ³é¢‘æ•°æ®
            noise_profile: å™ªéŸ³æ ·æœ¬ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            é™å™ªåçš„éŸ³é¢‘æ•°æ®
        """
        # è½¬æ¢ä¸º numpy æ•°ç»„
        audio_np, sr = self._bytes_to_numpy(audio_data)
        
        # æ‰§è¡Œé™å™ª
        if noise_profile:
            noise_np, _ = self._bytes_to_numpy(noise_profile)
            reduced_noise = nr.reduce_noise(
                y=audio_np,
                sr=sr,
                y_noise=noise_np,
                prop_decrease=1.0
            )
        else:
            # æ— ç›‘ç£é™å™ª
            reduced_noise = nr.reduce_noise(
                y=audio_np,
                sr=sr,
                stationary=True
            )
        
        # è½¬æ¢å› bytes
        return self._numpy_to_bytes(reduced_noise, sr)
    
    async def enhance(
        self,
        audio_data: bytes,
        enhancements: List[str]
    ) -> bytes:
        """
        éŸ³é¢‘å¢å¼º
        
        Args:
            audio_data: åŸå§‹éŸ³é¢‘
            enhancements: å¢å¼ºåˆ—è¡¨ ['normalize', 'equalize', 'compress']
            
        Returns:
            å¢å¼ºåçš„éŸ³é¢‘
        """
        audio_np, sr = self._bytes_to_numpy(audio_data)
        
        for enhancement in enhancements:
            if enhancement == 'normalize':
                # å½’ä¸€åŒ–
                audio_np = librosa.util.normalize(audio_np)
            
            elif enhancement == 'equalize':
                # å‡è¡¡åŒ–
                audio_np = self._equalize(audio_np, sr)
            
            elif enhancement == 'compress':
                # åŠ¨æ€èŒƒå›´å‹ç¼©
                audio_np = self._compress(audio_np)
        
        return self._numpy_to_bytes(audio_np, sr)
    
    async def change_speed(
        self,
        audio_data: bytes,
        rate: float
    ) -> bytes:
        """
        æ”¹å˜è¯­é€Ÿï¼ˆä¸æ”¹å˜éŸ³è°ƒï¼‰
        
        Args:
            audio_data: åŸå§‹éŸ³é¢‘
            rate: é€Ÿç‡ (1.0 = åŸé€Ÿ, 1.5 = 1.5å€é€Ÿ)
            
        Returns:
            è°ƒæ•´åçš„éŸ³é¢‘
        """
        audio_np, sr = self._bytes_to_numpy(audio_data)
        
        # æ—¶é—´æ‹‰ä¼¸
        stretched = librosa.effects.time_stretch(audio_np, rate=rate)
        
        return self._numpy_to_bytes(stretched, sr)
    
    async def change_pitch(
        self,
        audio_data: bytes,
        semitones: int
    ) -> bytes:
        """
        æ”¹å˜éŸ³è°ƒï¼ˆä¸æ”¹å˜è¯­é€Ÿï¼‰
        
        Args:
            audio_data: åŸå§‹éŸ³é¢‘
            semitones: åŠéŸ³æ•° (12 = å‡é«˜1ä¸ªå…«åº¦, -12 = é™ä½1ä¸ªå…«åº¦)
            
        Returns:
            è°ƒæ•´åçš„éŸ³é¢‘
        """
        audio_np, sr = self._bytes_to_numpy(audio_data)
        
        # éŸ³è°ƒå˜æ¢
        shifted = librosa.effects.pitch_shift(
            audio_np,
            sr=sr,
            n_steps=semitones
        )
        
        return self._numpy_to_bytes(shifted, sr)
    
    async def mix_audio(
        self,
        audio_list: List[bytes],
        volumes: List[float]
    ) -> bytes:
        """
        æ··éŸ³
        
        Args:
            audio_list: éŸ³é¢‘åˆ—è¡¨
            volumes: å¯¹åº”çš„éŸ³é‡ (0.0-1.0)
            
        Returns:
            æ··éŸ³åçš„éŸ³é¢‘
        """
        if len(audio_list) != len(volumes):
            raise ValueError("Audio count must match volume count")
        
        # è½¬æ¢æ‰€æœ‰éŸ³é¢‘
        audio_arrays = []
        max_length = 0
        
        for audio_data in audio_list:
            audio_np, sr = self._bytes_to_numpy(audio_data)
            audio_arrays.append(audio_np)
            max_length = max(max_length, len(audio_np))
        
        # å¡«å……åˆ°ç›¸åŒé•¿åº¦å¹¶æ··éŸ³
        mixed = np.zeros(max_length)
        
        for audio_np, volume in zip(audio_arrays, volumes):
            # å¡«å……
            padded = np.pad(audio_np, (0, max_length - len(audio_np)))
            # æ··éŸ³
            mixed += padded * volume
        
        # å½’ä¸€åŒ–
        mixed = librosa.util.normalize(mixed)
        
        return self._numpy_to_bytes(mixed, sr)
    
    def _bytes_to_numpy(self, audio_data: bytes) -> Tuple[np.ndarray, int]:
        """bytes è½¬ numpy"""
        # ä½¿ç”¨ librosa åŠ è½½
        audio_np, sr = librosa.load(
            io.BytesIO(audio_data),
            sr=self.sample_rate
        )
        return audio_np, sr
    
    def _numpy_to_bytes(self, audio_np: np.ndarray, sr: int) -> bytes:
        """numpy è½¬ bytes"""
        buffer = io.BytesIO()
        sf.write(buffer, audio_np, sr, format='WAV')
        buffer.seek(0)
        return buffer.read()
    
    def _equalize(self, audio: np.ndarray, sr: int) -> np.ndarray:
        """å‡è¡¡åŒ–"""
        # ç®€å•çš„é¢‘æ®µå¢å¼º
        return audio
    
    def _compress(self, audio: np.ndarray) -> np.ndarray:
        """åŠ¨æ€èŒƒå›´å‹ç¼©"""
        threshold = 0.5
        ratio = 4.0
        
        compressed = np.copy(audio)
        mask = np.abs(compressed) > threshold
        compressed[mask] = threshold + (compressed[mask] - threshold) / ratio
        
        return compressed
```

**API ç«¯ç‚¹**:
```python
@router.post("/api/v1/audio/denoise")
async def denoise_audio(file: UploadFile = File(...)):
    """é™å™ª"""
    audio_data = await file.read()
    processed = await audio_processing_service.denoise(audio_data)
    return Response(content=processed, media_type="audio/wav")

@router.post("/api/v1/audio/enhance")
async def enhance_audio(
    file: UploadFile = File(...),
    enhancements: List[str] = Query(...)
):
    """å¢å¼º"""
    audio_data = await file.read()
    processed = await audio_processing_service.enhance(audio_data, enhancements)
    return Response(content=processed, media_type="audio/wav")
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] é™å™ªåŠŸèƒ½å®Œå–„
- [ ] éŸ³é¢‘å¢å¼ºæ•ˆæœå¥½
- [ ] æ”¯æŒè¯­é€Ÿã€éŸ³è°ƒè°ƒæ•´
- [ ] æ”¯æŒæ··éŸ³

---

### ğŸ”„ P1 - é«˜çº§åŠŸèƒ½ï¼ˆè¿­ä»£2ï¼š1å‘¨ï¼‰

#### 1. å¤šè¯­è¨€æ”¯æŒå¢å¼º
- [ ] æ”¯æŒè‡ªåŠ¨è¯­è¨€æ£€æµ‹
- [ ] æ”¯æŒå¤šè¯­è¨€æ··åˆè¯†åˆ«
- [ ] å¢åŠ æ›´å¤šè¯­è¨€æ”¯æŒ

#### 2. å®æ—¶æ€§èƒ½ä¼˜åŒ–
- [ ] æ¨¡å‹é‡åŒ–ï¼ˆINT8ï¼‰
- [ ] æ‰¹å¤„ç†ä¼˜åŒ–
- [ ] GPU åŠ é€Ÿæ”¯æŒ
- [ ] ç¼“å­˜ä¼˜åŒ–

#### 3. é«˜çº§ VAD
- [ ] ç«¯ç‚¹æ£€æµ‹ä¼˜åŒ–
- [ ] å™ªéŸ³é²æ£’æ€§å¢å¼º
- [ ] è‡ªé€‚åº”é˜ˆå€¼

---

### ğŸ”„ P2 - ä¼˜åŒ–å¢å¼ºï¼ˆè¿­ä»£3ï¼š1å‘¨ï¼‰

#### 1. è´¨é‡ä¼˜åŒ–
- [ ] ASR åå¤„ç†ï¼ˆæ ‡ç‚¹ã€çº é”™ï¼‰
- [ ] TTS éŸµå¾‹ä¼˜åŒ–
- [ ] æƒ…æ„Ÿè¯†åˆ«å’Œåˆæˆ

#### 2. å¯è§‚æµ‹æ€§
- [ ] Prometheus æŒ‡æ ‡
- [ ] æ€§èƒ½ç›‘æ§
- [ ] è´¨é‡ç›‘æ§

#### 3. æˆæœ¬ä¼˜åŒ–
- [ ] æ¨¡å‹ç¼“å­˜
- [ ] æ™ºèƒ½é™çº§
- [ ] æ‰¹å¤„ç†ä¼˜åŒ–

---

## ä¸‰ã€è¯¦ç»†å®æ–½æ–¹æ¡ˆ

### é˜¶æ®µ1ï¼šæ ¸å¿ƒåŠŸèƒ½ï¼ˆWeek 1-2ï¼‰

#### Day 1-4: æµå¼è¯†åˆ«å’Œ Azure é›†æˆ
1. å®ç° WebSocket æµå¼ ASR
2. é›†æˆ Azure Speech SDK
3. æµ‹è¯•å’Œä¼˜åŒ–

#### Day 5-7: å…¨åŒå·¥å’Œè¯­éŸ³å…‹éš†
1. å®Œå–„å…¨åŒå·¥å¼•æ“
2. å®ç°è¯­éŸ³å…‹éš†åŠŸèƒ½
3. æµ‹è¯•

#### Day 8-10: éŸ³é¢‘å¤„ç†
1. å®ç°é™å™ªå’Œå¢å¼º
2. å®ç°è¯­é€Ÿã€éŸ³è°ƒè°ƒæ•´
3. æµ‹è¯•

#### Day 11-14: é›†æˆå’Œä¼˜åŒ–
1. ç«¯åˆ°ç«¯æµ‹è¯•
2. æ€§èƒ½ä¼˜åŒ–
3. æ–‡æ¡£æ›´æ–°

### é˜¶æ®µ2ï¼šé«˜çº§åŠŸèƒ½ï¼ˆWeek 3ï¼‰

#### Day 15-17: å¤šè¯­è¨€å’Œæ€§èƒ½
1. å¤šè¯­è¨€æ”¯æŒå¢å¼º
2. æ€§èƒ½ä¼˜åŒ–
3. æµ‹è¯•

#### Day 18-21: è´¨é‡å’Œç›‘æ§
1. è´¨é‡ä¼˜åŒ–
2. ç›‘æ§æŒ‡æ ‡
3. æ–‡æ¡£

---

## å››ã€éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶
- [ ] æµå¼è¯†åˆ«å»¶è¿Ÿ < 500ms
- [ ] Azure é›†æˆå®Œæˆ
- [ ] å…¨åŒå·¥æ­£å¸¸å·¥ä½œ
- [ ] è¯­éŸ³å…‹éš†è´¨é‡é«˜
- [ ] éŸ³é¢‘å¤„ç†æ•ˆæœå¥½

### æ€§èƒ½éªŒæ”¶
- [ ] ASR å®æ—¶ç‡ > 1.0
- [ ] TTS é¦–å­—èŠ‚æ—¶é—´ < 200ms
- [ ] æ”¯æŒ 50 å¹¶å‘
- [ ] å†…å­˜ä½¿ç”¨ < 2GB

### è´¨é‡éªŒæ”¶
- [ ] ASR å‡†ç¡®ç‡ > 95%
- [ ] TTS è‡ªç„¶åº¦è¯„åˆ† > 4.0/5.0
- [ ] æ‰€æœ‰ TODO æ¸…ç†
- [ ] æ–‡æ¡£å®Œæ•´

---

## æ€»ç»“

Voice Engine çš„ä¸»è¦å¾…å®ŒæˆåŠŸèƒ½ï¼š

1. **æµå¼è¯†åˆ«**: WebSocket å®æ—¶ ASR
2. **Azure é›†æˆ**: ä¼ä¸šçº§è¯­éŸ³æœåŠ¡
3. **å…¨åŒå·¥**: åŒæ—¶å½•éŸ³å’Œæ’­æ”¾ï¼Œæ”¯æŒæ‰“æ–­
4. **è¯­éŸ³å…‹éš†**: ä¸ªæ€§åŒ–è¯­éŸ³åˆæˆ
5. **éŸ³é¢‘å¤„ç†**: é™å™ªã€å¢å¼ºã€å˜é€Ÿå˜è°ƒ

å®Œæˆåå°†å…·å¤‡ç”Ÿäº§çº§è¯­éŸ³æœåŠ¡èƒ½åŠ›ã€‚


