# Voice Engine - åŠŸèƒ½æ¸…å•ä¸è¿­ä»£è®¡åˆ’

> **æœåŠ¡åç§°**: Voice Engine (algo/voice-engine)
> **å½“å‰ç‰ˆæœ¬**: v1.0.0
> **å®Œæˆåº¦**: 70%
> **ä¼˜å…ˆçº§**: P0 (æ ¸å¿ƒæœåŠ¡)

---

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

Voice Engine æ˜¯è¯­éŸ³å¤„ç†çš„æ ¸å¿ƒæœåŠ¡,æä¾› ASR(è¯­éŸ³è¯†åˆ«)ã€TTS(è¯­éŸ³åˆæˆ)å’Œ VAD(è¯­éŸ³æ´»åŠ¨æ£€æµ‹)åŠŸèƒ½ã€‚å½“å‰å·²å®ç°åŸºç¡€åŠŸèƒ½,ä½†åœ¨æµå¼å¤„ç†ã€å¤šå‚å•†æ”¯æŒå’Œæ€§èƒ½ä¼˜åŒ–æ–¹é¢ä»æœ‰æå‡ç©ºé—´ã€‚

### å…³é”®æŒ‡æ ‡

| æŒ‡æ ‡           | å½“å‰çŠ¶æ€        | ç›®æ ‡çŠ¶æ€ | å·®è·  |
| -------------- | --------------- | -------- | ----- |
| ASR è¯†åˆ«å‡†ç¡®ç‡ | ~85%            | > 95%    | âš ï¸ ä¸­ |
| TTS ç¼“å­˜å‘½ä¸­ç‡ | 0% (æœªå®ç°)     | > 40%    | âš ï¸ é«˜ |
| VAD æ£€æµ‹å‡†ç¡®ç‡ | ~85% (Silero)   | > 95%    | âš ï¸ ä¸­ |
| æµå¼ ASR       | ä¸æ”¯æŒ          | å®Œæ•´æ”¯æŒ | âš ï¸ é«˜ |
| å‚å•†æ”¯æŒ       | ä»… Whisper/Edge | +Azure   | âš ï¸ ä¸­ |

---

## ğŸ” æœªå®ŒæˆåŠŸèƒ½æ¸…å•

### P0 çº§åˆ« (é˜»å¡æ€§åŠŸèƒ½)

#### 1. æµå¼ ASR è¯†åˆ«

**æ–‡ä»¶**: `app/routers/asr.py:91`

**é—®é¢˜æè¿°**:

```python
# TODO: å®ç°æµå¼è¯†åˆ«
```

**å½±å“**: æ— æ³•å®ç°å®æ—¶è¯­éŸ³å¯¹è¯,ç”¨æˆ·ä½“éªŒå·®

**è¯¦ç»†è®¾è®¡æ–¹æ¡ˆ**:

```python
# app/services/streaming_asr_service.py

from typing import AsyncIterator
import asyncio
import numpy as np
from faster_whisper import WhisperModel
import webrtcvad

class StreamingASRService:
    """æµå¼ASRæœåŠ¡"""

    def __init__(
        self,
        model_size: str = "base",
        chunk_duration_ms: int = 300,
        vad_mode: int = 3
    ):
        # Whisperæ¨¡å‹
        self.whisper_model = WhisperModel(
            model_size,
            device="cpu",
            compute_type="int8"
        )

        # VAD (WebRTC)
        self.vad = webrtcvad.Vad(vad_mode)

        # éŸ³é¢‘ç¼“å†²åŒº
        self.chunk_duration_ms = chunk_duration_ms
        self.sample_rate = 16000
        self.chunk_size = int(self.sample_rate * chunk_duration_ms / 1000)

        # çŠ¶æ€ç®¡ç†
        self.audio_buffer = bytearray()
        self.speech_buffer = bytearray()
        self.is_speaking = False
        self.silence_duration = 0
        self.max_silence_duration_ms = 1500  # 1.5ç§’é™éŸ³è§¦å‘è¯†åˆ«

    async def process_stream(
        self,
        audio_stream: AsyncIterator[bytes]
    ) -> AsyncIterator[dict]:
        """å¤„ç†éŸ³é¢‘æµ"""

        yield {
            "type": "session_start",
            "sample_rate": self.sample_rate,
            "chunk_duration_ms": self.chunk_duration_ms
        }

        async for audio_chunk in audio_stream:
            # æ·»åŠ åˆ°ç¼“å†²åŒº
            self.audio_buffer.extend(audio_chunk)

            # å¤„ç†å®Œæ•´çš„chunk
            while len(self.audio_buffer) >= self.chunk_size * 2:  # 16-bit = 2 bytes
                chunk = bytes(self.audio_buffer[:self.chunk_size * 2])
                self.audio_buffer = self.audio_buffer[self.chunk_size * 2:]

                # VADæ£€æµ‹
                is_speech = self.vad.is_speech(chunk, self.sample_rate)

                if is_speech:
                    # æ£€æµ‹åˆ°è¯­éŸ³
                    if not self.is_speaking:
                        # è¯­éŸ³å¼€å§‹
                        self.is_speaking = True
                        self.speech_buffer = bytearray()

                        yield {
                            "type": "speech_start",
                            "timestamp": self._get_timestamp()
                        }

                    # ç´¯ç§¯è¯­éŸ³æ•°æ®
                    self.speech_buffer.extend(chunk)
                    self.silence_duration = 0

                    # å¢é‡è¯†åˆ« (å¯é€‰)
                    if len(self.speech_buffer) >= self.sample_rate * 2 * 3:  # æ¯3ç§’
                        partial_text = await self._recognize_partial(self.speech_buffer)

                        yield {
                            "type": "partial_result",
                            "text": partial_text,
                            "is_final": False
                        }

                else:
                    # é™éŸ³
                    if self.is_speaking:
                        self.silence_duration += self.chunk_duration_ms
                        self.speech_buffer.extend(chunk)  # ä¿ç•™é™éŸ³ä»¥ä¿æŒè¿è´¯æ€§

                        # é™éŸ³è¶…è¿‡é˜ˆå€¼,è§¦å‘æœ€ç»ˆè¯†åˆ«
                        if self.silence_duration >= self.max_silence_duration_ms:
                            final_text = await self._recognize_final(self.speech_buffer)

                            yield {
                                "type": "final_result",
                                "text": final_text,
                                "is_final": True,
                                "duration_ms": len(self.speech_buffer) / self.sample_rate / 2 * 1000
                            }

                            # é‡ç½®çŠ¶æ€
                            self.is_speaking = False
                            self.speech_buffer = bytearray()
                            self.silence_duration = 0

                            yield {
                                "type": "speech_end",
                                "timestamp": self._get_timestamp()
                            }

        # æµç»“æŸ,å¤„ç†å‰©ä½™æ•°æ®
        if self.is_speaking and len(self.speech_buffer) > 0:
            final_text = await self._recognize_final(self.speech_buffer)

            yield {
                "type": "final_result",
                "text": final_text,
                "is_final": True
            }

        yield {
            "type": "session_end"
        }

    async def _recognize_partial(self, audio_data: bytes) -> str:
        """å¢é‡è¯†åˆ« (å¿«é€Ÿ,ä½å‡†ç¡®ç‡)"""
        # ä½¿ç”¨è¾ƒå°çš„beam_sizeåŠ é€Ÿ
        segments, _ = self.whisper_model.transcribe(
            self._bytes_to_audio(audio_data),
            beam_size=1,
            best_of=1,
            temperature=0.0
        )

        text = " ".join([seg.text for seg in segments])
        return text.strip()

    async def _recognize_final(self, audio_data: bytes) -> str:
        """æœ€ç»ˆè¯†åˆ« (å®Œæ•´å‡†ç¡®ç‡)"""
        segments, info = self.whisper_model.transcribe(
            self._bytes_to_audio(audio_data),
            beam_size=5,
            best_of=5,
            temperature=0.0,
            vad_filter=True
        )

        text = " ".join([seg.text for seg in segments])
        return text.strip()

    def _bytes_to_audio(self, audio_bytes: bytes) -> np.ndarray:
        """å­—èŠ‚è½¬éŸ³é¢‘æ•°ç»„"""
        audio_array = np.frombuffer(audio_bytes, dtype=np.int16)
        return audio_array.astype(np.float32) / 32768.0

    def _get_timestamp(self) -> float:
        """è·å–æ—¶é—´æˆ³"""
        import time
        return time.time()


# WebSocket API å®ç°

from fastapi import WebSocket, WebSocketDisconnect
import json

@router.websocket("/ws/asr/stream")
async def websocket_asr_stream(websocket: WebSocket):
    """WebSocket æµå¼ASR"""

    await websocket.accept()

    try:
        # æ¥æ”¶é…ç½®
        config_msg = await websocket.receive_json()

        model_size = config_msg.get("model_size", "base")
        language = config_msg.get("language", "zh")

        # åˆå§‹åŒ–æµå¼ASRæœåŠ¡
        streaming_asr = StreamingASRService(
            model_size=model_size,
            chunk_duration_ms=300
        )

        async def audio_generator():
            """éŸ³é¢‘ç”Ÿæˆå™¨"""
            while True:
                try:
                    message = await websocket.receive()

                    if "bytes" in message:
                        yield message["bytes"]
                    elif "text" in message:
                        # æ§åˆ¶å‘½ä»¤
                        cmd = json.loads(message["text"])
                        if cmd.get("type") == "end":
                            break

                except WebSocketDisconnect:
                    break

        # å¤„ç†æµå¼è¯†åˆ«
        async for result in streaming_asr.process_stream(audio_generator()):
            await websocket.send_json(result)

    except WebSocketDisconnect:
        print("Client disconnected")

    except Exception as e:
        await websocket.send_json({
            "type": "error",
            "error": str(e)
        })

    finally:
        await websocket.close()
```

**å‰ç«¯é›†æˆç¤ºä¾‹**:

```javascript
// React æµå¼ASRå®¢æˆ·ç«¯

class StreamingASRClient {
  constructor(wsUrl, config) {
    this.wsUrl = wsUrl;
    this.config = config;
    this.ws = null;
    this.mediaRecorder = null;
    this.onResult = null;
  }

  async start() {
    // è¿æ¥WebSocket
    this.ws = new WebSocket(this.wsUrl);

    this.ws.onopen = () => {
      // å‘é€é…ç½®
      this.ws.send(JSON.stringify(this.config));

      // å¼€å§‹å½•éŸ³
      this.startRecording();
    };

    this.ws.onmessage = (event) => {
      const result = JSON.parse(event.data);

      if (this.onResult) {
        this.onResult(result);
      }
    };
  }

  async startRecording() {
    const stream = await navigator.mediaDevices.getUserMedia({
      audio: {
        sampleRate: 16000,
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
      },
    });

    // ä½¿ç”¨MediaRecorderå½•éŸ³
    this.mediaRecorder = new MediaRecorder(stream, {
      mimeType: 'audio/webm;codecs=opus',
    });

    this.mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0 && this.ws.readyState === WebSocket.OPEN) {
        // å‘é€éŸ³é¢‘æ•°æ®
        this.ws.send(event.data);
      }
    };

    // æ¯300mså‘é€ä¸€æ¬¡æ•°æ®
    this.mediaRecorder.start(300);
  }

  stop() {
    if (this.mediaRecorder) {
      this.mediaRecorder.stop();
    }

    if (this.ws) {
      this.ws.send(JSON.stringify({ type: 'end' }));
      this.ws.close();
    }
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const client = new StreamingASRClient('ws://localhost:8001/ws/asr/stream', {
  model_size: 'base',
  language: 'zh',
});

client.onResult = (result) => {
  if (result.type === 'partial_result') {
    console.log('å¢é‡ç»“æœ:', result.text);
    setPartialText(result.text);
  } else if (result.type === 'final_result') {
    console.log('æœ€ç»ˆç»“æœ:', result.text);
    setFinalText(result.text);
  }
};

client.start();
```

**éªŒæ”¶æ ‡å‡†**:

- [ ] WebSocket è¿æ¥ç¨³å®š
- [ ] å¢é‡è¯†åˆ«å»¶è¿Ÿ < 500ms
- [ ] æœ€ç»ˆè¯†åˆ«å‡†ç¡®ç‡ > 90%
- [ ] ç«¯ç‚¹æ£€æµ‹å‡†ç¡® (VAD)
- [ ] å¹¶å‘æ”¯æŒ > 20 è¿æ¥

**å·¥ä½œé‡**: 3-4 å¤©

---

#### 2. TTS Redis ç¼“å­˜

**æ–‡ä»¶**: `app/services/tts_service.py:194,199`

**é—®é¢˜æè¿°**:

```python
# TODO: ä½¿ç”¨ Redis ç¼“å­˜
```

**å½±å“**: ç›¸åŒæ–‡æœ¬é‡å¤åˆæˆ,æµªè´¹èµ„æºå’Œæ—¶é—´

**è¯¦ç»†è®¾è®¡æ–¹æ¡ˆ**:

```python
# app/infrastructure/tts_cache.py

import redis
import hashlib
import base64
from typing import Optional
from datetime import timedelta

class TTSRedisCache:
    """TTS Redisç¼“å­˜"""

    def __init__(
        self,
        redis_url: str = "redis://localhost:6379/1",
        ttl_days: int = 30,
        max_cache_size_mb: int = 1000
    ):
        self.redis = redis.from_url(redis_url)
        self.ttl = timedelta(days=ttl_days)
        self.max_cache_size = max_cache_size_mb * 1024 * 1024  # è½¬ä¸ºå­—èŠ‚

    def _generate_key(
        self,
        text: str,
        voice: str,
        rate: str,
        pitch: str,
        format: str
    ) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_input = f"{text}:{voice}:{rate}:{pitch}:{format}"
        hash_key = hashlib.sha256(cache_input.encode()).hexdigest()
        return f"tts:cache:{hash_key}"

    def get(
        self,
        text: str,
        voice: str,
        rate: str = "+0%",
        pitch: str = "+0Hz",
        format: str = "mp3"
    ) -> Optional[bytes]:
        """è·å–ç¼“å­˜"""
        key = self._generate_key(text, voice, rate, pitch, format)

        cached = self.redis.get(key)

        if cached:
            # æ›´æ–°è®¿é—®ç»Ÿè®¡
            self.redis.hincrby(f"{key}:stats", "hits", 1)
            self.redis.expire(key, self.ttl)  # åˆ·æ–°TTL

            return base64.b64decode(cached)

        return None

    def set(
        self,
        text: str,
        voice: str,
        rate: str,
        pitch: str,
        format: str,
        audio_data: bytes
    ):
        """è®¾ç½®ç¼“å­˜"""
        key = self._generate_key(text, voice, rate, pitch, format)

        # æ£€æŸ¥ç¼“å­˜å¤§å°
        current_size = self._get_total_cache_size()
        audio_size = len(audio_data)

        if current_size + audio_size > self.max_cache_size:
            self._evict_lru(audio_size)

        # ç¼–ç å¹¶å­˜å‚¨
        encoded = base64.b64encode(audio_data)
        self.redis.setex(key, self.ttl, encoded)

        # å­˜å‚¨å…ƒæ•°æ®
        self.redis.hset(f"{key}:stats", mapping={
            "text_length": len(text),
            "audio_size": audio_size,
            "created_at": self._get_timestamp(),
            "hits": 0
        })

        # æ·»åŠ åˆ°LRUåˆ—è¡¨
        self.redis.zadd("tts:lru", {key: self._get_timestamp()})

    def _get_total_cache_size(self) -> int:
        """è·å–æ€»ç¼“å­˜å¤§å°"""
        total = 0
        for key in self.redis.scan_iter("tts:cache:*"):
            if not key.endswith(b":stats"):
                total += self.redis.memory_usage(key) or 0
        return total

    def _evict_lru(self, needed_size: int):
        """LRUæ·˜æ±°"""
        evicted_size = 0

        # æŒ‰æ—¶é—´æˆ³å‡åºè·å–(æœ€ä¹…æœªä½¿ç”¨)
        keys = self.redis.zrange("tts:lru", 0, -1)

        for key in keys:
            if evicted_size >= needed_size:
                break

            # è·å–å¤§å°
            size = int(self.redis.hget(f"{key}:stats", "audio_size") or 0)

            # åˆ é™¤
            self.redis.delete(key)
            self.redis.delete(f"{key}:stats")
            self.redis.zrem("tts:lru", key)

            evicted_size += size

        print(f"ğŸ—‘ï¸  LRUæ·˜æ±°: {len(keys)}ä¸ªç¼“å­˜, {evicted_size/1024/1024:.2f}MB")

    def get_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        keys = list(self.redis.scan_iter("tts:cache:*"))
        cache_keys = [k for k in keys if not k.endswith(b":stats")]

        total_size = sum(self.redis.memory_usage(k) or 0 for k in cache_keys)
        total_hits = sum(
            int(self.redis.hget(f"{k}:stats", "hits") or 0)
            for k in cache_keys
        )

        return {
            "total_entries": len(cache_keys),
            "total_size_mb": total_size / 1024 / 1024,
            "total_hits": total_hits,
            "hit_rate": total_hits / max(len(cache_keys), 1)
        }

    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        for key in self.redis.scan_iter("tts:cache:*"):
            self.redis.delete(key)
        self.redis.delete("tts:lru")

    def _get_timestamp(self) -> float:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        import time
        return time.time()


# é›†æˆåˆ° TTSService

class TTSService:
    def __init__(self):
        self.cache = TTSRedisCache(
            redis_url=settings.REDIS_URL,
            ttl_days=30,
            max_cache_size_mb=1000
        )

    async def synthesize(self, request: TTSRequest) -> TTSResponse:
        """åˆæˆè¯­éŸ³"""
        # 1. æ£€æŸ¥ç¼“å­˜
        cached_audio = self.cache.get(
            text=request.text,
            voice=request.voice,
            rate=request.rate,
            pitch=request.pitch,
            format=request.format
        )

        if cached_audio:
            return TTSResponse(
                audio_base64=base64.b64encode(cached_audio).decode(),
                cached=True,
                duration_ms=self._calculate_duration(cached_audio),
                processing_time_ms=0
            )

        # 2. åˆæˆ
        start_time = time.time()

        audio_data = await self._synthesize_with_edge_tts(request)

        processing_time = (time.time() - start_time) * 1000

        # 3. å­˜å…¥ç¼“å­˜
        self.cache.set(
            text=request.text,
            voice=request.voice,
            rate=request.rate,
            pitch=request.pitch,
            format=request.format,
            audio_data=audio_data
        )

        return TTSResponse(
            audio_base64=base64.b64encode(audio_data).decode(),
            cached=False,
            duration_ms=self._calculate_duration(audio_data),
            processing_time_ms=processing_time
        )


# ç¼“å­˜ç®¡ç†API

@router.get("/api/v1/tts/cache/stats")
async def get_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡"""
    return tts_service.cache.get_stats()

@router.post("/api/v1/tts/cache/clear")
async def clear_cache():
    """æ¸…ç©ºç¼“å­˜"""
    tts_service.cache.clear()
    return {"message": "ç¼“å­˜å·²æ¸…ç©º"}
```

**Grafana ç›‘æ§é¢æ¿**:

```python
# app/infrastructure/metrics.py

from prometheus_client import Counter, Histogram, Gauge

# TTSç¼“å­˜æŒ‡æ ‡
tts_cache_hits = Counter(
    "tts_cache_hits_total",
    "TTSç¼“å­˜å‘½ä¸­æ¬¡æ•°"
)

tts_cache_misses = Counter(
    "tts_cache_misses_total",
    "TTSç¼“å­˜æœªå‘½ä¸­æ¬¡æ•°"
)

tts_cache_size = Gauge(
    "tts_cache_size_bytes",
    "TTSç¼“å­˜æ€»å¤§å°(å­—èŠ‚)"
)

tts_cache_entries = Gauge(
    "tts_cache_entries_total",
    "TTSç¼“å­˜æ¡ç›®æ•°"
)

# åœ¨TTSServiceä¸­åŸ‹ç‚¹
async def synthesize(self, request: TTSRequest):
    cached_audio = self.cache.get(...)

    if cached_audio:
        tts_cache_hits.inc()
    else:
        tts_cache_misses.inc()

    # æ›´æ–°ç¼“å­˜æŒ‡æ ‡
    stats = self.cache.get_stats()
    tts_cache_size.set(stats["total_size_mb"] * 1024 * 1024)
    tts_cache_entries.set(stats["total_entries"])
```

**éªŒæ”¶æ ‡å‡†**:

- [ ] ç¼“å­˜å‘½ä¸­ç‡ > 40%
- [ ] ç¼“å­˜å‘½ä¸­æ—¶å»¶è¿Ÿ < 50ms
- [ ] LRU æ·˜æ±°æœºåˆ¶æ­£å¸¸
- [ ] Redis å†…å­˜ä½¿ç”¨ < 1GB
- [ ] ç»Ÿè®¡æ•°æ®å‡†ç¡®

**å·¥ä½œé‡**: 2 å¤©

---

#### 3. VAD å‡çº§ - Silero VAD

**å‚è€ƒ**: [voicehelper é¡¹ç›®å·²å®ç°]

**å½“å‰é—®é¢˜**: ç°æœ‰ VAD ä½¿ç”¨ Silero,ä½†é…ç½®ä¸å¤Ÿç²¾ç»†

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```python
# app/services/vad_service_v2.py

import torch
import numpy as np
from typing import List, Dict, Tuple

class SileroVADService:
    """Silero VAD å¢å¼ºç‰ˆ"""

    def __init__(
        self,
        model_path: str = None,
        threshold: float = 0.5,
        min_speech_duration_ms: int = 250,
        max_speech_duration_s: float = float('inf'),
        min_silence_duration_ms: int = 100,
        window_size_samples: int = 512,
        speech_pad_ms: int = 30
    ):
        # åŠ è½½æ¨¡å‹
        if model_path:
            self.model = torch.jit.load(model_path)
        else:
            self.model, utils = torch.hub.load(
                repo_or_dir='snakers4/silero-vad',
                model='silero_vad',
                force_reload=False
            )

        self.model.eval()

        # å‚æ•°
        self.threshold = threshold
        self.min_speech_duration_ms = min_speech_duration_ms
        self.max_speech_duration_s = max_speech_duration_s
        self.min_silence_duration_ms = min_silence_duration_ms
        self.window_size_samples = window_size_samples
        self.speech_pad_ms = speech_pad_ms

        # é‡‡æ ·ç‡
        self.sample_rate = 16000

    async def detect_voice_activity(
        self,
        audio_data: bytes,
        return_timestamps: bool = True
    ) -> Dict:
        """æ£€æµ‹è¯­éŸ³æ´»åŠ¨"""

        # è½¬æ¢éŸ³é¢‘
        audio_array = self._bytes_to_float32(audio_data)

        # å½’ä¸€åŒ–
        audio_tensor = torch.from_numpy(audio_array)

        # VADæ£€æµ‹
        speech_timestamps = self._get_speech_timestamps(
            audio_tensor,
            threshold=self.threshold,
            min_speech_duration_ms=self.min_speech_duration_ms,
            max_speech_duration_s=self.max_speech_duration_s,
            min_silence_duration_ms=self.min_silence_duration_ms,
            window_size_samples=self.window_size_samples,
            speech_pad_ms=self.speech_pad_ms
        )

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_duration_ms = len(audio_array) / self.sample_rate * 1000
        speech_duration_ms = sum(
            (ts['end'] - ts['start']) / self.sample_rate * 1000
            for ts in speech_timestamps
        )
        speech_ratio = speech_duration_ms / total_duration_ms if total_duration_ms > 0 else 0

        result = {
            "has_speech": len(speech_timestamps) > 0,
            "speech_ratio": speech_ratio,
            "total_duration_ms": total_duration_ms,
            "speech_duration_ms": speech_duration_ms,
            "speech_segments_count": len(speech_timestamps)
        }

        if return_timestamps:
            result["timestamps"] = [
                {
                    "start_ms": ts['start'] / self.sample_rate * 1000,
                    "end_ms": ts['end'] / self.sample_rate * 1000,
                    "duration_ms": (ts['end'] - ts['start']) / self.sample_rate * 1000
                }
                for ts in speech_timestamps
            ]

        return result

    def _get_speech_timestamps(
        self,
        audio: torch.Tensor,
        **kwargs
    ) -> List[Dict]:
        """è·å–è¯­éŸ³æ—¶é—´æˆ³"""

        # ä½¿ç”¨æ»‘åŠ¨çª—å£
        num_samples = len(audio)
        num_windows = num_samples // self.window_size_samples

        # è®¡ç®—æ¯ä¸ªçª—å£çš„è¯­éŸ³æ¦‚ç‡
        speech_probs = []

        for i in range(num_windows):
            start = i * self.window_size_samples
            end = start + self.window_size_samples

            chunk = audio[start:end]

            with torch.no_grad():
                prob = self.model(chunk.unsqueeze(0), self.sample_rate).item()

            speech_probs.append({
                'start': start,
                'end': end,
                'probability': prob
            })

        # æ ¹æ®é˜ˆå€¼ç­›é€‰
        speech_chunks = [
            chunk for chunk in speech_probs
            if chunk['probability'] >= self.threshold
        ]

        # åˆå¹¶è¿ç»­çš„è¯­éŸ³å—
        timestamps = []
        if speech_chunks:
            current_start = speech_chunks[0]['start']
            current_end = speech_chunks[0]['end']

            for chunk in speech_chunks[1:]:
                gap_ms = (chunk['start'] - current_end) / self.sample_rate * 1000

                if gap_ms <= self.min_silence_duration_ms:
                    # åˆå¹¶
                    current_end = chunk['end']
                else:
                    # ä¿å­˜å½“å‰æ®µ
                    duration_ms = (current_end - current_start) / self.sample_rate * 1000
                    if duration_ms >= self.min_speech_duration_ms:
                        timestamps.append({
                            'start': current_start,
                            'end': current_end
                        })

                    # å¼€å§‹æ–°æ®µ
                    current_start = chunk['start']
                    current_end = chunk['end']

            # ä¿å­˜æœ€åä¸€æ®µ
            duration_ms = (current_end - current_start) / self.sample_rate * 1000
            if duration_ms >= self.min_speech_duration_ms:
                timestamps.append({
                    'start': current_start,
                    'end': current_end
                })

        return timestamps

    def _bytes_to_float32(self, audio_bytes: bytes) -> np.ndarray:
        """å­—èŠ‚è½¬float32éŸ³é¢‘æ•°ç»„"""
        audio_int16 = np.frombuffer(audio_bytes, dtype=np.int16)
        audio_float32 = audio_int16.astype(np.float32) / 32768.0
        return audio_float32


# APIç«¯ç‚¹

@router.post("/api/v1/vad/detect/v2")
async def detect_vad_v2(request: VADRequest):
    """VADæ£€æµ‹ (Sileroå¢å¼ºç‰ˆ)"""

    result = await vad_service_v2.detect_voice_activity(
        audio_data=base64.b64decode(request.audio_base64),
        return_timestamps=request.return_timestamps
    )

    return result
```

**é…ç½®ä¼˜åŒ–**:

```yaml
# config/voice-engine.yaml

vad:
  provider: 'silero' # "silero" or "webrtc"

  silero:
    model_path: 'models/silero_vad.jit'
    threshold: 0.5 # 0.0-1.0, è¶Šé«˜è¶Šä¸¥æ ¼
    min_speech_duration_ms: 250 # æœ€çŸ­è¯­éŸ³æ®µ
    max_speech_duration_s: 30 # æœ€é•¿è¯­éŸ³æ®µ
    min_silence_duration_ms: 100 # æœ€çŸ­é™éŸ³æ®µ
    window_size_samples: 512 # çª—å£å¤§å°
    speech_pad_ms: 30 # è¯­éŸ³æ®µpadding
```

**éªŒæ”¶æ ‡å‡†**:

- [ ] VAD å‡†ç¡®ç‡ > 95%
- [ ] æ£€æµ‹å»¶è¿Ÿ < 200ms
- [ ] æ”¯æŒè‡ªå®šä¹‰é˜ˆå€¼
- [ ] æ—¶é—´æˆ³ç²¾ç¡®åˆ° 10ms

**å·¥ä½œé‡**: 2 å¤©

---

#### 4. Azure Speech SDK é›†æˆ

**æ–‡ä»¶**:

- `app/services/tts_service.py:132`
- `app/services/asr_service.py:190`

**é—®é¢˜æè¿°**:

```python
# TODO: å®ç° Azure Speech SDK é›†æˆ
```

**å½±å“**: ç¼ºå°‘å¤‡ç”¨è¯­éŸ³æœåŠ¡æä¾›å•†

**è¯¦ç»†è®¾è®¡æ–¹æ¡ˆ**:

```python
# app/services/azure_speech_service.py

import azure.cognitiveservices.speech as speechsdk
from typing import Optional, List
import asyncio

class AzureSpeechService:
    """Azure è¯­éŸ³æœåŠ¡"""

    def __init__(
        self,
        subscription_key: str,
        region: str = "eastasia"
    ):
        self.subscription_key = subscription_key
        self.region = region

        # ASRé…ç½®
        self.speech_config = speechsdk.SpeechConfig(
            subscription=subscription_key,
            region=region
        )

        # TTSé…ç½®
        self.tts_config = speechsdk.SpeechConfig(
            subscription=subscription_key,
            region=region
        )

    async def recognize_from_bytes(
        self,
        audio_data: bytes,
        language: str = "zh-CN"
    ) -> dict:
        """ASRè¯†åˆ«"""

        # è®¾ç½®è¯­è¨€
        self.speech_config.speech_recognition_language = language

        # ä»å­—èŠ‚æµåˆ›å»ºéŸ³é¢‘é…ç½®
        audio_format = speechsdk.audio.AudioStreamFormat(
            samples_per_second=16000,
            bits_per_sample=16,
            channels=1
        )

        # åˆ›å»ºæ¨é€æµ
        push_stream = speechsdk.audio.PushAudioInputStream(audio_format)
        push_stream.write(audio_data)
        push_stream.close()

        audio_config = speechsdk.audio.AudioConfig(stream=push_stream)

        # åˆ›å»ºè¯†åˆ«å™¨
        speech_recognizer = speechsdk.SpeechRecognizer(
            speech_config=self.speech_config,
            audio_config=audio_config
        )

        # åŒæ­¥è¯†åˆ«
        result = speech_recognizer.recognize_once()

        if result.reason == speechsdk.ResultReason.RecognizedSpeech:
            return {
                "text": result.text,
                "confidence": result.json.get("NBest", [{}])[0].get("Confidence", 0),
                "language": language,
                "duration_ms": result.duration.total_seconds() * 1000
            }

        elif result.reason == speechsdk.ResultReason.NoMatch:
            return {
                "text": "",
                "error": "æœªè¯†åˆ«åˆ°è¯­éŸ³"
            }

        elif result.reason == speechsdk.ResultReason.Canceled:
            cancellation = result.cancellation_details
            return {
                "text": "",
                "error": f"è¯†åˆ«å–æ¶ˆ: {cancellation.reason}, {cancellation.error_details}"
            }

    async def synthesize(
        self,
        text: str,
        voice: str = "zh-CN-XiaoxiaoNeural",
        rate: str = "0%",
        pitch: str = "0%"
    ) -> bytes:
        """TTSåˆæˆ"""

        # è®¾ç½®è¯­éŸ³
        self.tts_config.speech_synthesis_voice_name = voice

        # æ„å»ºSSML
        ssml = f"""
        <speak version='1.0' xml:lang='zh-CN'>
            <voice name='{voice}'>
                <prosody rate='{rate}' pitch='{pitch}'>
                    {text}
                </prosody>
            </voice>
        </speak>
        """

        # åˆ›å»ºåˆæˆå™¨
        synthesizer = speechsdk.SpeechSynthesizer(
            speech_config=self.tts_config,
            audio_config=None  # ä½¿ç”¨å†…å­˜æµ
        )

        # åˆæˆ
        result = synthesizer.speak_ssml_async(ssml).get()

        if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
            return result.audio_data

        elif result.reason == speechsdk.ResultReason.Canceled:
            cancellation = result.cancellation_details
            raise Exception(f"åˆæˆå¤±è´¥: {cancellation.reason}, {cancellation.error_details}")


# å¤šå‚å•†é€‚é…å™¨

from enum import Enum

class SpeechProvider(str, Enum):
    WHISPER = "whisper"
    AZURE = "azure"
    EDGE_TTS = "edge_tts"

class MultiProviderSpeechService:
    """å¤šå‚å•†è¯­éŸ³æœåŠ¡é€‚é…å™¨"""

    def __init__(self):
        # åˆå§‹åŒ–å„å‚å•†æœåŠ¡
        self.whisper = WhisperASRService()
        self.azure = AzureSpeechService(
            subscription_key=settings.AZURE_SPEECH_KEY,
            region=settings.AZURE_SPEECH_REGION
        )
        self.edge_tts = EdgeTTSService()

        # é»˜è®¤æä¾›å•†
        self.default_asr_provider = SpeechProvider.WHISPER
        self.default_tts_provider = SpeechProvider.EDGE_TTS

    async def recognize(
        self,
        audio_data: bytes,
        provider: Optional[SpeechProvider] = None,
        **kwargs
    ) -> dict:
        """ASRè¯†åˆ« (è‡ªåŠ¨é™çº§)"""

        provider = provider or self.default_asr_provider

        try:
            if provider == SpeechProvider.WHISPER:
                return await self.whisper.recognize_from_bytes(audio_data, **kwargs)

            elif provider == SpeechProvider.AZURE:
                return await self.azure.recognize_from_bytes(audio_data, **kwargs)

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ASRæä¾›å•†: {provider}")

        except Exception as e:
            print(f"âš ï¸  {provider} ASRå¤±è´¥: {e}, å°è¯•é™çº§")

            # é™çº§ç­–ç•¥
            if provider == SpeechProvider.AZURE:
                return await self.whisper.recognize_from_bytes(audio_data, **kwargs)
            else:
                raise

    async def synthesize(
        self,
        text: str,
        provider: Optional[SpeechProvider] = None,
        **kwargs
    ) -> bytes:
        """TTSåˆæˆ (è‡ªåŠ¨é™çº§)"""

        provider = provider or self.default_tts_provider

        try:
            if provider == SpeechProvider.EDGE_TTS:
                return await self.edge_tts.synthesize(text, **kwargs)

            elif provider == SpeechProvider.AZURE:
                return await self.azure.synthesize(text, **kwargs)

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„TTSæä¾›å•†: {provider}")

        except Exception as e:
            print(f"âš ï¸  {provider} TTSå¤±è´¥: {e}, å°è¯•é™çº§")

            # é™çº§ç­–ç•¥
            if provider == SpeechProvider.AZURE:
                return await self.edge_tts.synthesize(text, **kwargs)
            else:
                raise


# APIæ›´æ–°

@router.post("/api/v1/asr/recognize")
async def recognize_speech(request: ASRRequest):
    """ASRè¯†åˆ« (å¤šå‚å•†æ”¯æŒ)"""

    result = await multi_provider_service.recognize(
        audio_data=base64.b64decode(request.audio_base64),
        provider=request.provider,  # æ–°å¢å­—æ®µ
        language=request.language
    )

    return ASRResponse(**result)
```

**é…ç½®**:

```bash
# .env

# Azure Speech Service
AZURE_SPEECH_KEY=your_azure_key
AZURE_SPEECH_REGION=eastasia

# é»˜è®¤æä¾›å•†
DEFAULT_ASR_PROVIDER=whisper  # whisper, azure
DEFAULT_TTS_PROVIDER=edge_tts  # edge_tts, azure
```

**éªŒæ”¶æ ‡å‡†**:

- [ ] Azure ASR è¯†åˆ«æ­£å¸¸
- [ ] Azure TTS åˆæˆæ­£å¸¸
- [ ] è‡ªåŠ¨é™çº§æœºåˆ¶ç”Ÿæ•ˆ
- [ ] å¤šå‚å•†åˆ‡æ¢æ— ç¼

**å·¥ä½œé‡**: 2-3 å¤©

---

### P1 çº§åˆ« (é‡è¦åŠŸèƒ½)

#### 5. éŸ³é¢‘é™å™ªä¸å¢å¼º

**æ–‡ä»¶**: `main.py:261,264`

**é—®é¢˜æè¿°**:

```python
# TODO: å®ç°é™å™ª
# TODO: å®ç°éŸ³é¢‘å¢å¼º
```

**æŠ€æœ¯æ–¹æ¡ˆ**:

```python
# app/services/audio_enhancement.py

import noisereduce as nr
import librosa
import numpy as np
import scipy.signal as signal

class AudioEnhancementService:
    """éŸ³é¢‘å¢å¼ºæœåŠ¡"""

    def __init__(self, sample_rate: int = 16000):
        self.sample_rate = sample_rate

    async def enhance(
        self,
        audio_data: bytes,
        enable_denoise: bool = True,
        enable_normalization: bool = True,
        enable_equalization: bool = True
    ) -> bytes:
        """éŸ³é¢‘å¢å¼º"""

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        audio_array = self._bytes_to_float32(audio_data)

        # 1. é™å™ª
        if enable_denoise:
            audio_array = self._reduce_noise(audio_array)

        # 2. å½’ä¸€åŒ–
        if enable_normalization:
            audio_array = self._normalize_volume(audio_array)

        # 3. å‡è¡¡å™¨
        if enable_equalization:
            audio_array = self._apply_eq(audio_array)

        # è½¬å›å­—èŠ‚
        return self._float32_to_bytes(audio_array)

    def _reduce_noise(self, audio: np.ndarray) -> np.ndarray:
        """é™å™ª"""
        # ä½¿ç”¨noisereduceåº“
        # è‡ªåŠ¨æ£€æµ‹å™ªå£°
        return nr.reduce_noise(
            y=audio,
            sr=self.sample_rate,
            stationary=True,
            prop_decrease=0.8
        )

    def _normalize_volume(self, audio: np.ndarray) -> np.ndarray:
        """éŸ³é‡å½’ä¸€åŒ–"""
        # è®¡ç®—RMS
        rms = np.sqrt(np.mean(audio ** 2))

        if rms > 0:
            # å½’ä¸€åŒ–åˆ° -3dB
            target_rms = 0.7
            gain = target_rms / rms
            audio = audio * gain

        # é™å¹…
        audio = np.clip(audio, -1.0, 1.0)

        return audio

    def _apply_eq(self, audio: np.ndarray) -> np.ndarray:
        """è¯­éŸ³å‡è¡¡å™¨"""
        # é«˜é€šæ»¤æ³¢å™¨ (å»é™¤ä½é¢‘å™ªå£°)
        sos = signal.butter(4, 80, 'hp', fs=self.sample_rate, output='sos')
        audio = signal.sosfilt(sos, audio)

        # å¢å¼ºä¸­é¢‘ (è¯­éŸ³ä¸»è¦é¢‘æ®µ 300-3400Hz)
        # è¿™é‡Œç®€åŒ–å¤„ç†,å®é™…å¯ä½¿ç”¨å‚æ•°EQ

        return audio
```

**å·¥ä½œé‡**: 2-3 å¤©

---

#### 6. è¯­éŸ³å…‹éš† (å¯é€‰)

**æ–‡ä»¶**: `app/core/tts_engine.py:347`

**é—®é¢˜æè¿°**:

```python
# TODO: å®ç°è¯­éŸ³å…‹éš†(ä½¿ç”¨ YourTTS, Coqui TTS ç­‰)
```

**æŠ€æœ¯æ–¹æ¡ˆ**:

```python
# app/services/voice_cloning_service.py

from TTS.api import TTS
import torch

class VoiceCloningService:
    """è¯­éŸ³å…‹éš†æœåŠ¡"""

    def __init__(self, model_name: str = "tts_models/multilingual/multi-dataset/xtts_v2"):
        self.tts = TTS(model_name).to("cuda" if torch.cuda.is_available() else "cpu")

    async def clone_voice(
        self,
        text: str,
        reference_audio_path: str,
        language: str = "zh-cn"
    ) -> bytes:
        """å…‹éš†è¯­éŸ³"""

        output_path = "/tmp/cloned_voice.wav"

        self.tts.tts_to_file(
            text=text,
            file_path=output_path,
            speaker_wav=reference_audio_path,
            language=language
        )

        with open(output_path, "rb") as f:
            return f.read()
```

**å·¥ä½œé‡**: 3-5 å¤© (éœ€ GPU)

---

## ğŸ“ˆ åç»­è¿­ä»£è®¡åˆ’

### Sprint 1: æ ¸å¿ƒåŠŸèƒ½è¡¥å…¨ (Week 1-2)

#### ä»»åŠ¡æ¸…å•

| ä»»åŠ¡              | ä¼˜å…ˆçº§ | å·¥ä½œé‡ | è´Ÿè´£äºº        | çŠ¶æ€      |
| ----------------- | ------ | ------ | ------------- | --------- |
| æµå¼ ASR è¯†åˆ«     | P0     | 3-4 å¤© | Backend Eng 1 | ğŸ“ å¾…å¼€å§‹ |
| TTS Redis ç¼“å­˜    | P0     | 2 å¤©   | Backend Eng 2 | ğŸ“ å¾…å¼€å§‹ |
| VAD ä¼˜åŒ– (Silero) | P0     | 2 å¤©   | AI Engineer   | ğŸ“ å¾…å¼€å§‹ |

#### éªŒæ”¶æ ‡å‡†

- [ ] WebSocket æµå¼ ASR å¯ç”¨
- [ ] TTS ç¼“å­˜å‘½ä¸­ç‡ > 40%
- [ ] VAD å‡†ç¡®ç‡ > 95%

---

### Sprint 2: å¤šå‚å•†ä¸å¢å¼º (Week 3-4)

#### ä»»åŠ¡æ¸…å•

| ä»»åŠ¡              | ä¼˜å…ˆçº§ | å·¥ä½œé‡ | è´Ÿè´£äºº        | çŠ¶æ€      |
| ----------------- | ------ | ------ | ------------- | --------- |
| Azure Speech é›†æˆ | P0     | 2-3 å¤© | Backend Eng 1 | ğŸ“ å¾…å¼€å§‹ |
| éŸ³é¢‘é™å™ªå¢å¼º      | P1     | 2-3 å¤© | AI Engineer   | ğŸ“ å¾…å¼€å§‹ |
| éŸ³é¢‘æ—¶é•¿è®¡ç®—      | P1     | 0.5 å¤© | Backend Eng 2 | ğŸ“ å¾…å¼€å§‹ |

#### éªŒæ”¶æ ‡å‡†

- [ ] Azure ASR/TTS å¯ç”¨
- [ ] é™å™ªæ•ˆæœæ˜æ˜¾æ”¹å–„
- [ ] è‡ªåŠ¨é™çº§æœºåˆ¶ç”Ÿæ•ˆ

---

### Sprint 3: é«˜çº§åŠŸèƒ½ (Week 5-6)

#### ä»»åŠ¡æ¸…å•

| ä»»åŠ¡            | ä¼˜å…ˆçº§ | å·¥ä½œé‡ | è´Ÿè´£äºº          | çŠ¶æ€      |
| --------------- | ------ | ------ | --------------- | --------- |
| è¯­éŸ³å…‹éš† (å¯é€‰) | P2     | 3-5 å¤© | AI Engineer     | ğŸ“ å¾…è§„åˆ’ |
| å…¨åŒå·¥å¯¹è¯      | P1     | 5-7 å¤© | Backend Eng 1+2 | ğŸ“ å¾…è§„åˆ’ |
| æƒ…æ„Ÿè¯†åˆ«        | P2     | 4-5 å¤© | AI Engineer     | ğŸ“ å¾…è§„åˆ’ |

---

## ğŸ¯ ä¸šç•Œå¯¹æ¯”

### ä¸ voicehelper å¯¹æ¯”

| åŠŸèƒ½       | voicehelper | æœ¬é¡¹ç›®å½“å‰ | æœ¬é¡¹ç›®ç›®æ ‡    |
| ---------- | ----------- | ---------- | ------------- |
| Silero VAD | âœ…          | âœ…         | âœ… ä¼˜åŒ–       |
| æµå¼ ASR   | âœ…          | âŒ         | âœ… å®ç°       |
| TTS ç¼“å­˜   | âŒ          | âŒ         | âœ… Redis      |
| å¤šå‚å•†æ”¯æŒ | âš ï¸          | âš ï¸         | âœ… Azure      |
| æƒ…æ„Ÿè¯†åˆ«   | âœ…          | âŒ         | âœ… (Sprint 3) |

---

## ğŸ“Š æˆåŠŸæŒ‡æ ‡

| æŒ‡æ ‡           | å½“å‰  | ç›®æ ‡ (Sprint 1) | ç›®æ ‡ (Sprint 3) |
| -------------- | ----- | --------------- | --------------- |
| ASR å‡†ç¡®ç‡     | ~85%  | > 90%           | > 95%           |
| TTS ç¼“å­˜å‘½ä¸­ç‡ | 0%    | > 40%           | > 60%           |
| VAD å‡†ç¡®ç‡     | ~85%  | > 95%           | > 98%           |
| å¹³å‡ ASR å»¶è¿Ÿ  | ~1.5s | < 1.0s          | < 500ms         |
| å¹³å‡ TTS å»¶è¿Ÿ  | ~1.2s | < 500ms         | < 300ms         |

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0
**æœ€åæ›´æ–°**: 2025-10-27
**ä¸‹æ¬¡æ›´æ–°**: Sprint 1 ç»“æŸå
