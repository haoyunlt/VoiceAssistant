# P0 ä»»åŠ¡å®æ–½æŒ‡å—

> ä¼˜å…ˆçº§ï¼šâš¡ æœ€é«˜
> é¢„è®¡å·¥æœŸï¼š2.5 äººæ—¥
> ç›®æ ‡ï¼šç»Ÿä¸€ LLM è°ƒç”¨å’Œå‘é‡å­˜å‚¨è®¿é—®

---

## âœ… P0-1: ç»Ÿä¸€ LLM è°ƒç”¨è·¯å¾„ï¼ˆ2 äººæ—¥ï¼‰

### ç›®æ ‡

æ‰€æœ‰ AI å¼•æ“æœåŠ¡å¿…é¡»é€šè¿‡`model-adapter`è°ƒç”¨ LLMï¼Œç¦æ­¢ç›´è¿ OpenAI/Anthropic ç­‰ã€‚

### å®æ–½æ­¥éª¤

#### ç¬¬ 1 æ­¥ï¼šåˆ›å»ºç»Ÿä¸€ LLM å®¢æˆ·ç«¯ï¼ˆâœ… å·²å®Œæˆï¼‰

å·²åˆ›å»º `algo/common/llm_client.py` - ç»Ÿä¸€ LLM è°ƒç”¨å®¢æˆ·ç«¯

**åŠŸèƒ½**:

- âœ… é€šè¿‡ model-adapter è°ƒç”¨
- âœ… æ”¯æŒ chatï¼ˆéæµå¼ï¼‰
- âœ… æ”¯æŒ chat_streamï¼ˆæµå¼ï¼‰
- âœ… æ”¯æŒ embedding
- âœ… ç»Ÿä¸€é”™è¯¯å¤„ç†
- âœ… é…ç½®åŒ–ï¼ˆç¯å¢ƒå˜é‡ï¼‰

**ä½¿ç”¨ç¤ºä¾‹**:

```python
from common.llm_client import UnifiedLLMClient

# åˆå§‹åŒ–
client = UnifiedLLMClient(
    model_adapter_url="http://model-adapter:8005",
    default_model="gpt-3.5-turbo"
)

# èŠå¤©
response = await client.chat(
    messages=[{"role": "user", "content": "Hello"}],
    temperature=0.7
)
print(response["content"])

# ç”Ÿæˆï¼ˆç®€åŒ–ç‰ˆï¼‰
text = await client.generate(prompt="What is RAG?")

# Embedding
embeddings = await client.create_embedding(["text1", "text2"])
```

---

#### ç¬¬ 2 æ­¥ï¼šä¿®æ”¹ RAG Engineï¼ˆâ³ è¿›è¡Œä¸­ï¼‰

**éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶**:

1. **`algo/rag-engine/app/services/query_service.py`**

   - åˆ é™¤ï¼šç›´æ¥çš„ httpx è°ƒç”¨åˆ° OpenAI
   - æ”¹ä¸ºï¼šä½¿ç”¨ UnifiedLLMClient

2. **`algo/rag-engine/app/services/generator_service.py`**

   - åˆ é™¤ï¼šfrom openai import AsyncOpenAI
   - æ”¹ä¸ºï¼šä½¿ç”¨ UnifiedLLMClient

3. **`algo/rag-engine/app/core/rag_engine.py`**
   - ä¿®æ”¹ï¼šllm_client åˆå§‹åŒ–é€»è¾‘

**ä¿®æ”¹ç¤ºä¾‹**:

```python
# ä¿®æ”¹å‰ âŒ
from openai import AsyncOpenAI
client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
response = await client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[...]
)

# ä¿®æ”¹å âœ…
from common.llm_client import UnifiedLLMClient
client = UnifiedLLMClient()
response = await client.chat(
    messages=[...],
    model="gpt-3.5-turbo"
)
```

---

#### ç¬¬ 3 æ­¥ï¼šä¿®æ”¹ Agent Engine

**éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶**:

1. **`algo/agent-engine/app/services/llm_service.py`**

   ```python
   # ä¿®æ”¹å‰
   async with httpx.AsyncClient(timeout=self.timeout) as client:
       response = await client.post(
           "https://api.openai.com/v1/chat/completions",
           ...
       )

   # ä¿®æ”¹å
   from common.llm_client import UnifiedLLMClient
   llm_client = UnifiedLLMClient()
   response = await llm_client.chat(messages=[...])
   ```

2. **`algo/agent-engine/app/llm/ollama_client.py`**
   - å¦‚æœç”¨äº Ollama æœ¬åœ°æ¨¡å‹ï¼Œä¿ç•™
   - å¦‚æœç”¨äº OpenAIï¼Œæ”¹ä¸º UnifiedLLMClient

---

#### ç¬¬ 4 æ­¥ï¼šä¿®æ”¹ Multimodal Engine

**éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶**:

1. **`algo/multimodal-engine/app/core/vision_engine.py`**

   ```python
   # ä¿®æ”¹å‰
   async with httpx.AsyncClient(timeout=30.0) as client:
       response = await client.post(
           "https://api.openai.com/v1/chat/completions",
           json={
               "model": "gpt-4-vision-preview",
               ...
           }
       )

   # ä¿®æ”¹å
   from common.llm_client import UnifiedLLMClient
   vision_client = UnifiedLLMClient(default_model="gpt-4-vision-preview")
   response = await vision_client.chat(
       messages=[{
           "role": "user",
           "content": [
               {"type": "text", "text": "What's in this image?"},
               {"type": "image_url", "image_url": {"url": image_url}}
           ]
       }]
   )
   ```

---

#### ç¬¬ 5 æ­¥ï¼šç¯å¢ƒå˜é‡é…ç½®

åœ¨å„æœåŠ¡çš„`.env`æˆ–`docker-compose.yml`ä¸­æ·»åŠ ï¼š

```bash
# Model Adapteré…ç½®
MODEL_ADAPTER_URL=http://model-adapter:8005
DEFAULT_LLM_MODEL=gpt-3.5-turbo

# å¯é€‰ï¼šå¦‚æœmodel-adapteréœ€è¦API key
# OPENAI_API_KEY=sk-xxx  # åªåœ¨model-adapterä¸­é…ç½®
```

---

#### ç¬¬ 6 æ­¥ï¼šéªŒè¯ä¸æµ‹è¯•

**éªŒè¯å‘½ä»¤**:

```bash
# 1. æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç›´è¿OpenAIçš„ä»£ç 
cd /Users/lintao/important/ai-customer/VoiceAssistant
grep -r "from openai import" algo/rag-engine/ algo/agent-engine/ algo/multimodal-engine/

# åº”è¯¥è¿”å›ç©ºï¼Œæˆ–åªåœ¨åºŸå¼ƒæ–‡ä»¶ä¸­

# 2. æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç›´æ¥è°ƒç”¨openai.comçš„ä»£ç 
grep -r "api.openai.com" algo/

# åº”è¯¥è¿”å›ç©ºï¼Œæˆ–åªåœ¨model-adapterä¸­

# 3. æµ‹è¯•RAG Engine
cd algo/rag-engine
python -m pytest tests/test_llm_integration.py

# 4. æµ‹è¯•Agent Engine
cd algo/agent-engine
python -m pytest tests/test_llm_service.py
```

**é›†æˆæµ‹è¯•**:

```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d model-adapter rag-engine agent-engine

# æµ‹è¯•RAG
curl -X POST http://localhost:8006/api/v1/rag/generate \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is RAG?",
    "knowledge_base_id": "test_kb",
    "tenant_id": "test_tenant"
  }'

# æµ‹è¯•Agent
curl -X POST http://localhost:8003/api/v1/agent/execute \
  -H "Content-Type: application/json" \
  -d '{
    "task": "Calculate 25 * 4 + 10",
    "tools": ["calculator"]
  }'
```

---

## âœ… P0-2: ç»Ÿä¸€å‘é‡å­˜å‚¨è®¿é—®ï¼ˆ0.5 äººæ—¥ï¼‰

### ç›®æ ‡

æ‰€æœ‰æœåŠ¡å¿…é¡»é€šè¿‡`vector-store-adapter`è®¿é—®å‘é‡æ•°æ®åº“ï¼Œç¦æ­¢ç›´è¿ Milvusã€‚

### å®æ–½æ­¥éª¤

#### ç¬¬ 1 æ­¥ï¼šä¿®æ”¹ Retrieval Service

**æ–‡ä»¶**: `algo/retrieval-service/app/infrastructure/vector_store_client.py`

```python
# ä¿®æ”¹å‰ âŒ
from pymilvus import connections, Collection

class VectorStoreClient:
    def __init__(self, host="localhost", port=19530):
        connections.connect(host=host, port=port)
        self.collection = Collection("document_chunks")

    def search(self, query_vector, top_k=10):
        results = self.collection.search(
            data=[query_vector],
            anns_field="embedding",
            param={"metric_type": "IP", "params": {"nprobe": 10}},
            limit=top_k
        )
        return results
```

```python
# ä¿®æ”¹å âœ…
import httpx
import logging

logger = logging.getLogger(__name__)

class VectorStoreClient:
    """å‘é‡å­˜å‚¨å®¢æˆ·ç«¯ - é€šè¿‡vector-store-adapterè®¿é—®"""

    def __init__(
        self,
        adapter_url: str = "http://vector-store-adapter:8003",
        collection_name: str = "document_chunks",
        backend: str = "milvus",
        timeout: float = 30.0
    ):
        self.adapter_url = adapter_url.rstrip("/")
        self.collection_name = collection_name
        self.backend = backend
        self.timeout = timeout

        logger.info(
            f"VectorStoreClient initialized: {adapter_url}, "
            f"collection={collection_name}, backend={backend}"
        )

    async def search(
        self,
        query_vector: list[float],
        top_k: int = 10,
        tenant_id: str = None,
        filters: dict = None
    ) -> list[dict]:
        """
        å‘é‡æ£€ç´¢

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›ç»“æœæ•°
            tenant_id: ç§Ÿæˆ·IDï¼ˆç”¨äºè¿‡æ»¤ï¼‰
            filters: é¢å¤–è¿‡æ»¤æ¡ä»¶

        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.adapter_url}/collections/{self.collection_name}/search",
                    json={
                        "backend": self.backend,
                        "query_vector": query_vector,
                        "top_k": top_k,
                        "tenant_id": tenant_id,
                        "filters": filters
                    }
                )
                response.raise_for_status()
                result = response.json()

                logger.info(
                    f"Search returned {result.get('count', 0)} results "
                    f"via adapter service"
                )

                return result.get("results", [])

        except httpx.HTTPError as e:
            logger.error(f"Vector search error: {e}")
            raise RuntimeError(f"Vector search failed: {e}")

    async def insert_batch(self, data_list: list[dict]):
        """æ‰¹é‡æ’å…¥å‘é‡"""
        if not data_list:
            logger.warning("Empty data list for insertion")
            return

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.adapter_url}/collections/{self.collection_name}/insert",
                    json={
                        "backend": self.backend,
                        "data": data_list
                    }
                )
                response.raise_for_status()
                result = response.json()

                logger.info(f"Inserted {len(data_list)} vectors via adapter service")
                return result

        except httpx.HTTPError as e:
            logger.error(f"Vector insert error: {e}")
            raise
```

---

#### ç¬¬ 2 æ­¥ï¼šæ›´æ–°é…ç½®

**æ–‡ä»¶**: `algo/retrieval-service/.env`

```bash
# ä¿®æ”¹å‰
MILVUS_HOST=localhost
MILVUS_PORT=19530

# ä¿®æ”¹å
VECTOR_STORE_ADAPTER_URL=http://vector-store-adapter:8003
VECTOR_COLLECTION_NAME=document_chunks
VECTOR_BACKEND=milvus  # æˆ– pgvector
```

---

#### ç¬¬ 3 æ­¥ï¼šéªŒè¯

```bash
# 1. æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç›´è¿Milvus
cd /Users/lintao/important/ai-customer/VoiceAssistant
grep -r "from pymilvus import" algo/retrieval-service/
grep -r "connections.connect" algo/retrieval-service/

# åº”è¯¥è¿”å›ç©º

# 2. æµ‹è¯•retrieval-service
cd algo/retrieval-service
docker-compose up -d vector-store-adapter retrieval-service

# 3. æµ‹è¯•æ£€ç´¢
curl -X POST http://localhost:8012/api/v1/retrieval/hybrid \
  -H "Content-Type: application/json" \
  -d '{
    "query": "machine learning",
    "top_k": 5,
    "tenant_id": "test_tenant"
  }'
```

---

#### ç¬¬ 4 æ­¥ï¼šç½‘ç»œéš”ç¦»ï¼ˆå¯é€‰ä½†æ¨èï¼‰

åœ¨ K8s ä¸­ä½¿ç”¨ NetworkPolicy ç¦æ­¢ç›´è¿ Milvusï¼š

```yaml
# deployments/k8s/network-policy-milvus.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: milvus-access-control
  namespace: voiceassistant
spec:
  podSelector:
    matchLabels:
      app: milvus
  policyTypes:
    - Ingress
  ingress:
    # åªå…è®¸vector-store-adapterè®¿é—®
    - from:
        - podSelector:
            matchLabels:
              app: vector-store-adapter
      ports:
        - protocol: TCP
          port: 19530
```

åº”ç”¨ï¼š

```bash
kubectl apply -f deployments/k8s/network-policy-milvus.yaml
```

---

## ğŸ“Š P0 å®Œæˆæ£€æŸ¥æ¸…å•

### ä»£ç æ£€æŸ¥

- [ ] rag-engine ä¸å†ç›´è¿ OpenAI
- [ ] agent-engine ä¸å†ç›´è¿ OpenAI
- [ ] multimodal-engine ä¸å†ç›´è¿ OpenAI
- [ ] retrieval-service ä¸å†ç›´è¿ Milvus
- [ ] æ‰€æœ‰æœåŠ¡éƒ½ä½¿ç”¨ç»Ÿä¸€å®¢æˆ·ç«¯
- [ ] ç¯å¢ƒå˜é‡å·²é…ç½®

### æµ‹è¯•æ£€æŸ¥

- [ ] RAG Engine é›†æˆæµ‹è¯•é€šè¿‡
- [ ] Agent Engine é›†æˆæµ‹è¯•é€šè¿‡
- [ ] Retrieval Service æµ‹è¯•é€šè¿‡
- [ ] E2E æµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½æµ‹è¯•æ— æ˜æ˜¾åŠ£åŒ–

### æ–‡æ¡£æ£€æŸ¥

- [ ] README æ›´æ–°ï¼ˆè°ƒç”¨æ–¹å¼ï¼‰
- [ ] API æ–‡æ¡£æ›´æ–°
- [ ] ç¯å¢ƒå˜é‡æ–‡æ¡£æ›´æ–°
- [ ] æ¶æ„å›¾æ›´æ–°

### éƒ¨ç½²æ£€æŸ¥

- [ ] Docker é•œåƒé‡æ–°æ„å»º
- [ ] ç¯å¢ƒå˜é‡æ›´æ–°ï¼ˆK8s ConfigMapï¼‰
- [ ] ç°åº¦å‘å¸ƒè®¡åˆ’åˆ¶å®š
- [ ] å›æ»šæ–¹æ¡ˆå‡†å¤‡

---

## ğŸš¨ é£é™©ä¸ç¼“è§£

### é£é™© 1ï¼šå»¶è¿Ÿå¢åŠ 

**å½±å“**: é€šè¿‡ adapter å¯èƒ½å¢åŠ  10-20ms å»¶è¿Ÿ
**ç¼“è§£**:

- Adapter åšè½»é‡çº§è½¬å‘ï¼Œæ— é‡é€»è¾‘
- éƒ¨ç½²å¤šå‰¯æœ¬ï¼Œè´Ÿè½½å‡è¡¡
- ç›‘æ§å»¶è¿Ÿï¼ŒåŠæ—¶ä¼˜åŒ–

### é£é™© 2ï¼šAdapter å•ç‚¹æ•…éšœ

**å½±å“**: Adapter æŒ‚äº†æ‰€æœ‰ LLM è°ƒç”¨å¤±è´¥
**ç¼“è§£**:

- éƒ¨ç½² 3 ä¸ªä»¥ä¸Šå‰¯æœ¬
- å¢åŠ å¥åº·æ£€æŸ¥å’Œè‡ªåŠ¨é‡å¯
- è®¾ç½®ç†”æ–­å™¨ï¼ˆCircuit Breakerï¼‰
- å‡†å¤‡å¿«é€Ÿå›æ»šæ–¹æ¡ˆ

### é£é™© 3ï¼šå…¼å®¹æ€§é—®é¢˜

**å½±å“**: ä¿®æ”¹åå¯èƒ½å‡ºç°æ¥å£ä¸å…¼å®¹
**ç¼“è§£**:

- å……åˆ†çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
- ç°åº¦å‘å¸ƒï¼Œé€æ­¥åˆ‡æ¢
- ä¿ç•™æ—§ä»£ç ä½œä¸º fallback
- ç›‘æ§é”™è¯¯ç‡ï¼Œå¼‚å¸¸ç«‹å³å›æ»š

---

## ğŸ“ˆ æˆåŠŸæŒ‡æ ‡

### æŠ€æœ¯æŒ‡æ ‡

```yaml
- LLMè°ƒç”¨è§„èŒƒæ€§: 100%ï¼ˆæ— ç›´è¿ï¼‰
- å‘é‡å­˜å‚¨è®¿é—®è§„èŒƒæ€§: 100%ï¼ˆé€šè¿‡adapterï¼‰
- P95å»¶è¿Ÿ: <520msï¼ˆå¢åŠ <20msï¼‰
- é”™è¯¯ç‡: <0.1%
- æµ‹è¯•è¦†ç›–ç‡: >80
```

### ä¸šåŠ¡æŒ‡æ ‡

```yaml
- RAGç”ŸæˆæˆåŠŸç‡: >99
- Agentæ‰§è¡ŒæˆåŠŸç‡: >98
- Tokenæˆæœ¬è¿½è¸ª: 100%å¯è§
- æ¨¡å‹åˆ‡æ¢æ—¶é—´: <2å°æ—¶ï¼ˆfrom 2å¤©ï¼‰
```

---

## ğŸ“ æ”¯æŒä¸è”ç³»

**é‡åˆ°é—®é¢˜ï¼Ÿ**

- Slack é¢‘é“: #architecture-refactor
- æ–‡æ¡£: docs/ARCHITECTURE_REVIEW.md
- è´Ÿè´£äºº: @æ¶æ„ç»„

**è¿›åº¦è¿½è¸ª**:

- ä»»åŠ¡çœ‹æ¿: [Jira/GitHub Projects]
- æ—¥æŠ¥: æ¯æ—¥ç«™ä¼š
- å‘¨æŠ¥: æ¯å‘¨äº”å‘é€

---

**åˆ›å»ºæ—¶é—´**: 2025-10-27
**æ›´æ–°æ—¶é—´**: 2025-10-27
**çŠ¶æ€**: ğŸ”„ è¿›è¡Œä¸­
**è´Ÿè´£äºº**: å¾…åˆ†é…
