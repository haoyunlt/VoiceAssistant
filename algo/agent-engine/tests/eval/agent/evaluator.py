"""
Agent Evaluator - Agent è¯„æµ‹å™¨

åŠŸèƒ½:
- è¿è¡Œæµ‹è¯•ç”¨ä¾‹å¹¶è®°å½•æ‰§è¡Œè½¨è¿¹
- è®¡ç®—å¤šç»´åº¦è¯„æµ‹æŒ‡æ ‡
- ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š
- æ”¯æŒ LLM-as-Judge è¯„ä¼°
"""

import json
import logging
import time
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Any

logger = logging.getLogger(__name__)


@dataclass
class TestCase:
    """æµ‹è¯•ç”¨ä¾‹"""

    id: str
    category: str  # simple_calculation, knowledge_retrieval, multi_step_reasoning, tool_composition
    task: str
    expected_answer: str | None = None
    expected_tools: list[str] | None = None
    max_steps: int = 10
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class EvaluationResult:
    """è¯„æµ‹ç»“æœ"""

    test_case_id: str
    task: str
    mode: str

    # æ‰§è¡Œç»“æœ
    success: bool
    final_answer: str
    execution_time_ms: float
    step_count: int
    tool_calls: list[str]

    # è¯„æµ‹æŒ‡æ ‡
    answer_correctness: float | None = None  # 0-1, LLM-as-Judge è¯„åˆ†
    tool_accuracy: float | None = None  # å·¥å…·é€‰æ‹©å‡†ç¡®ç‡
    cost_usd: float | None = None

    # é”™è¯¯ä¿¡æ¯
    error: str | None = None

    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


class AgentEvaluator:
    """
    Agent è¯„æµ‹å™¨

    ç”¨æ³•:
        evaluator = AgentEvaluator(agent_engine)

        # åŠ è½½æµ‹è¯•ç”¨ä¾‹
        test_cases = evaluator.load_test_cases("tests/eval/agent/datasets/benchmark.json")

        # è¿è¡Œè¯„æµ‹
        results = await evaluator.evaluate(
            test_cases=test_cases,
            modes=["react", "plan_execute"]
        )

        # ç”ŸæˆæŠ¥å‘Š
        report = evaluator.generate_report(results)
        evaluator.save_report(report, "reports/evaluation_result.json")
    """

    def __init__(
        self,
        agent_engine: Any,  # AgentEngine å®ä¾‹
        llm_judge_model: str = "gpt-4",
        enable_llm_judge: bool = True,
    ):
        """
        åˆå§‹åŒ–è¯„æµ‹å™¨

        Args:
            agent_engine: Agent Engine å®ä¾‹
            llm_judge_model: LLM-as-Judge ä½¿ç”¨çš„æ¨¡å‹
            enable_llm_judge: æ˜¯å¦å¯ç”¨ LLM-as-Judge
        """
        self.agent_engine = agent_engine
        self.llm_judge_model = llm_judge_model
        self.enable_llm_judge = enable_llm_judge

        logger.info(
            f"AgentEvaluator initialized (LLM Judge: {llm_judge_model if enable_llm_judge else 'disabled'})"
        )

    def load_test_cases(self, dataset_path: str) -> list[TestCase]:
        """
        åŠ è½½æµ‹è¯•ç”¨ä¾‹

        Args:
            dataset_path: æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼ˆJSONï¼‰

        Returns:
            æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
        """
        path = Path(dataset_path)
        if not path.exists():
            logger.error(f"Dataset file not found: {dataset_path}")
            return []

        with open(path, encoding="utf-8") as f:
            data = json.load(f)

        test_cases = []
        for item in data:
            test_cases.append(TestCase(**item))

        logger.info(f"Loaded {len(test_cases)} test cases from {dataset_path}")
        return test_cases

    async def evaluate(
        self, test_cases: list[TestCase], modes: list[str] = None, save_traces: bool = True
    ) -> list[EvaluationResult]:
        """
        è¿è¡Œè¯„æµ‹

        Args:
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
            modes: æ‰§è¡Œæ¨¡å¼åˆ—è¡¨ï¼ˆé»˜è®¤: ["react"]ï¼‰
            save_traces: æ˜¯å¦ä¿å­˜æ‰§è¡Œè½¨è¿¹

        Returns:
            è¯„æµ‹ç»“æœåˆ—è¡¨
        """
        if modes is None:
            modes = ["react"]

        results = []
        total = len(test_cases) * len(modes)
        completed = 0

        logger.info(
            f"Starting evaluation: {len(test_cases)} cases Ã— {len(modes)} modes = {total} runs"
        )

        for mode in modes:
            for test_case in test_cases:
                completed += 1
                logger.info(f"[{completed}/{total}] Running {test_case.id} with {mode}")

                try:
                    result = await self._evaluate_single(test_case, mode, save_traces)
                    results.append(result)
                except Exception as e:
                    logger.error(f"Error evaluating {test_case.id}: {e}", exc_info=True)

                    # åˆ›å»ºå¤±è´¥ç»“æœ
                    results.append(
                        EvaluationResult(
                            test_case_id=test_case.id,
                            task=test_case.task,
                            mode=mode,
                            success=False,
                            final_answer="",
                            execution_time_ms=0,
                            step_count=0,
                            tool_calls=[],
                            error=str(e),
                        )
                    )

        logger.info(f"Evaluation completed: {len(results)} results")
        return results

    async def _evaluate_single(
        self, test_case: TestCase, mode: str, save_traces: bool
    ) -> EvaluationResult:
        """è¯„æµ‹å•ä¸ªç”¨ä¾‹"""
        start_time = time.time()

        # æ‰§è¡Œä»»åŠ¡
        try:
            result = await self.agent_engine.execute(
                task=test_case.task,
                mode=mode,
                max_steps=test_case.max_steps,
            )

            success = True
            final_answer = result.get("final_answer", "")
            step_count = result.get("step_count", 0)
            tool_calls = result.get("tool_calls", [])
            error = None

        except Exception as e:
            success = False
            final_answer = ""
            step_count = 0
            tool_calls = []
            error = str(e)

        execution_time_ms = (time.time() - start_time) * 1000

        # è®¡ç®—å·¥å…·é€‰æ‹©å‡†ç¡®ç‡
        tool_accuracy = None
        if test_case.expected_tools:
            tool_accuracy = self._compute_tool_accuracy(tool_calls, test_case.expected_tools)

        # LLM-as-Judge è¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§
        answer_correctness = None
        if self.enable_llm_judge and test_case.expected_answer and success:
            answer_correctness = await self._judge_answer_correctness(
                task=test_case.task,
                expected_answer=test_case.expected_answer,
                actual_answer=final_answer,
            )

        # ä¼°ç®—æˆæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”ä»è¿½è¸ªå™¨è·å–ï¼‰
        cost_usd = result.get("cost_usd", 0.0)

        return EvaluationResult(
            test_case_id=test_case.id,
            task=test_case.task,
            mode=mode,
            success=success,
            final_answer=final_answer,
            execution_time_ms=execution_time_ms,
            step_count=step_count,
            tool_calls=tool_calls,
            answer_correctness=answer_correctness,
            tool_accuracy=tool_accuracy,
            cost_usd=cost_usd,
            error=error,
        )

    def _compute_tool_accuracy(self, actual_tools: list[str], expected_tools: list[str]) -> float:
        """
        è®¡ç®—å·¥å…·é€‰æ‹©å‡†ç¡®ç‡

        ä½¿ç”¨ Jaccard ç›¸ä¼¼åº¦: |intersection| / |union|
        """
        if not expected_tools:
            return 1.0 if not actual_tools else 0.0

        actual_set = set(actual_tools)
        expected_set = set(expected_tools)

        intersection = actual_set & expected_set
        union = actual_set | expected_set

        if not union:
            return 1.0

        return len(intersection) / len(union)

    async def _judge_answer_correctness(
        self, task: str, expected_answer: str, actual_answer: str
    ) -> float:
        """
        LLM-as-Judge: è¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§

        Returns:
            0-1 çš„è¯„åˆ†
        """
        try:
            # æ„å»ºè¯„ä¼° Prompt
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„ç­”æ¡ˆè¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚

ä»»åŠ¡: {task}

å‚è€ƒç­”æ¡ˆ: {expected_answer}

å®é™…ç­”æ¡ˆ: {actual_answer}

è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¯„åˆ†ï¼ˆ0-1ï¼‰:
- 1.0: å®Œå…¨æ­£ç¡®ï¼Œå†…å®¹å‡†ç¡®ä¸”å®Œæ•´
- 0.8: åŸºæœ¬æ­£ç¡®ï¼Œæœ‰å¾®å°ç‘•ç–µ
- 0.6: éƒ¨åˆ†æ­£ç¡®ï¼Œç¼ºå°‘å…³é”®ä¿¡æ¯
- 0.4: æœ‰ç›¸å…³å†…å®¹ï¼Œä½†ä¸å¤Ÿå‡†ç¡®
- 0.2: ç­”éæ‰€é—®
- 0.0: å®Œå…¨é”™è¯¯

åªè¾“å‡ºä¸€ä¸ª 0-1 ä¹‹é—´çš„æ•°å­—ï¼Œä¸è¦è§£é‡Šã€‚"""

            # è°ƒç”¨ LLMï¼ˆè¿™é‡Œéœ€è¦å®é™…çš„ LLM å®¢æˆ·ç«¯ï¼‰
            # ç®€åŒ–å®ç°ï¼šä½¿ç”¨ agent_engine çš„ llm_client
            if hasattr(self.agent_engine, "llm_client"):
                response = await self.agent_engine.llm_client.chat(
                    messages=[{"role": "user", "content": prompt}],
                    model=self.llm_judge_model,
                    temperature=0.0,
                )

                # è§£æè¯„åˆ†
                score_str = response.strip()
                score = float(score_str)
                return max(0.0, min(1.0, score))  # é™åˆ¶åœ¨ [0, 1]

            # é™çº§ï¼šç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…
            if expected_answer.lower() in actual_answer.lower():
                return 0.8
            return 0.0

        except Exception as e:
            logger.error(f"Error in LLM-as-Judge: {e}")
            return 0.0

    def generate_report(self, results: list[EvaluationResult]) -> dict:
        """
        ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š

        Args:
            results: è¯„æµ‹ç»“æœåˆ—è¡¨

        Returns:
            æŠ¥å‘Šå­—å…¸
        """
        from .metrics import compute_metrics

        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        overall_metrics = compute_metrics(results)

        # æŒ‰æ¨¡å¼åˆ†ç»„ç»Ÿè®¡
        mode_metrics = {}
        for mode in {r.mode for r in results}:
            mode_results = [r for r in results if r.mode == mode]
            mode_metrics[mode] = compute_metrics(mode_results)

        # æŒ‰ç±»åˆ«åˆ†ç»„ç»Ÿè®¡
        category_metrics = {}
        test_cases_by_category = {}
        for result in results:
            # ä» test_case_id æå–ç±»åˆ«ï¼ˆå‡è®¾æ ¼å¼ä¸º "category_xxx"ï¼‰
            category = result.test_case_id.split("_")[0]
            if category not in category_metrics:
                category_metrics[category] = []
                test_cases_by_category[category] = []

            category_metrics[category].append(result)
            test_cases_by_category[category].append(result.test_case_id)

        for category in category_metrics:
            category_metrics[category] = compute_metrics(category_metrics[category])

        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "summary": {
                "total_cases": len(results),
                "timestamp": time.time(),
                "modes": list(mode_metrics.keys()),
            },
            "overall_metrics": overall_metrics,
            "mode_metrics": mode_metrics,
            "category_metrics": category_metrics,
            "detailed_results": [r.to_dict() for r in results],
        }

        logger.info(
            f"Generated evaluation report: success_rate={overall_metrics['success_rate']:.2%}"
        )

        return report

    def save_report(self, report: dict, output_path: str):
        """
        ä¿å­˜è¯„æµ‹æŠ¥å‘Š

        Args:
            report: æŠ¥å‘Šå­—å…¸
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        path = Path(output_path)
        path.parent.mkdir(parents=True, exist_ok=True)

        with open(path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        logger.info(f"Evaluation report saved to {output_path}")

    def print_summary(self, report: dict):
        """æ‰“å°è¯„æµ‹æ‘˜è¦"""
        metrics = report["overall_metrics"]

        print("\n" + "=" * 60)
        print("ğŸ“Š Agent Evaluation Report")
        print("=" * 60)
        print(f"Total Test Cases: {report['summary']['total_cases']}")
        print(f"Modes: {', '.join(report['summary']['modes'])}")
        print("\n--- Overall Metrics ---")
        print(f"Success Rate:           {metrics['success_rate']:.2%}")
        print(f"Avg Steps:              {metrics['avg_steps']:.1f}")
        print(f"Avg Execution Time:     {metrics['avg_execution_time_ms']:.0f} ms")
        print(f"Avg Cost:               ${metrics['avg_cost_usd']:.4f}")

        if metrics.get("avg_answer_correctness") is not None:
            print(f"Avg Answer Correctness: {metrics['avg_answer_correctness']:.2f}")
        if metrics.get("avg_tool_accuracy") is not None:
            print(f"Avg Tool Accuracy:      {metrics['avg_tool_accuracy']:.2%}")

        print("\n--- Mode Comparison ---")
        for mode, mode_metrics in report["mode_metrics"].items():
            print(f"\n{mode}:")
            print(f"  Success Rate: {mode_metrics['success_rate']:.2%}")
            print(f"  Avg Steps:    {mode_metrics['avg_steps']:.1f}")
            print(f"  Avg Time:     {mode_metrics['avg_execution_time_ms']:.0f} ms")

        print("\n" + "=" * 60)
