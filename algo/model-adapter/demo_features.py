"""æ¼”ç¤ºæ–°åŠŸèƒ½çš„è„šæœ¬."""

import asyncio


async def demo_resilience():
    """æ¼”ç¤ºé‡è¯•å’Œç†”æ–­åŠŸèƒ½."""
    print("\n" + "=" * 60)
    print("ğŸ”¥ Demo 1: é‡è¯•å’Œç†”æ–­å™¨")
    print("=" * 60)

    from app.core.resilience import get_circuit_breaker, with_resilience

    # æ¨¡æ‹Ÿå¯èƒ½å¤±è´¥çš„APIè°ƒç”¨
    call_count = 0

    @with_resilience("demo_provider", max_attempts=3, timeout=2.0)
    async def flaky_api_call():
        nonlocal call_count
        call_count += 1
        print(f"  ğŸ”„ å°è¯• #{call_count}...")

        if call_count < 2:
            raise Exception("æ¨¡æ‹ŸAPIå¤±è´¥")

        return {"result": "success"}

    try:
        result = await flaky_api_call()
        print(f"  âœ… æˆåŠŸ: {result}")
    except Exception as e:
        print(f"  âŒ å¤±è´¥: {e}")

    # æŸ¥çœ‹ç†”æ–­å™¨çŠ¶æ€
    breaker = get_circuit_breaker("demo_provider")
    print(f"  ğŸ“Š ç†”æ–­å™¨çŠ¶æ€: {breaker.current_state}")


async def demo_cache():
    """æ¼”ç¤ºç¼“å­˜åŠŸèƒ½."""
    print("\n" + "=" * 60)
    print("âš¡ Demo 2: è¯­ä¹‰ç¼“å­˜")
    print("=" * 60)

    from app.core.cache import SemanticCache

    cache = SemanticCache(
        redis_url="redis://localhost:6379/0",
        default_ttl=120,
        enabled=True,
    )

    await cache.connect()

    messages = [{"role": "user", "content": "Hello, how are you?"}]

    # ç¬¬ä¸€æ¬¡è¯·æ±‚ - ç¼“å­˜æœªå‘½ä¸­
    cached = await cache.get("openai", "gpt-3.5-turbo", messages)
    print(f"  ğŸ” ç¬¬ä¸€æ¬¡æŸ¥è¯¢: {'å‘½ä¸­' if cached else 'æœªå‘½ä¸­'} âŒ")

    # è®¾ç½®ç¼“å­˜
    await cache.set(
        "openai",
        "gpt-3.5-turbo",
        messages,
        {"content": "I'm doing great!", "usage": {"total_tokens": 20}},
    )
    print("  ğŸ’¾ å·²ç¼“å­˜å“åº”")

    # ç¬¬äºŒæ¬¡è¯·æ±‚ - ç¼“å­˜å‘½ä¸­
    cached = await cache.get("openai", "gpt-3.5-turbo", messages)
    print(f"  ğŸ” ç¬¬äºŒæ¬¡æŸ¥è¯¢: {'å‘½ä¸­' if cached else 'æœªå‘½ä¸­'} âœ…")

    if cached:
        print(f"  ğŸ“„ ç¼“å­˜å†…å®¹: {cached['content']}")

    # ç»Ÿè®¡ä¿¡æ¯
    stats = await cache.get_stats()
    print(f"  ğŸ“Š ç¼“å­˜ç»Ÿè®¡: {stats}")

    await cache.close()


def demo_token_counter():
    """æ¼”ç¤ºTokenè®¡æ•°."""
    print("\n" + "=" * 60)
    print("ğŸ”¢ Demo 3: Tokenè®¡æ•° (tiktoken)")
    print("=" * 60)

    from app.core.token_counter import CostCalculator, TokenCounter

    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "è¯·ç”¨ä¸­æ–‡ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²ã€‚"},
    ]

    # Tokenè®¡æ•°
    tokens = TokenCounter.count_message_tokens(messages, model="gpt-4")
    print(f"  ğŸ“Š æ€»Tokenæ•°: {tokens}")

    # æˆæœ¬è®¡ç®—
    cost = CostCalculator.calculate_cost(
        model="gpt-4",
        input_tokens=tokens,
        output_tokens=500,
    )

    print(f"  ğŸ’° è¾“å…¥æˆæœ¬: ${cost['input_cost']:.6f}")
    print(f"  ğŸ’° è¾“å‡ºæˆæœ¬: ${cost['output_cost']:.6f}")
    print(f"  ğŸ’° æ€»æˆæœ¬: ${cost['total_cost']:.6f}")

    # æ¨¡å‹å¯¹æ¯”
    models = ["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo", "gpt-4o"]
    comparison = CostCalculator.compare_model_costs(models, tokens, 500)

    print("\n  ğŸ“Š æ¨¡å‹æˆæœ¬å¯¹æ¯”:")
    for model, cost_data in comparison.items():
        print(f"    {model:20s}: ${cost_data['total_cost']:.6f}")

    # æœ€ä¾¿å®œçš„æ¨¡å‹
    cheapest_model, cheapest_cost = CostCalculator.get_cheapest_model(models, tokens, 500)
    print(f"\n  ğŸ† æœ€ä¾¿å®œ: {cheapest_model} (${cheapest_cost['total_cost']:.6f})")


def demo_smart_router():
    """æ¼”ç¤ºæ™ºèƒ½è·¯ç”±."""
    print("\n" + "=" * 60)
    print("ğŸ¯ Demo 4: æ™ºèƒ½è·¯ç”±")
    print("=" * 60)

    from app.core.smart_router import SmartRouter

    router = SmartRouter()

    messages = [{"role": "user", "content": "What is 2+2?"}]

    # ä½è´¨é‡è¦æ±‚ - åº”è¯¥é€‰æ‹©ä¾¿å®œæ¨¡å‹
    result = router.route(
        quality_level="low",
        messages=messages,
        available_providers=["openai", "zhipu"],
    )

    print("  ğŸ“ è´¨é‡è¦æ±‚: low")
    print(f"  âœ… é€‰æ‹©æ¨¡å‹: {result.provider}/{result.model}")
    print(f"  ğŸ’° é¢„ä¼°æˆæœ¬: ${result.estimated_cost:.6f}")

    # é«˜è´¨é‡è¦æ±‚ - åº”è¯¥é€‰æ‹©é«˜æ€§èƒ½æ¨¡å‹
    result = router.route(
        quality_level="high",
        messages=messages,
        available_providers=["openai", "claude"],
    )

    print("\n  ğŸ“ è´¨é‡è¦æ±‚: high")
    print(f"  âœ… é€‰æ‹©æ¨¡å‹: {result.provider}/{result.model}")
    print(f"  ğŸ’° é¢„ä¼°æˆæœ¬: ${result.estimated_cost:.6f}")

    # é¢„ç®—è·¯ç”±
    result = router.route_with_budget(
        messages=messages,
        budget_usd=0.01,
        available_providers=["openai"],
    )

    print("\n  ğŸ’µ é¢„ç®—é™åˆ¶: $0.01")
    print(f"  âœ… é€‰æ‹©æ¨¡å‹: {result.provider}/{result.model}")
    print(f"  ğŸ’° é¢„ä¼°æˆæœ¬: ${result.estimated_cost:.6f}")


def demo_budget_manager():
    """æ¼”ç¤ºé¢„ç®—ç®¡ç†."""
    print("\n" + "=" * 60)
    print("ğŸ’µ Demo 5: é¢„ç®—ç®¡ç†")
    print("=" * 60)

    from app.core.budget_manager import get_budget_manager

    manager = get_budget_manager()

    # è®¾ç½®é¢„ç®—
    manager.set_budget(
        tenant_id="demo_tenant",
        daily_limit_usd=100.0,
        warning_threshold=0.8,
        auto_downgrade=True,
    )
    print("  âš™ï¸  å·²è®¾ç½®é¢„ç®—: $100/å¤©, è­¦å‘Šé˜ˆå€¼80%")

    # æ¨¡æ‹Ÿä½¿ç”¨
    print("\n  ğŸ“Š æ¨¡æ‹Ÿä½¿ç”¨:")
    for i in range(5):
        usage = 18.5
        manager.record_usage("demo_tenant", usage)
        status = manager.get_status("demo_tenant")

        emoji = "ğŸŸ¢" if status.usage_percent < 0.8 else "ğŸŸ¡" if status.usage_percent < 1.0 else "ğŸ”´"
        print(
            f"    ç¬¬{i + 1}æ¬¡: +${usage:.2f} â†’ ${status.current_usage_usd:.2f} / ${status.daily_limit_usd:.2f} ({status.usage_percent * 100:.1f}%) {emoji}"
        )

        if status.is_exceeded:
            print(f"      âš ï¸  è¶…é¢„ç®—! è‡ªåŠ¨é™çº§: {status.auto_downgraded}")
            break

    # æ£€æŸ¥æ˜¯å¦åº”é™çº§
    should_downgrade, quality = manager.should_downgrade("demo_tenant")
    if should_downgrade:
        print(f"\n  â¬‡ï¸  åº”é™çº§åˆ°: {quality}")


def demo_metrics():
    """æ¼”ç¤ºæŒ‡æ ‡æ”¶é›†."""
    print("\n" + "=" * 60)
    print("ğŸ“Š Demo 6: PrometheusæŒ‡æ ‡")
    print("=" * 60)

    from app.core.metrics import MetricsRecorder

    # è®°å½•ä¸€æ¬¡è¯·æ±‚
    MetricsRecorder.record_request(
        provider="openai",
        model="gpt-4",
        duration_seconds=1.23,
        input_tokens=1234,
        output_tokens=567,
        cost_usd=0.071,
        success=True,
    )
    print("  âœ… å·²è®°å½•è¯·æ±‚æŒ‡æ ‡")

    # è®°å½•ç¼“å­˜
    MetricsRecorder.record_cache_hit("openai", "gpt-4")
    MetricsRecorder.record_cache_miss("openai", "gpt-4")
    print("  âœ… å·²è®°å½•ç¼“å­˜æŒ‡æ ‡")

    # æ›´æ–°ç†”æ–­å™¨çŠ¶æ€
    MetricsRecorder.update_circuit_breaker_state("openai", "closed")
    print("  âœ… å·²æ›´æ–°ç†”æ–­å™¨æŒ‡æ ‡")

    print("\n  ğŸ“ è®¿é—®æŒ‡æ ‡: http://localhost:8005/metrics")
    print("  ğŸ“ å…³é”®æŒ‡æ ‡:")
    print("    - model_request_duration_seconds (P50/P95/P99)")
    print("    - cache_hit_rate")
    print("    - circuit_breaker_state")
    print("    - cost_usd_total")


async def main():
    """ä¸»å‡½æ•°."""
    print("\n" + "=" * 80)
    print(" ğŸš€ Model Adapter å¢å¼ºåŠŸèƒ½æ¼”ç¤º")
    print("=" * 80)

    await demo_resilience()
    await demo_cache()
    demo_token_counter()
    demo_smart_router()
    demo_budget_manager()
    demo_metrics()

    print("\n" + "=" * 80)
    print(" âœ… æ¼”ç¤ºå®Œæˆ!")
    print("=" * 80)
    print("\nğŸ“š æ›´å¤šä¿¡æ¯:")
    print("  - å®ç°æ€»ç»“: IMPLEMENTATION_SUMMARY.md")
    print("  - å®Œæ•´æ–‡æ¡£: OPTIMIZATION.md")
    print("  - APIæ–‡æ¡£: http://localhost:8005/docs")
    print("  - æŒ‡æ ‡: http://localhost:8005/metrics")
    print("")


if __name__ == "__main__":
    asyncio.run(main())
