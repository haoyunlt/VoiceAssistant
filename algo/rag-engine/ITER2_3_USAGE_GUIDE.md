# RAG Engine v2.0 ä½¿ç”¨æŒ‡å—

> **Iter 2 & 3 åŠŸèƒ½ï¼šGraph RAG + Query Decomposition + Self-RAG + Context Compression**

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
cd algo/rag-engine

# åŸºç¡€ä¾èµ–ï¼ˆå¿…éœ€ï¼‰
pip install -r requirements.txt

# å¯é€‰ï¼šNeo4j é©±åŠ¨ï¼ˆä¼ä¸šçº§å›¾è°±ï¼‰
pip install neo4j==5.15.0

# å¯é€‰ï¼šTransformersï¼ˆNLI å¹»è§‰æ£€æµ‹ï¼‰
pip install transformers==4.36.0

# å¯é€‰ï¼šLLMLinguaï¼ˆé«˜è´¨é‡å‹ç¼©ï¼‰
pip install llmlingua==0.2.0
```

### 2. å¯åŠ¨ Redisï¼ˆç¼“å­˜ï¼‰

```bash
docker run -d -p 6379:6379 redis:7-alpine
```

### 3. å¯é€‰ï¼šå¯åŠ¨ Neo4jï¼ˆå›¾è°±ï¼‰

```bash
docker run -d \
  -p 7474:7474 -p 7687:7687 \
  -e NEO4J_AUTH=neo4j/password \
  neo4j:5.15-community
```

### 4. è¿è¡ŒæœåŠ¡

```bash
# å¼€å‘æ¨¡å¼
make run

# æˆ–
python main.py
```

---

## ğŸ“– åŸºç¡€ç”¨æ³•

### åœºæ™¯ 1: ç®€å•é—®ç­”ï¼ˆä½¿ç”¨æ‰€æœ‰ä¼˜åŒ–ï¼‰

```python
import asyncio
from app.services.ultimate_rag_service import get_ultimate_rag_service

async def main():
    # åˆå§‹åŒ–æœåŠ¡ï¼ˆå¯ç”¨æ‰€æœ‰åŠŸèƒ½ï¼‰
    service = get_ultimate_rag_service(
        retrieval_client=retrieval_client,
        llm_client=llm_client,
        enable_hybrid=True,          # Iter 1: æ··åˆæ£€ç´¢
        enable_rerank=True,          # Iter 1: é‡æ’
        enable_cache=True,           # Iter 1: è¯­ä¹‰ç¼“å­˜
        enable_graph=True,           # Iter 2: å›¾è°±æ£€ç´¢
        enable_decomposition=True,   # Iter 2: æŸ¥è¯¢åˆ†è§£
        enable_self_rag=True,        # Iter 3: è‡ªæˆ‘çº é”™
        enable_compression=True,     # Iter 3: ä¸Šä¸‹æ–‡å‹ç¼©
        compression_ratio=0.5,
    )

    # æŸ¥è¯¢
    result = await service.query(
        query="ä»€ä¹ˆæ˜¯RAGï¼Ÿå®ƒå¦‚ä½•å·¥ä½œï¼Ÿ",
        tenant_id="user123",
        top_k=5,
        temperature=0.7,
    )

    # æ‰“å°ç»“æœ
    print("ç­”æ¡ˆ:", result["answer"])
    print("ä½¿ç”¨çš„ç­–ç•¥:", result["strategy"])
    print("å¯ç”¨çš„åŠŸèƒ½:", result["features_used"])
    print("Self-RAG ä¿¡æ¯:", result["self_rag"])
    print("å‹ç¼©ä¿¡æ¯:", result["compression"])

asyncio.run(main())
```

### åœºæ™¯ 2: å¤æ‚å¤šè·³æ¨ç†ï¼ˆå¼ºè°ƒå›¾è°±ï¼‰

```python
result = await service.query(
    query="å¼ ä¸‰è®¤è¯†æå››ï¼Œæå››è®¤è¯†ç‹äº”ï¼Œé‚£ä¹ˆå¼ ä¸‰å’Œç‹äº”ä¹‹é—´æ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ",
    tenant_id="user123",
    top_k=5,
    use_graph=True,           # å¯ç”¨å›¾è°±æ£€ç´¢
    use_decomposition=False,  # ä¸éœ€è¦åˆ†è§£
    use_self_rag=True,        # å¯ç”¨è‡ªæˆ‘çº é”™
    use_compression=False,    # ä¸å‹ç¼©ï¼ˆä¿ç•™å®Œæ•´ä¿¡æ¯ï¼‰
)

# ç»“æœä¼šåŒ…å«å›¾è°±è·¯å¾„
print("æ£€ç´¢ç­–ç•¥:", result["strategy"])  # graph_enhanced
if "entities" in result:
    print("è¯†åˆ«çš„å®ä½“:", result["entities"])
if "graph_results_count" in result:
    print("å›¾è°±ç»“æœæ•°:", result["graph_results_count"])
```

### åœºæ™¯ 3: å¤æ‚ç»„åˆæŸ¥è¯¢ï¼ˆä½¿ç”¨æŸ¥è¯¢åˆ†è§£ï¼‰

```python
result = await service.query(
    query="å¯¹æ¯”Pythonå’ŒJavaçš„æ€§èƒ½å·®å¼‚ï¼Œå¹¶è¯´æ˜å®ƒä»¬å„è‡ªçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿",
    tenant_id="user123",
    top_k=5,
    use_graph=False,
    use_decomposition=True,  # å¯ç”¨æŸ¥è¯¢åˆ†è§£
    use_self_rag=True,
    use_compression=True,
)

# ç»“æœä¼šåŒ…å«å­æŸ¥è¯¢ä¿¡æ¯
if result.get("strategy") == "query_decomposition":
    print("å­æŸ¥è¯¢:", result["sub_queries"])
    print("å­ç­”æ¡ˆ:", result["sub_answers"])
```

### åœºæ™¯ 4: é•¿ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆå¼ºè°ƒå‹ç¼©ï¼‰

```python
result = await service.query(
    query="æ€»ç»“è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè§‚ç‚¹",
    tenant_id="user123",
    top_k=10,  # æ£€ç´¢æ›´å¤šæ–‡æ¡£
    use_graph=False,
    use_decomposition=False,
    use_self_rag=False,
    use_compression=True,     # å¯ç”¨å‹ç¼©
    compression_ratio=0.3,    # é«˜å‹ç¼©ç‡ï¼ˆ70% Token èŠ‚çœï¼‰
)

# æŸ¥çœ‹å‹ç¼©æ•ˆæœ
print("Token èŠ‚çœ:", result["compression"]["tokens_saved"])
print("å‹ç¼©ç‡:", result["compression"]["ratio"])
```

### åœºæ™¯ 5: é«˜è´¨é‡å›ç­”ï¼ˆå¼ºè°ƒ Self-RAGï¼‰

```python
result = await service.query(
    query="è§£é‡Šé‡å­åŠ›å­¦çš„ä¸ç¡®å®šæ€§åŸç†",
    tenant_id="user123",
    top_k=5,
    use_graph=False,
    use_decomposition=False,
    use_self_rag=True,       # å¯ç”¨è‡ªæˆ‘çº é”™
    use_compression=False,
    temperature=0.3,          # ä½æ¸©åº¦ï¼Œæ›´ç²¾ç¡®
)

# æŸ¥çœ‹ Self-RAG ç»“æœ
print("æ˜¯å¦æœ‰å¹»è§‰:", result["self_rag"]["has_hallucination"])
print("ç½®ä¿¡åº¦:", result["self_rag"]["confidence"])
print("ä¿®æ­£æ¬¡æ•°:", result["self_rag"]["attempts"])
```

---

## ğŸ”§ é«˜çº§é…ç½®

### 1. è‡ªå®šä¹‰å›¾è°±åç«¯

#### ä½¿ç”¨ NetworkXï¼ˆé»˜è®¤ï¼‰

```python
service = get_ultimate_rag_service(
    retrieval_client=retrieval_client,
    llm_client=llm_client,
    enable_graph=True,
    graph_backend="networkx",  # è½»é‡çº§ï¼Œé€‚åˆå¼€å‘
)
```

#### ä½¿ç”¨ Neo4jï¼ˆç”Ÿäº§æ¨èï¼‰

```python
service = get_ultimate_rag_service(
    retrieval_client=retrieval_client,
    llm_client=llm_client,
    enable_graph=True,
    graph_backend="neo4j",
    neo4j_uri="bolt://localhost:7687",
    neo4j_user="neo4j",
    neo4j_password="password",
)
```

### 2. è‡ªå®šä¹‰å‹ç¼©ç­–ç•¥

#### ä½¿ç”¨è§„åˆ™å‹ç¼©ï¼ˆé»˜è®¤ï¼‰

```python
service = get_ultimate_rag_service(
    retrieval_client=retrieval_client,
    llm_client=llm_client,
    enable_compression=True,
    use_llmlingua=False,  # ä½¿ç”¨è§„åˆ™æ–¹æ³•
    compression_ratio=0.5,
)
```

#### ä½¿ç”¨ LLMLinguaï¼ˆé«˜è´¨é‡ï¼‰

```python
service = get_ultimate_rag_service(
    retrieval_client=retrieval_client,
    llm_client=llm_client,
    enable_compression=True,
    use_llmlingua=True,  # éœ€è¦å®‰è£… llmlingua
    compression_ratio=0.5,
)
```

### 3. è°ƒæ•´ Self-RAG è¡Œä¸º

```python
from app.self_rag.hallucination_detector import HallucinationDetector
from app.self_rag.self_rag_service import SelfRAGService

# è‡ªå®šä¹‰ Self-RAG æœåŠ¡
detector = HallucinationDetector(
    llm_client=llm_client,
    use_nli=True,  # å¯ç”¨ NLI æ¨¡å‹ï¼ˆéœ€è¦ transformersï¼‰
)

self_rag_service = SelfRAGService(
    llm_client=llm_client,
    hallucination_detector=detector,
    max_refinement_attempts=3,  # å¢åŠ ä¿®æ­£æ¬¡æ•°
    hallucination_threshold=0.8,  # æé«˜æ£€æµ‹é˜ˆå€¼
)
```

---

## ğŸ“Š ç›‘æ§ä¸è°ƒè¯•

### æŸ¥çœ‹æœåŠ¡çŠ¶æ€

```python
status = await service.get_service_status()

print("å¯ç”¨çš„åŠŸèƒ½:")
print("- Iter 1:", status["iter1_features"])
print("- Iter 2:", status["iter2_features"])
print("- Iter 3:", status["iter3_features"])

if "cache_stats" in status:
    print("\nç¼“å­˜ç»Ÿè®¡:")
    print(status["cache_stats"])

if "graph_stats" in status:
    print("\nå›¾è°±ç»Ÿè®¡:")
    print(status["graph_stats"])
```

### æŸ¥çœ‹ Prometheus æŒ‡æ ‡

```bash
# è®¿é—®æŒ‡æ ‡ç«¯ç‚¹
curl http://localhost:8006/metrics | grep rag_

# ç¤ºä¾‹æŒ‡æ ‡
# rag_query_total{mode="ultimate",status="success",tenant_id="user123"} 42
# rag_query_latency_seconds{mode="ultimate",tenant_id="user123"} 2.1
# rag_cache_hit_ratio 0.65
# rag_graph_query_total 15
# rag_hallucination_detected_total 2
# rag_tokens_saved_total 1250
```

### æ—¥å¿—çº§åˆ«

```python
import logging

# è°ƒæ•´æ—¥å¿—çº§åˆ«
logging.getLogger("app.graph").setLevel(logging.DEBUG)
logging.getLogger("app.self_rag").setLevel(logging.DEBUG)
logging.getLogger("app.compression").setLevel(logging.DEBUG)
```

---

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. ç¼“å­˜é¢„çƒ­

```python
# é¢„åŠ è½½å¸¸è§æŸ¥è¯¢åˆ°ç¼“å­˜
common_queries = [
    "ä»€ä¹ˆæ˜¯RAGï¼Ÿ",
    "å¦‚ä½•ä½¿ç”¨RAGï¼Ÿ",
    "RAGæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
]

for query in common_queries:
    await service.query(query, tenant_id="warmup")
```

### 2. å›¾è°±é¢„æ„å»º

```python
from app.graph.graph_store import get_graph_store
from app.graph.entity_extractor import EntityExtractor

# ç¦»çº¿æ„å»ºå›¾è°±
graph_store = get_graph_store(backend="networkx")
extractor = EntityExtractor(llm_client)

# æ‰¹é‡å¤„ç†æ–‡æ¡£
for doc in documents:
    entities, relations = await extractor.extract_entities_and_relations(
        text=doc["content"],
        use_llm=True
    )

    for entity in entities:
        graph_store.add_node(
            node_id=entity["name"],
            node_type=entity["type"],
            properties={"source": doc["id"]}
        )

    for rel in relations:
        graph_store.add_edge(
            source_id=rel["source"],
            target_id=rel["target"],
            relation=rel["relation"]
        )

# ä¿å­˜åˆ°æ–‡ä»¶
graph_store.save_to_file("knowledge_graph.json")

# æœåŠ¡å¯åŠ¨æ—¶åŠ è½½
graph_store.load_from_file("knowledge_graph.json")
```

### 3. å¹¶è¡ŒæŸ¥è¯¢

```python
import asyncio

# æ‰¹é‡æŸ¥è¯¢
queries = [
    "æŸ¥è¯¢1",
    "æŸ¥è¯¢2",
    "æŸ¥è¯¢3",
]

results = await asyncio.gather(*[
    service.query(q, tenant_id="batch") for q in queries
])
```

---

## ğŸ§ª æµ‹è¯•ç¤ºä¾‹

### å•å…ƒæµ‹è¯•

```python
# tests/test_graph_rag.py
import pytest
from app.graph.graph_store import get_graph_store

@pytest.mark.asyncio
async def test_graph_store():
    store = get_graph_store(backend="networkx")

    # æ·»åŠ èŠ‚ç‚¹
    store.add_node("A", "Entity", {"name": "å®ä½“A"})
    store.add_node("B", "Entity", {"name": "å®ä½“B"})

    # æ·»åŠ è¾¹
    store.add_edge("A", "B", "related_to")

    # æŸ¥è¯¢
    neighbors = store.get_neighbors("A", max_hops=1)
    assert len(neighbors) == 1
    assert neighbors[0]["id"] == "B"

@pytest.mark.asyncio
async def test_self_rag():
    from app.self_rag.self_rag_service import SelfRAGService
    from app.self_rag.hallucination_detector import HallucinationDetector

    detector = HallucinationDetector(llm_client)
    service = SelfRAGService(llm_client, detector)

    result = await service.generate_with_self_check(
        query="æµ‹è¯•æŸ¥è¯¢",
        context="æµ‹è¯•ä¸Šä¸‹æ–‡",
        temperature=0.7
    )

    assert "answer" in result
    assert "has_hallucination" in result
    assert result["attempts"] >= 1
```

### é›†æˆæµ‹è¯•

```python
# tests/test_integration.py
@pytest.mark.asyncio
async def test_ultimate_rag_e2e():
    service = get_ultimate_rag_service(
        retrieval_client=mock_retrieval_client,
        llm_client=mock_llm_client,
        enable_hybrid=True,
        enable_graph=True,
        enable_self_rag=True,
    )

    result = await service.query(
        query="æµ‹è¯•å¤æ‚æŸ¥è¯¢",
        tenant_id="test",
        use_graph=True,
        use_self_rag=True,
    )

    assert result["answer"]
    assert result["strategy"] in ["graph_enhanced", "query_decomposition", "base"]
    assert "self_rag" in result
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: å›¾è°±æ£€ç´¢æ— ç»“æœ

**ç—‡çŠ¶**: `graph_results_count: 0`

**å¯èƒ½åŸå› **:
1. å›¾è°±æœªæ„å»ºæˆ–ä¸ºç©º
2. å®ä½“æŠ½å–å¤±è´¥
3. å›¾è°±åç«¯è¿æ¥å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ£€æŸ¥å›¾è°±çŠ¶æ€
graph_store = service.advanced_rag.graph_store
stats = graph_store.get_stats()
print(f"å›¾è°±èŠ‚ç‚¹æ•°: {stats['num_nodes']}")
print(f"å›¾è°±è¾¹æ•°: {stats['num_edges']}")

# æ‰‹åŠ¨æµ‹è¯•å®ä½“æŠ½å–
from app.graph.entity_extractor import EntityExtractor
extractor = EntityExtractor(llm_client)
entities, relations = await extractor.extract_entities_and_relations("æµ‹è¯•æ–‡æœ¬")
print(f"è¯†åˆ«çš„å®ä½“: {entities}")
```

### é—®é¢˜ 2: Self-RAG æ€»æ˜¯æŠ¥å¹»è§‰

**ç—‡çŠ¶**: `has_hallucination: true` (ç½®ä¿¡åº¦é«˜)

**å¯èƒ½åŸå› **:
1. ä¸Šä¸‹æ–‡è´¨é‡å·®
2. æ£€æµ‹é˜ˆå€¼è¿‡ä½
3. LLM ç”Ÿæˆè´¨é‡é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. æé«˜æ£€æµ‹é˜ˆå€¼
service = get_ultimate_rag_service(
    ...,
    enable_self_rag=True,
    # è‡ªå®šä¹‰ Self-RAG
)

# 2. æ£€æŸ¥ä¸Šä¸‹æ–‡
print("æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°:", len(result["documents"]))
print("æ–‡æ¡£å†…å®¹:", [d["content"][:100] for d in result["documents"]])

# 3. æŸ¥çœ‹æ£€æµ‹è¯¦æƒ…
print("æ£€æµ‹ç»“æœ:", result["self_rag"])
print("æ£€æµ‹å†å²:", result.get("detection_history"))
```

### é—®é¢˜ 3: å‹ç¼©åå‡†ç¡®ç‡ä¸‹é™

**ç—‡çŠ¶**: å¯ç”¨å‹ç¼©åç­”æ¡ˆè´¨é‡å˜å·®

**å¯èƒ½åŸå› **:
1. å‹ç¼©ç‡è¿‡é«˜
2. å…³é”®ä¿¡æ¯è¢«å‹ç¼©

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. é™ä½å‹ç¼©ç‡
result = await service.query(
    query=query,
    use_compression=True,
    compression_ratio=0.6,  # ä» 0.5 æé«˜åˆ° 0.6
)

# 2. å¯¹æ¯”å‹ç¼©å‰å
compressor = service.compressor
compression_result = compressor.compress(
    context="åŸå§‹ä¸Šä¸‹æ–‡",
    query=query,
    compression_ratio=0.5,
    preserve_entities=True,  # ä¿ç•™å®ä½“
    preserve_numbers=True,   # ä¿ç•™æ•°å­—
)

print("åŸå§‹é•¿åº¦:", compression_result["original_length"])
print("å‹ç¼©åé•¿åº¦:", compression_result["compressed_length"])
print("å‹ç¼©å†…å®¹:", compression_result["compressed_context"])
```

### é—®é¢˜ 4: æŸ¥è¯¢åˆ†è§£æ•ˆæœä¸å¥½

**ç—‡çŠ¶**: å­æŸ¥è¯¢ä¸åˆç†æˆ–ç­”æ¡ˆåˆå¹¶å¤±è´¥

**å¯èƒ½åŸå› **:
1. æŸ¥è¯¢ä¸å¤Ÿå¤æ‚ï¼Œä¸éœ€è¦åˆ†è§£
2. LLM Prompt éœ€è¦ä¼˜åŒ–

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. æ£€æŸ¥æŸ¥è¯¢åˆ†æ
from app.query.query_classifier import QueryClassifier
classifier = QueryClassifier(llm_client)
analysis = await classifier.classify(query)
print("æŸ¥è¯¢åˆ†æ:", analysis)
# å¤æ‚åº¦ < 7 æ—¶ä¸ä¼šè‡ªåŠ¨åˆ†è§£

# 2. æ‰‹åŠ¨è§¦å‘åˆ†è§£
from app.query.query_decomposer import QueryDecomposer
decomposer = QueryDecomposer(llm_client)
sub_queries = await decomposer.decompose(query, max_sub_queries=3)
print("å­æŸ¥è¯¢:", sub_queries)

# 3. å¼ºåˆ¶å¯ç”¨åˆ†è§£
result = await service.query(
    query=query,
    use_decomposition=True,  # å¼ºåˆ¶å¯ç”¨
)
```

---

## ğŸ“š æ›´å¤šèµ„æº

- [å®Œæ•´æ–‡æ¡£](../../docs/RAG_ITER2_3_COMPLETION.md)
- [è¿­ä»£è®¡åˆ’](../../docs/RAG_ENGINE_ITERATION_PLAN.md)
- [Iter 1 ä½¿ç”¨æŒ‡å—](./ITER1_USAGE_GUIDE.md)
- [æ¶æ„æ–‡æ¡£](../../docs/arch/overview.md)

---

## ğŸ’¡ æœ€ä½³å®è·µæ€»ç»“

### 1. åŠŸèƒ½é€‰æ‹©

| åœºæ™¯ | æ¨èé…ç½® |
|-----|---------|
| ç®€å•é—®ç­” | Hybrid + Cache |
| å¤æ‚æ¨ç† | Hybrid + Graph + Self-RAG |
| é•¿æ–‡æ¡£ | Hybrid + Compression |
| å¤šæ­¥ä»»åŠ¡ | Decomposition + Self-RAG |
| ä½å»¶è¿Ÿ | Cache only, ç¦ç”¨å…¶ä»– |
| é«˜è´¨é‡ | å…¨éƒ¨å¯ç”¨ |

### 2. å‚æ•°å»ºè®®

```python
# å¹³è¡¡æ¨¡å¼ï¼ˆæ¨èï¼‰
result = await service.query(
    query=query,
    top_k=5,
    use_graph=True,
    use_decomposition=True,
    use_self_rag=True,
    use_compression=True,
    compression_ratio=0.5,
    temperature=0.7,
)

# é«˜é€Ÿæ¨¡å¼
result = await service.query(
    query=query,
    top_k=3,
    use_graph=False,
    use_decomposition=False,
    use_self_rag=False,
    use_compression=False,
    temperature=0.7,
)

# é«˜è´¨é‡æ¨¡å¼
result = await service.query(
    query=query,
    top_k=10,
    use_graph=True,
    use_decomposition=True,
    use_self_rag=True,
    use_compression=False,
    temperature=0.3,
)
```

---

**ç‰ˆæœ¬**: v2.0
**æ›´æ–°æ—¥æœŸ**: 2025-10-29
