# RAG Engine Iter 1 ä½¿ç”¨æŒ‡å—

> **å¿«é€Ÿä¸Šæ‰‹ RAG Engine v2.0 æ–°åŠŸèƒ½**

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
cd algo/rag-engine

# å®‰è£…æ‰€æœ‰ä¾èµ–
pip install -r requirements.txt

# æˆ–è€…ä»…å®‰è£… Iter 1 æ–°å¢ä¾èµ–
pip install rank-bm25==0.2.2 faiss-cpu==1.7.4 sentence-transformers==2.5.1
```

### 2. å¯åŠ¨ Redis

```bash
# ä½¿ç”¨ Docker å¯åŠ¨
docker run -d --name rag-redis -p 6379:6379 redis:7-alpine

# æˆ–ä½¿ç”¨ docker-compose
docker-compose up -d redis
```

### 3. å¯åŠ¨ RAG Engine

```bash
# å¼€å‘æ¨¡å¼
python main.py

# æˆ–ä½¿ç”¨ make
make run
```

æœåŠ¡å°†åœ¨ `http://localhost:8006` å¯åŠ¨

---

## ğŸ“ åŠŸèƒ½ä½¿ç”¨ç¤ºä¾‹

### 1. æ··åˆæ£€ç´¢ï¼ˆVector + BM25 + RRFï¼‰

```python
from app.retrieval.hybrid_retriever import get_hybrid_retriever

# åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨
hybrid_retriever = get_hybrid_retriever()

# å‘é‡æ£€ç´¢ç»“æœï¼ˆæ¥è‡ªå‘é‡æ•°æ®åº“ï¼‰
vector_results = [
    {"id": "doc1", "text": "...", "score": 0.85},
    {"id": "doc2", "text": "...", "score": 0.78},
]

# æ‰§è¡Œæ··åˆæ£€ç´¢ï¼ˆè‡ªåŠ¨è°ƒç”¨ BM25 å¹¶èåˆï¼‰
results = hybrid_retriever.retrieve(
    query="ä»€ä¹ˆæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Ÿ",
    vector_results=vector_results,
    top_k=10,
    fusion_method="rrf",  # æ¨èä½¿ç”¨ RRF
)

# æŸ¥çœ‹èåˆåçš„ç»“æœ
for doc in results[:3]:
    print(f"ID: {doc['id']}, RRF Score: {doc['rrf_score']:.4f}")
```

**é…ç½®å‚æ•°**ï¼ˆç¯å¢ƒå˜é‡ï¼‰ï¼š
```bash
export HYBRID_BM25_WEIGHT=0.3      # BM25 æƒé‡
export HYBRID_VECTOR_WEIGHT=0.7     # Vector æƒé‡
export HYBRID_RRF_K=60              # RRF å‚æ•° k
```

---

### 2. Cross-Encoder é‡æ’

```python
from app.reranking.reranker import get_reranker

# åˆå§‹åŒ–é‡æ’å™¨
reranker = get_reranker()

# æ–‡æ¡£åˆ—è¡¨
documents = [
    {"id": "doc1", "text": "æ£€ç´¢å¢å¼ºç”Ÿæˆæ˜¯ä¸€ç§ç»“åˆæ£€ç´¢ä¸ç”Ÿæˆçš„æŠ€æœ¯...", "score": 0.85},
    {"id": "doc2", "text": "RAG é€šè¿‡æ£€ç´¢ç›¸å…³æ–‡æ¡£æ¥å¢å¼ºLLMçš„å›ç­”...", "score": 0.78},
    {"id": "doc3", "text": "å‘é‡æ•°æ®åº“ç”¨äºå­˜å‚¨æ–‡æ¡£çš„å‘é‡è¡¨ç¤º...", "score": 0.72},
]

# é‡æ’åº
reranked_docs = reranker.rerank(
    query="ä»€ä¹ˆæ˜¯RAGï¼Ÿ",
    documents=documents,
)

# æŸ¥çœ‹é‡æ’åçš„ç»“æœ
for doc in reranked_docs:
    print(f"ID: {doc['id']}")
    print(f"  Original Score: {doc['original_score']:.4f}")
    print(f"  Rerank Score: {doc['rerank_score']:.4f}")
```

**é…ç½®å‚æ•°**ï¼š
```bash
export RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
export RERANK_TOP_K=10
export RERANK_DEVICE=cpu  # or cuda
```

---

### 3. FAISS è¯­ä¹‰ç¼“å­˜

```python
from app.services.semantic_cache_service import SemanticCacheService

# åˆå§‹åŒ–ç¼“å­˜æœåŠ¡
cache_service = SemanticCacheService(
    redis_host="localhost",
    redis_port=6379,
    similarity_threshold=0.92,  # æé«˜é˜ˆå€¼ï¼Œæ›´ä¸¥æ ¼åŒ¹é…
    use_faiss=True,              # å¯ç”¨ FAISS åŠ é€Ÿ
)

# æŸ¥è¯¢ç¼“å­˜
query = "ä»€ä¹ˆæ˜¯RAGï¼Ÿ"
cached_answer = await cache_service.get_cached_answer(query)

if cached_answer:
    print(f"âœ… Cache HIT!")
    print(f"   Similarity: {cached_answer.similarity:.4f}")
    print(f"   Original Query: {cached_answer.original_query}")
    print(f"   Answer: {cached_answer.answer}")
else:
    print("âŒ Cache MISS, proceeding with retrieval...")

    # æ‰§è¡Œ RAG æŸ¥è¯¢...
    answer = "æ£€ç´¢å¢å¼ºç”Ÿæˆæ˜¯..."
    sources = [...]

    # ç¼“å­˜ç»“æœ
    await cache_service.set_cached_answer(query, answer, sources)

# æŸ¥çœ‹ç»Ÿè®¡
stats = await cache_service.get_cache_stats()
print(f"\nğŸ“Š Cache Stats:")
print(f"   Hit Rate: {stats['runtime_stats']['hit_rate']:.2f}%")
print(f"   Avg Latency: {stats['runtime_stats']['avg_latency_ms']:.2f}ms")
print(f"   FAISS Indexed: {stats['faiss_indexed_count']} vectors")
```

---

### 4. Prometheus æŒ‡æ ‡æ”¶é›†

```python
from app.observability.metrics import RAGMetrics, record_cache_hit, record_cache_miss

# æ–¹å¼ 1: ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆæ¨èï¼‰
with RAGMetrics(mode="advanced", tenant_id="tenant_001") as metrics:
    # æ‰§è¡Œ RAG æµç¨‹

    # è®°å½•æ£€ç´¢
    metrics.record_retrieval(
        strategy="hybrid",
        latency_ms=450.5,
        doc_count=20
    )

    # è®°å½•é‡æ’
    metrics.record_rerank(
        model="cross-encoder",
        latency_ms=150.2,
        avg_score=0.85
    )

    # è®°å½• LLM
    metrics.record_llm(
        model="gpt-4-turbo",
        latency_ms=1200.3,
        tokens_used=1500
    )

    # è®°å½•ç­”æ¡ˆ
    metrics.record_answer("è¿™æ˜¯ç”Ÿæˆçš„ç­”æ¡ˆ...")

    # è‡ªåŠ¨è®°å½• E2E å»¶è¿Ÿå’Œè¯·æ±‚çŠ¶æ€

# æ–¹å¼ 2: æ‰‹åŠ¨è®°å½•ç‰¹å®šæŒ‡æ ‡
from app.observability.metrics import record_retrieval_latency

record_retrieval_latency("hybrid", "tenant_001", 450.5)
record_cache_hit("tenant_001")
```

**æŸ¥çœ‹æŒ‡æ ‡**ï¼š
```bash
# è®¿é—® Prometheus metrics ç«¯ç‚¹
curl http://localhost:8006/metrics

# ç­›é€‰ RAG ç›¸å…³æŒ‡æ ‡
curl http://localhost:8006/metrics | grep rag_

# ç¤ºä¾‹è¾“å‡ºï¼š
# rag_retrieval_latency_ms_bucket{strategy="hybrid",tenant_id="tenant_001",le="500.0"} 8
# rag_cache_operations_total{operation="hit",tenant_id="tenant_001"} 15
# rag_token_usage_total{phase="generation",model="gpt-4",tenant_id="tenant_001"} 12500
```

---

### 5. æ•´åˆä½¿ç”¨ï¼ˆEnhanced RAG Serviceï¼‰

```python
from app.services.enhanced_rag_service import get_enhanced_rag_service

# åˆå§‹åŒ–ï¼ˆéœ€è¦æä¾› retrieval_client å’Œ llm_clientï¼‰
rag_service = get_enhanced_rag_service(
    retrieval_client=your_retrieval_client,
    llm_client=your_llm_client,
    enable_hybrid=True,    # å¯ç”¨æ··åˆæ£€ç´¢
    enable_rerank=True,    # å¯ç”¨é‡æ’
    enable_cache=True,     # å¯ç”¨è¯­ä¹‰ç¼“å­˜
)

# æ‰§è¡ŒæŸ¥è¯¢ï¼ˆè‡ªåŠ¨å®Œæˆæ‰€æœ‰ä¼˜åŒ–æµç¨‹ï¼‰
result = await rag_service.query(
    query="è§£é‡Šä¸€ä¸‹RAGæŠ€æœ¯çš„å·¥ä½œåŸç†",
    tenant_id="tenant_001",
    mode="advanced",
    top_k=5,
    temperature=0.7,
)

# æŸ¥çœ‹ç»“æœ
print(f"Answer: {result['answer']}")
print(f"From Cache: {result['from_cache']}")
print(f"Retrieved Docs: {result['retrieved_count']}")
print(f"\nMetrics:")
print(f"  Retrieval: {result['metrics']['retrieval_latency_ms']:.2f}ms")
print(f"  Rerank: {result['metrics']['rerank_latency_ms']:.2f}ms")
print(f"  LLM: {result['metrics']['llm_latency_ms']:.2f}ms")

# æŸ¥çœ‹æœåŠ¡ç»Ÿè®¡
stats = await rag_service.get_stats()
print(f"\nğŸ“ˆ Service Stats:")
print(f"Enabled Features: {stats['enabled_features']}")
print(f"Cache Hit Rate: {stats['cache_stats']['runtime_stats']['hit_rate']}%")
```

---

## ğŸ”§ é…ç½®å‚æ•°

### ç¯å¢ƒå˜é‡

```bash
# RAG Engine åŸºç¡€é…ç½®
export RAG_SERVICE_PORT=8006
export RAG_LOG_LEVEL=INFO

# æ··åˆæ£€ç´¢
export HYBRID_BM25_WEIGHT=0.3
export HYBRID_VECTOR_WEIGHT=0.7
export HYBRID_RRF_K=60

# é‡æ’å™¨
export RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
export RERANK_TOP_K=10
export RERANK_DEVICE=cpu

# è¯­ä¹‰ç¼“å­˜
export REDIS_HOST=localhost
export REDIS_PORT=6379
export REDIS_DB=0
export CACHE_SIMILARITY_THRESHOLD=0.92
export CACHE_TTL=3600
export CACHE_MAX_QUERIES=10000
export CACHE_USE_FAISS=true

# å¯è§‚æµ‹æ€§
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
export PROMETHEUS_METRICS_PORT=8006
```

### YAML é…ç½®ï¼ˆæ¨èï¼‰

å‚è€ƒ `configs/rag-engine.yaml` çš„é…ç½®ç»“æ„ï¼ˆå·²åˆ›å»ºï¼‰

---

## ğŸ“Š æ€§èƒ½ç›‘æ§

### Grafana Dashboard

**è®¿é—®**: `http://grafana.example.com/d/rag-engine`

**å…³é”®é¢æ¿**:
1. **æ ¸å¿ƒæŒ‡æ ‡**
   - E2E å»¶è¿Ÿ (P50/P95/P99)
   - QPS & æˆåŠŸç‡
   - ç¼“å­˜å‘½ä¸­ç‡
   - Token ä½¿ç”¨è¶‹åŠ¿

2. **æ£€ç´¢åˆ†æ**
   - æ£€ç´¢ç­–ç•¥åˆ†å¸ƒï¼ˆVector/BM25/Hybridï¼‰
   - é‡æ’åˆ†æ•°åˆ†å¸ƒ
   - æ£€ç´¢æ–‡æ¡£æ•°åˆ†å¸ƒ

3. **æˆæœ¬åˆ†æ**
   - æ¯è¯·æ±‚æˆæœ¬ï¼ˆæŒ‰ç§Ÿæˆ·ï¼‰
   - Token ä½¿ç”¨åˆ†è§£ï¼ˆæ£€ç´¢/ç”Ÿæˆ/éªŒè¯ï¼‰
   - Top 10 è€—è´¹æŸ¥è¯¢

### PromQL æŸ¥è¯¢ç¤ºä¾‹

```promql
# æ£€ç´¢å»¶è¿Ÿ P95
histogram_quantile(0.95, rate(rag_retrieval_latency_ms_bucket[5m]))

# ç¼“å­˜å‘½ä¸­ç‡
rate(rag_cache_operations_total{operation="hit"}[5m]) /
rate(rag_cache_operations_total[5m]) * 100

# æ¯åˆ†é’Ÿ Token æ¶ˆè€—
rate(rag_token_usage_total[1m])
```

---

## ğŸ§ª æµ‹è¯•ä¸éªŒè¯

### å¥åº·æ£€æŸ¥

```bash
# æœåŠ¡å¥åº·
curl http://localhost:8006/health

# é¢„æœŸè¾“å‡º
{
  "status": "healthy",
  "service": "rag-engine"
}
```

### åŠŸèƒ½æµ‹è¯•

```bash
# æµ‹è¯•æ··åˆæ£€ç´¢
curl -X POST http://localhost:8006/api/v1/rag/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "ä»€ä¹ˆæ˜¯RAGï¼Ÿ",
    "mode": "advanced",
    "tenant_id": "test_tenant",
    "top_k": 5
  }'
```

### è¿è¡Œè¯„æµ‹

```bash
cd tests/eval

# å‡†å¤‡æ•°æ®é›†ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
# python scripts/prepare_datasets.py

# è¿è¡Œè¯„æµ‹
python scripts/run_eval.py \
  --dataset datasets/general_qa.jsonl \
  --output results/iter1_$(date +%Y%m%d).json

# æŸ¥çœ‹ç»“æœ
cat results/iter1_*.json | jq .summary
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

**Q1: FAISS å®‰è£…å¤±è´¥**

```bash
# æ–¹æ³• 1: ä½¿ç”¨ conda
conda install -c conda-forge faiss-cpu

# æ–¹æ³• 2: ä½¿ç”¨ pipï¼ˆç¡®ä¿ç‰ˆæœ¬åŒ¹é…ï¼‰
pip install faiss-cpu==1.7.4

# å¦‚æœä»å¤±è´¥ï¼Œå¯ä»¥ç¦ç”¨ FAISS
export CACHE_USE_FAISS=false
```

**Q2: Redis è¿æ¥å¤±è´¥**

```bash
# æ£€æŸ¥ Redis æ˜¯å¦è¿è¡Œ
docker ps | grep redis

# æµ‹è¯•è¿æ¥
redis-cli ping
# åº”è¿”å›: PONG

# æ£€æŸ¥é…ç½®
echo $REDIS_HOST
echo $REDIS_PORT
```

**Q3: Cross-Encoder æ¨¡å‹åŠ è½½æ…¢**

```bash
# é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œå¯ä»¥æå‰ä¸‹è½½
python -c "from sentence_transformers import CrossEncoder; CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"

# æˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹
export RERANK_MODEL=/path/to/local/model
```

**Q4: å†…å­˜ä¸è¶³**

```bash
# å‡å°‘ç¼“å­˜å¤§å°
export CACHE_MAX_QUERIES=5000

# ä½¿ç”¨æ›´å°çš„é‡æ’æ¨¡å‹
export RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2  # å·²æ˜¯æœ€å°ç‰ˆæœ¬

# ç¦ç”¨ FAISSï¼ˆèŠ‚çœå†…å­˜ï¼‰
export CACHE_USE_FAISS=false
```

---

## ğŸ“– æ›´å¤šèµ„æº

- [RAG Engine ä¼˜åŒ–è¿­ä»£è®¡åˆ’](../../../docs/RAG_ENGINE_ITERATION_PLAN.md)
- [Iter 1 å®Œæˆæ€»ç»“](../../../docs/RAG_ITER1_COMPLETION.md)
- [è¯„æµ‹æŒ‡å—](tests/eval/README.md)
- [é…ç½®æ–‡ä»¶](../../../configs/rag-engine.yaml)

---

## ğŸ¤ åé¦ˆä¸è´¡çŒ®

é‡åˆ°é—®é¢˜æˆ–æœ‰å»ºè®®ï¼Ÿ

- **Issue**: [GitHub Issues](https://github.com/xxx/voicehelper/issues)
- **Slack**: #rag-optimization
- **Email**: team@example.com

---

**ç‰ˆæœ¬**: v2.0 (Iter 1)
**æœ€åæ›´æ–°**: 2025-01-29
