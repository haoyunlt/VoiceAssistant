"""
ä¸¤çº§å‘é‡ç¼“å­˜æ¼”ç¤º

å±•ç¤ºå¦‚ä½•ä½¿ç”¨ L1 (å†…å­˜) + L2 (Redis) ç¼“å­˜æ¥åŠ é€Ÿå‘é‡åŒ–
"""

import asyncio
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def demo_basic_cache():
    """åŸºç¡€ç¼“å­˜æ¼”ç¤º"""
    logger.info("=== Demo 1: Basic Two-Level Cache ===\n")

    import redis.asyncio as redis
    from app.core.embedder import BGE_M3_Embedder
    from app.infrastructure.vector_cache import VectorCache

    # 1. åˆå§‹åŒ–ç»„ä»¶
    redis_client = await redis.from_url("redis://localhost:6379/0")
    embedder = BGE_M3_Embedder()

    vector_cache = VectorCache(
        redis_client=redis_client,
        l1_max_size=100,  # å°å®¹é‡ç”¨äºæ¼”ç¤º
        l2_ttl=3600,      # 1å°æ—¶
    )

    # 2. æµ‹è¯•æ–‡æœ¬
    texts = [
        "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ",
        "æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯",
        "æ·±åº¦å­¦ä¹ å–å¾—äº†çªç ´æ€§è¿›å±•",
    ]

    # 3. ç¬¬ä¸€æ¬¡è¯·æ±‚ - ç¼“å­˜æœªå‘½ä¸­ï¼Œéœ€è¦è®¡ç®—
    logger.info("First request (cache miss):")
    start_time = time.time()

    for text in texts:
        vector = await vector_cache.get_or_compute(
            text=text,
            model="bge-m3",
            compute_fn=lambda t=text: embedder.embed(t),
        )
        logger.info(f"  - Vector dimension: {len(vector)}")

    first_duration = time.time() - start_time
    logger.info(f"â±ï¸  Duration: {first_duration:.3f}s\n")

    # 4. ç¬¬äºŒæ¬¡è¯·æ±‚ - L1 ç¼“å­˜å‘½ä¸­
    logger.info("Second request (L1 cache hit):")
    start_time = time.time()

    for text in texts:
        vector = await vector_cache.get_or_compute(
            text=text,
            model="bge-m3",
            compute_fn=lambda t=text: embedder.embed(t),
        )

    second_duration = time.time() - start_time
    logger.info(f"â±ï¸  Duration: {second_duration:.3f}s")
    logger.info(f"ğŸš€ Speedup: {first_duration / second_duration:.1f}x\n")

    # 5. æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
    stats = vector_cache.get_stats()
    logger.info("ğŸ“Š Cache Statistics:")
    logger.info(f"  L1 - Size: {stats['L1']['size']}, Hit Rate: {stats['L1']['hit_rate']:.2%}")
    logger.info(f"  L2 - Hits: {stats['L2']['hits']}, Hit Rate: {stats['L2']['hit_rate']:.2%}")
    logger.info(f"  Overall Hit Rate: {stats['overall']['hit_rate']:.2%}")
    logger.info(f"  Cache Efficiency: {stats['overall']['cache_efficiency']:.2%}\n")

    # æ¸…ç†
    await redis_client.close()


async def demo_batch_cache():
    """æ‰¹é‡ç¼“å­˜æ¼”ç¤º"""
    logger.info("=== Demo 2: Batch Cache with Partial Hits ===\n")

    import redis.asyncio as redis
    from app.core.embedder import BGE_M3_Embedder
    from app.infrastructure.vector_cache import VectorCache

    # åˆå§‹åŒ–
    redis_client = await redis.from_url("redis://localhost:6379/0")
    embedder = BGE_M3_Embedder()

    vector_cache = VectorCache(
        redis_client=redis_client,
        l1_max_size=1000,
        l2_ttl=3600,
    )

    # æµ‹è¯•æ•°æ®
    all_texts = [
        "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯",
        "è®¡ç®—æœºè§†è§‰åº”ç”¨",
        "è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ",
        "æ¨èç®—æ³•ä¼˜åŒ–",
        "æ•°æ®æŒ–æ˜æ–¹æ³•",
    ]

    # ç¬¬ä¸€æ¬¡ï¼šç¼“å­˜éƒ¨åˆ†æ–‡æœ¬
    logger.info("Step 1: Cache 3 texts")
    await vector_cache.get_batch_or_compute(
        texts=all_texts[:3],
        model="bge-m3",
        compute_fn=lambda texts: embedder.embed_batch(texts),
    )

    # ç¬¬äºŒæ¬¡ï¼šæ··åˆè¯·æ±‚ï¼ˆéƒ¨åˆ†å‘½ä¸­ï¼Œéƒ¨åˆ†æœªå‘½ä¸­ï¼‰
    logger.info("\nStep 2: Mixed request (2 cached, 3 new)")
    start_time = time.time()

    vectors = await vector_cache.get_batch_or_compute(
        texts=all_texts,
        model="bge-m3",
        compute_fn=lambda texts: embedder.embed_batch(texts),
    )

    duration = time.time() - start_time
    logger.info(f"  Processed {len(vectors)} vectors in {duration:.3f}s")

    # ç»Ÿè®¡
    stats = vector_cache.get_stats()
    logger.info(f"\nğŸ“Š Final Stats:")
    logger.info(f"  Total Requests: {stats['overall']['total_requests']}")
    logger.info(f"  Total Hits: {stats['overall']['total_hits']}")
    logger.info(f"  Compute Count: {stats['overall']['compute_count']}")
    logger.info(f"  Overall Hit Rate: {stats['overall']['hit_rate']:.2%}\n")

    await redis_client.close()


async def demo_cached_embedder():
    """å¸¦ç¼“å­˜çš„ Embedder æ¼”ç¤º"""
    logger.info("=== Demo 3: CachedEmbedder Wrapper ===\n")

    import redis.asyncio as redis
    from app.core.embedder import BGE_M3_Embedder
    from app.infrastructure.vector_cache import VectorCache
    from app.core.cached_embedder import CachedEmbedder

    # åˆå§‹åŒ–
    redis_client = await redis.from_url("redis://localhost:6379/0")

    # åŸå§‹ Embedder
    base_embedder = BGE_M3_Embedder()

    # åˆ›å»ºç¼“å­˜
    vector_cache = VectorCache(
        redis_client=redis_client,
        l1_max_size=1000,
        l2_ttl=3600,
    )

    # åŒ…è£…ä¸ºå¸¦ç¼“å­˜çš„ Embedder
    cached_embedder = CachedEmbedder(
        embedder=base_embedder,
        vector_cache=vector_cache,
        model_name="bge-m3",
    )

    # æµ‹è¯•æ•°æ®
    texts = [
        "æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ",
        "ç¥ç»ç½‘ç»œæ¶æ„æœç´¢",
        "è¿ç§»å­¦ä¹ åº”ç”¨",
        "å¼ºåŒ–å­¦ä¹ ç®—æ³•",
        "è”é‚¦å­¦ä¹ æ¡†æ¶",
    ] * 2  # é‡å¤ä¸¤æ¬¡

    # ä½¿ç”¨å¸¦ç¼“å­˜çš„ Embedder
    logger.info("Processing 10 texts (5 unique, 5 duplicates)...")

    start_time = time.time()
    vectors = await cached_embedder.embed_batch(texts)
    duration = time.time() - start_time

    logger.info(f"â±ï¸  Duration: {duration:.3f}s")
    logger.info(f"  Processed {len(vectors)} vectors")

    # æŸ¥çœ‹ç»Ÿè®¡
    stats = cached_embedder.get_cache_stats()
    logger.info(f"\nğŸ“Š Cache Stats:")
    logger.info(f"  Hit Rate: {stats['overall']['hit_rate']:.2%}")
    logger.info(f"  Compute Count: {stats['overall']['compute_count']} (expected: 5)")
    logger.info(f"  Cache Efficiency: {stats['overall']['cache_efficiency']:.2%}\n")

    await redis_client.close()


async def demo_cache_warmup():
    """ç¼“å­˜é¢„çƒ­æ¼”ç¤º"""
    logger.info("=== Demo 4: Cache Warmup ===\n")

    import redis.asyncio as redis
    from app.core.embedder import BGE_M3_Embedder
    from app.infrastructure.vector_cache import VectorCache
    from app.core.cached_embedder import CachedEmbedder

    # åˆå§‹åŒ–
    redis_client = await redis.from_url("redis://localhost:6379/0")
    base_embedder = BGE_M3_Embedder()

    vector_cache = VectorCache(
        redis_client=redis_client,
        l1_max_size=1000,
        l2_ttl=3600,
    )

    cached_embedder = CachedEmbedder(
        embedder=base_embedder,
        vector_cache=vector_cache,
        model_name="bge-m3",
    )

    # å‡†å¤‡å¸¸ç”¨æ–‡æœ¬ï¼ˆå¦‚å¸¸è§é—®é¢˜ï¼‰
    common_texts = [
        "å¦‚ä½•ä½¿ç”¨äº§å“ï¼Ÿ",
        "ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ",
        "æœ‰ä»€ä¹ˆä¼˜æƒ æ´»åŠ¨ï¼Ÿ",
        "æ”¯æŒå“ªäº›æ”¯ä»˜æ–¹å¼ï¼Ÿ",
        "é…é€éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ",
        "å¦‚ä½•ç”³è¯·é€€æ¬¾ï¼Ÿ",
        "äº§å“ä¿ä¿®æœŸå¤šä¹…ï¼Ÿ",
        "æœ‰ä»€ä¹ˆå”®åæœåŠ¡ï¼Ÿ",
    ]

    # é¢„çƒ­ç¼“å­˜
    logger.info(f"Warming up cache with {len(common_texts)} common texts...")
    start_time = time.time()

    await cached_embedder.warmup_cache(
        texts=common_texts,
        batch_size=4,
    )

    warmup_duration = time.time() - start_time
    logger.info(f"âœ… Warmup completed in {warmup_duration:.3f}s\n")

    # ç°åœ¨æŸ¥è¯¢è¿™äº›æ–‡æœ¬ï¼ˆåº”è¯¥å…¨éƒ¨å‘½ä¸­ç¼“å­˜ï¼‰
    logger.info("Querying warmed-up texts...")
    start_time = time.time()

    vectors = await cached_embedder.embed_batch(common_texts)

    query_duration = time.time() - start_time
    logger.info(f"â±ï¸  Query duration: {query_duration:.3f}s")
    logger.info(f"ğŸš€ Speedup: {warmup_duration / query_duration:.1f}x\n")

    # ç»Ÿè®¡
    stats = cached_embedder.get_cache_stats()
    logger.info(f"ğŸ“Š Cache Stats:")
    logger.info(f"  Hit Rate: {stats['overall']['hit_rate']:.2%}")
    logger.info(f"  Total Hits: {stats['overall']['total_hits']}\n")

    await redis_client.close()


async def demo_multi_model_cache():
    """å¤šæ¨¡å‹ç¼“å­˜æ¼”ç¤º"""
    logger.info("=== Demo 5: Multi-Model Cache ===\n")

    import redis.asyncio as redis
    from app.core.embedder import BGE_M3_Embedder
    from app.infrastructure.vector_cache import MultiModelVectorCache
    from app.core.cached_embedder import MultiModelCachedEmbedder

    # åˆå§‹åŒ–
    redis_client = await redis.from_url("redis://localhost:6379/0")

    # åˆ›å»ºå¤šä¸ª Embedderï¼ˆè¿™é‡Œç”¨åŒä¸€ä¸ªæ¨¡æ‹Ÿä¸åŒæ¨¡å‹ï¼‰
    embedders = {
        "bge-m3": BGE_M3_Embedder(),
        "bge-m3-v2": BGE_M3_Embedder(),  # æ¨¡æ‹Ÿå¦ä¸€ä¸ªæ¨¡å‹
    }

    # å¤šæ¨¡å‹ç¼“å­˜
    multi_model_cache = MultiModelVectorCache(
        redis_client=redis_client,
        l1_max_size_per_model=500,
        l2_ttl=3600,
    )

    # å¤šæ¨¡å‹å¸¦ç¼“å­˜çš„ Embedder
    multi_cached_embedder = MultiModelCachedEmbedder(
        embedders=embedders,
        multi_model_cache=multi_model_cache,
    )

    # æµ‹è¯•æ–‡æœ¬
    texts = ["AIæŠ€æœ¯å‘å±•", "æœºå™¨å­¦ä¹ åº”ç”¨", "æ•°æ®ç§‘å­¦"]

    # ä½¿ç”¨ä¸åŒæ¨¡å‹
    logger.info("Processing with bge-m3...")
    vectors_m3 = await multi_cached_embedder.embed_batch(texts, model="bge-m3")
    logger.info(f"  Generated {len(vectors_m3)} vectors")

    logger.info("\nProcessing with bge-m3-v2...")
    vectors_m3v2 = await multi_cached_embedder.embed_batch(texts, model="bge-m3-v2")
    logger.info(f"  Generated {len(vectors_m3v2)} vectors")

    # å†æ¬¡ä½¿ç”¨ bge-m3ï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰
    logger.info("\nProcessing with bge-m3 again (cache hit)...")
    start_time = time.time()
    vectors_m3_again = await multi_cached_embedder.embed_batch(texts, model="bge-m3")
    duration = time.time() - start_time
    logger.info(f"â±ï¸  Duration: {duration:.3f}s (fast!)")

    # æŸ¥çœ‹æ‰€æœ‰æ¨¡å‹çš„ç»Ÿè®¡
    all_stats = multi_cached_embedder.get_all_stats()
    logger.info(f"\nğŸ“Š Multi-Model Cache Stats:")

    for model, stats in all_stats.items():
        logger.info(f"\n  {model}:")
        logger.info(f"    Hit Rate: {stats['overall']['hit_rate']:.2%}")
        logger.info(f"    L1 Size: {stats['L1']['size']}")
        logger.info(f"    Compute Count: {stats['overall']['compute_count']}")

    await redis_client.close()


async def demo_performance_comparison():
    """æ€§èƒ½å¯¹æ¯”æ¼”ç¤º"""
    logger.info("=== Demo 6: Performance Comparison ===\n")

    import redis.asyncio as redis
    from app.core.embedder import BGE_M3_Embedder
    from app.infrastructure.vector_cache import VectorCache
    from app.core.cached_embedder import CachedEmbedder

    # åˆå§‹åŒ–
    redis_client = await redis.from_url("redis://localhost:6379/0")
    base_embedder = BGE_M3_Embedder()

    vector_cache = VectorCache(
        redis_client=redis_client,
        l1_max_size=10000,
        l2_ttl=3600,
    )

    cached_embedder = CachedEmbedder(
        embedder=base_embedder,
        vector_cache=vector_cache,
        model_name="bge-m3",
    )

    # æµ‹è¯•æ•°æ®ï¼ˆ100ä¸ªæ–‡æœ¬ï¼‰
    texts = [f"æµ‹è¯•æ–‡æœ¬ {i}" for i in range(100)]

    # 1. æ— ç¼“å­˜æ€§èƒ½
    logger.info("Test 1: Without cache (baseline)")
    start_time = time.time()
    await base_embedder.embed_batch(texts)
    baseline_duration = time.time() - start_time
    logger.info(f"  Duration: {baseline_duration:.3f}s")

    # 2. é¦–æ¬¡ä½¿ç”¨ç¼“å­˜ï¼ˆæœªå‘½ä¸­ï¼‰
    logger.info("\nTest 2: With cache (first time, cache miss)")
    await cached_embedder.clear_cache()
    start_time = time.time()
    await cached_embedder.embed_batch(texts)
    first_cache_duration = time.time() - start_time
    logger.info(f"  Duration: {first_cache_duration:.3f}s")
    logger.info(f"  Overhead: {(first_cache_duration - baseline_duration):.3f}s")

    # 3. ç¬¬äºŒæ¬¡ä½¿ç”¨ç¼“å­˜ï¼ˆå…¨éƒ¨å‘½ä¸­ï¼‰
    logger.info("\nTest 3: With cache (second time, cache hit)")
    start_time = time.time()
    await cached_embedder.embed_batch(texts)
    second_cache_duration = time.time() - start_time
    logger.info(f"  Duration: {second_cache_duration:.3f}s")
    logger.info(f"  Speedup: {baseline_duration / second_cache_duration:.1f}x")

    # 4. æ··åˆåœºæ™¯ï¼ˆ50%å‘½ä¸­ï¼‰
    logger.info("\nTest 4: Mixed scenario (50% cache hit)")
    mixed_texts = texts[:50] + [f"æ–°æ–‡æœ¬ {i}" for i in range(50)]
    start_time = time.time()
    await cached_embedder.embed_batch(mixed_texts)
    mixed_duration = time.time() - start_time
    logger.info(f"  Duration: {mixed_duration:.3f}s")
    logger.info(f"  Speedup: {baseline_duration / mixed_duration:.1f}x")

    # æœ€ç»ˆç»Ÿè®¡
    stats = cached_embedder.get_cache_stats()
    logger.info(f"\nğŸ“Š Final Cache Stats:")
    logger.info(f"  Total Requests: {stats['overall']['total_requests']}")
    logger.info(f"  Total Hits: {stats['overall']['total_hits']}")
    logger.info(f"  Hit Rate: {stats['overall']['hit_rate']:.2%}")
    logger.info(f"  Cache Efficiency: {stats['overall']['cache_efficiency']:.2%}")

    # æˆæœ¬èŠ‚çœä¼°ç®—
    cost_saved = (stats['overall']['total_requests'] - stats['overall']['compute_count']) / stats['overall']['total_requests']
    logger.info(f"\nğŸ’° Cost Savings:")
    logger.info(f"  Compute Reduction: {cost_saved:.2%}")
    logger.info(f"  Estimated Token Savings: {int(cost_saved * 100)}%\n")

    await redis_client.close()


async def main():
    """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    try:
        await demo_basic_cache()
        await demo_batch_cache()
        await demo_cached_embedder()
        await demo_cache_warmup()
        await demo_multi_model_cache()
        await demo_performance_comparison()

        logger.info("âœ… All demos completed successfully!")

    except Exception as e:
        logger.error(f"Demo failed: {e}", exc_info=True)


if __name__ == "__main__":
    asyncio.run(main())
