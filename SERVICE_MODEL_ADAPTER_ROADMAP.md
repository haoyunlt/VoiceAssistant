# Model Adapter æœåŠ¡åŠŸèƒ½æ¸…å•ä¸è¿­ä»£è®¡åˆ’

## æœåŠ¡æ¦‚è¿°

Model Adapter æ˜¯æ¨¡å‹é€‚é…å™¨æœåŠ¡ï¼Œæä¾›ç»Ÿä¸€çš„ AI æ¨¡å‹è°ƒç”¨æ¥å£ï¼Œæ”¯æŒå¤šä¸ª LLM æä¾›å•†ã€‚

**æŠ€æœ¯æ ˆ**: FastAPI + Python 3.11+ + OpenAI SDK + Anthropic SDK

**ç«¯å£**: 8005

---

## ä¸€ã€åŠŸèƒ½å®Œæˆåº¦è¯„ä¼°

### âœ… å·²å®ŒæˆåŠŸèƒ½

#### 1. æ ¸å¿ƒé€‚é…å™¨
- âœ… OpenAI é€‚é…å™¨ï¼ˆChatã€Completionã€Embeddingï¼‰
- âœ… Azure OpenAI é€‚é…å™¨
- âœ… Anthropic é€‚é…å™¨ï¼ˆéƒ¨åˆ†ï¼‰
- âœ… ç»Ÿä¸€æ¥å£æŠ½è±¡
- âœ… æµå¼å“åº”æ”¯æŒï¼ˆOpenAIï¼‰

#### 2. API æ¥å£
- âœ… `/api/v1/chat/completions` - èŠå¤©æ¥å£
- âœ… `/api/v1/completion/create` - è¡¥å…¨æ¥å£
- âœ… `/api/v1/embedding/create` - å‘é‡åŒ–æ¥å£
- âœ… `/api/v1/chat/models` - æ¨¡å‹åˆ—è¡¨
- âœ… `/health` - å¥åº·æ£€æŸ¥

#### 3. åŸºç¡€è®¾æ–½
- âœ… FastAPI æ¡†æ¶
- âœ… Pydantic æ•°æ®æ¨¡å‹
- âœ… é…ç½®ç®¡ç†
- âœ… æ—¥å¿—ç³»ç»Ÿ

---

## äºŒã€å¾…å®ŒæˆåŠŸèƒ½æ¸…å•

### ğŸ”„ P0 - å›½äº§æ¨¡å‹é€‚é…ï¼ˆè¿­ä»£1ï¼š2å‘¨ï¼‰

#### 1. æ™ºè°± AI (GLM) é€‚é…å™¨
**å½“å‰çŠ¶æ€**: éƒ¨åˆ†å®ç°ï¼Œéœ€å®Œå–„

**ä½ç½®**: `algo/model-adapter/app/services/providers/zhipu_adapter.py`

**å¾…å®ç°**:

```python
# æ–‡ä»¶: algo/model-adapter/app/services/providers/zhipu_adapter.py

import zhipuai
from typing import AsyncIterator

class ZhipuAdapter(BaseAdapter):
    """æ™ºè°± AI é€‚é…å™¨"""
    
    def __init__(self):
        self.api_key = os.getenv("ZHIPU_API_KEY")
        if not self.api_key:
            raise ValueError("ZHIPU_API_KEY not configured")
        
        zhipuai.api_key = self.api_key
        self.client = zhipuai.ZhipuAI(api_key=self.api_key)
    
    async def chat(self, request: ChatRequest) -> ChatResponse:
        """
        æ™ºè°± AI èŠå¤©æ¥å£
        """
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            messages = self._convert_messages(request.messages)
            
            # è°ƒç”¨æ™ºè°± API
            response = self.client.chat.completions.create(
                model=request.model or "glm-4",
                messages=messages,
                temperature=request.temperature or 0.95,
                top_p=request.top_p or 0.7,
                max_tokens=request.max_tokens or 2048,
                stream=False
            )
            
            # è½¬æ¢å“åº”æ ¼å¼
            return ChatResponse(
                id=response.id,
                model=response.model,
                choices=[
                    Choice(
                        index=0,
                        message=Message(
                            role="assistant",
                            content=response.choices[0].message.content
                        ),
                        finish_reason=response.choices[0].finish_reason
                    )
                ],
                usage=Usage(
                    prompt_tokens=response.usage.prompt_tokens,
                    completion_tokens=response.usage.completion_tokens,
                    total_tokens=response.usage.total_tokens
                )
            )
        
        except Exception as e:
            logger.error(f"Zhipu chat error: {e}")
            raise AdapterError(f"Zhipu API call failed: {str(e)}")
    
    async def chat_stream(self, request: ChatRequest) -> AsyncIterator[str]:
        """
        æ™ºè°± AI æµå¼èŠå¤©
        """
        try:
            messages = self._convert_messages(request.messages)
            
            # æµå¼è°ƒç”¨
            response = self.client.chat.completions.create(
                model=request.model or "glm-4",
                messages=messages,
                temperature=request.temperature or 0.95,
                max_tokens=request.max_tokens or 2048,
                stream=True
            )
            
            # æµå¼è¿”å›
            for chunk in response:
                if chunk.choices[0].delta.content:
                    yield self._format_stream_chunk(
                        chunk_id=chunk.id,
                        content=chunk.choices[0].delta.content,
                        finish_reason=chunk.choices[0].finish_reason
                    )
        
        except Exception as e:
            logger.error(f"Zhipu stream error: {e}")
            raise AdapterError(f"Zhipu stream failed: {str(e)}")
    
    async def embedding(self, request: EmbeddingRequest) -> EmbeddingResponse:
        """
        æ™ºè°± AI Embedding
        """
        try:
            # è°ƒç”¨æ™ºè°± Embedding API
            response = self.client.embeddings.create(
                model="embedding-2",
                input=request.input
            )
            
            return EmbeddingResponse(
                model="embedding-2",
                data=[
                    EmbeddingData(
                        embedding=item.embedding,
                        index=item.index
                    )
                    for item in response.data
                ],
                usage=Usage(
                    prompt_tokens=response.usage.prompt_tokens,
                    total_tokens=response.usage.total_tokens
                )
            )
        
        except Exception as e:
            logger.error(f"Zhipu embedding error: {e}")
            raise AdapterError(f"Zhipu embedding failed: {str(e)}")
    
    def _convert_messages(self, messages: List[Message]) -> List[dict]:
        """è½¬æ¢æ¶ˆæ¯æ ¼å¼"""
        return [
            {
                "role": msg.role,
                "content": msg.content
            }
            for msg in messages
        ]
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] Chat æ¥å£å®Œæˆ
- [ ] æµå¼æ¥å£å®Œæˆ
- [ ] Embedding æ¥å£å®Œæˆ
- [ ] é”™è¯¯å¤„ç†å®Œå–„

#### 2. é€šä¹‰åƒé—® (Qwen) é€‚é…å™¨
**å½“å‰çŠ¶æ€**: éƒ¨åˆ†å®ç°ï¼Œéœ€å®Œå–„

**ä½ç½®**: `algo/model-adapter/app/services/providers/qwen_adapter.py`

**å¾…å®ç°**:

```python
# æ–‡ä»¶: algo/model-adapter/app/services/providers/qwen_adapter.py

import dashscope
from dashscope import Generation, TextEmbedding
from http import HTTPStatus

class QwenAdapter(BaseAdapter):
    """é€šä¹‰åƒé—®é€‚é…å™¨"""
    
    def __init__(self):
        self.api_key = os.getenv("QWEN_API_KEY") or os.getenv("DASHSCOPE_API_KEY")
        if not self.api_key:
            raise ValueError("QWEN_API_KEY not configured")
        
        dashscope.api_key = self.api_key
    
    async def chat(self, request: ChatRequest) -> ChatResponse:
        """
        é€šä¹‰åƒé—®èŠå¤©æ¥å£
        """
        try:
            messages = self._convert_messages(request.messages)
            
            # è°ƒç”¨é€šä¹‰åƒé—® API
            response = Generation.call(
                model=request.model or "qwen-max",
                messages=messages,
                result_format='message',
                temperature=request.temperature or 1.0,
                top_p=request.top_p or 0.8,
                max_tokens=request.max_tokens or 2000,
                stream=False
            )
            
            if response.status_code == HTTPStatus.OK:
                return ChatResponse(
                    id=response.request_id,
                    model=request.model or "qwen-max",
                    choices=[
                        Choice(
                            index=0,
                            message=Message(
                                role="assistant",
                                content=response.output.choices[0].message.content
                            ),
                            finish_reason=response.output.choices[0].finish_reason
                        )
                    ],
                    usage=Usage(
                        prompt_tokens=response.usage.input_tokens,
                        completion_tokens=response.usage.output_tokens,
                        total_tokens=response.usage.total_tokens
                    )
                )
            else:
                raise AdapterError(f"Qwen API error: {response.message}")
        
        except Exception as e:
            logger.error(f"Qwen chat error: {e}")
            raise AdapterError(f"Qwen API call failed: {str(e)}")
    
    async def chat_stream(self, request: ChatRequest) -> AsyncIterator[str]:
        """
        é€šä¹‰åƒé—®æµå¼èŠå¤©
        """
        try:
            messages = self._convert_messages(request.messages)
            
            # æµå¼è°ƒç”¨
            responses = Generation.call(
                model=request.model or "qwen-max",
                messages=messages,
                result_format='message',
                temperature=request.temperature or 1.0,
                max_tokens=request.max_tokens or 2000,
                stream=True,
                incremental_output=True
            )
            
            for response in responses:
                if response.status_code == HTTPStatus.OK:
                    content = response.output.choices[0].message.content
                    if content:
                        yield self._format_stream_chunk(
                            chunk_id=response.request_id,
                            content=content,
                            finish_reason=response.output.choices[0].finish_reason
                        )
                else:
                    raise AdapterError(f"Qwen stream error: {response.message}")
        
        except Exception as e:
            logger.error(f"Qwen stream error: {e}")
            raise AdapterError(f"Qwen stream failed: {str(e)}")
    
    async def embedding(self, request: EmbeddingRequest) -> EmbeddingResponse:
        """
        é€šä¹‰åƒé—® Embedding
        """
        try:
            # è°ƒç”¨é€šä¹‰åƒé—® Embedding API
            response = TextEmbedding.call(
                model="text-embedding-v2",
                input=request.input
            )
            
            if response.status_code == HTTPStatus.OK:
                return EmbeddingResponse(
                    model="text-embedding-v2",
                    data=[
                        EmbeddingData(
                            embedding=item.embedding,
                            index=idx
                        )
                        for idx, item in enumerate(response.output.embeddings)
                    ],
                    usage=Usage(
                        prompt_tokens=response.usage.total_tokens,
                        total_tokens=response.usage.total_tokens
                    )
                )
            else:
                raise AdapterError(f"Qwen embedding error: {response.message}")
        
        except Exception as e:
            logger.error(f"Qwen embedding error: {e}")
            raise AdapterError(f"Qwen embedding failed: {str(e)}")
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] Chat æ¥å£å®Œæˆ
- [ ] æµå¼æ¥å£å®Œæˆ
- [ ] Embedding æ¥å£å®Œæˆ
- [ ] é”™è¯¯å¤„ç†å®Œå–„

#### 3. ç™¾åº¦æ–‡å¿ƒ (ERNIE) é€‚é…å™¨
**å½“å‰çŠ¶æ€**: éƒ¨åˆ†å®ç°ï¼Œéœ€å®Œå–„

**ä½ç½®**: `algo/model-adapter/app/services/providers/baidu_adapter.py`

**å¾…å®ç°**:

```python
# æ–‡ä»¶: algo/model-adapter/app/services/providers/baidu_adapter.py

import requests
import json

class BaiduAdapter(BaseAdapter):
    """ç™¾åº¦æ–‡å¿ƒé€‚é…å™¨"""
    
    def __init__(self):
        self.api_key = os.getenv("BAIDU_API_KEY")
        self.secret_key = os.getenv("BAIDU_SECRET_KEY")
        
        if not self.api_key or not self.secret_key:
            raise ValueError("Baidu credentials not configured")
        
        self.access_token = None
        self.token_expires_at = 0
    
    async def _get_access_token(self) -> str:
        """è·å–è®¿é—®ä»¤ç‰Œ"""
        import time
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
        if self.access_token and time.time() < self.token_expires_at:
            return self.access_token
        
        # è·å–æ–°ä»¤ç‰Œ
        url = "https://aip.baidubce.com/oauth/2.0/token"
        params = {
            "grant_type": "client_credentials",
            "client_id": self.api_key,
            "client_secret": self.secret_key
        }
        
        response = requests.post(url, params=params)
        result = response.json()
        
        if "access_token" in result:
            self.access_token = result["access_token"]
            self.token_expires_at = time.time() + result.get("expires_in", 2592000) - 300
            return self.access_token
        else:
            raise AdapterError(f"Failed to get access token: {result}")
    
    async def chat(self, request: ChatRequest) -> ChatResponse:
        """
        ç™¾åº¦æ–‡å¿ƒèŠå¤©æ¥å£
        """
        try:
            access_token = await self._get_access_token()
            
            # é€‰æ‹©æ¨¡å‹ç«¯ç‚¹
            model_map = {
                "ernie-bot": "completions",
                "ernie-bot-4": "completions_pro",
                "ernie-bot-turbo": "eb-instant"
            }
            model_name = request.model or "ernie-bot"
            endpoint = model_map.get(model_name, "completions")
            
            url = f"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/{endpoint}"
            
            # æ„å»ºè¯·æ±‚
            messages = self._convert_messages(request.messages)
            
            payload = {
                "messages": messages,
                "temperature": request.temperature or 0.95,
                "top_p": request.top_p or 0.8,
                "max_output_tokens": request.max_tokens or 2048,
                "stream": False
            }
            
            # è°ƒç”¨ API
            response = requests.post(
                url,
                params={"access_token": access_token},
                json=payload,
                headers={"Content-Type": "application/json"}
            )
            
            result = response.json()
            
            if "error_code" in result:
                raise AdapterError(f"Baidu API error: {result}")
            
            return ChatResponse(
                id=result.get("id", ""),
                model=model_name,
                choices=[
                    Choice(
                        index=0,
                        message=Message(
                            role="assistant",
                            content=result["result"]
                        ),
                        finish_reason="stop"
                    )
                ],
                usage=Usage(
                    prompt_tokens=result["usage"]["prompt_tokens"],
                    completion_tokens=result["usage"]["completion_tokens"],
                    total_tokens=result["usage"]["total_tokens"]
                )
            )
        
        except Exception as e:
            logger.error(f"Baidu chat error: {e}")
            raise AdapterError(f"Baidu API call failed: {str(e)}")
    
    async def chat_stream(self, request: ChatRequest) -> AsyncIterator[str]:
        """
        ç™¾åº¦æ–‡å¿ƒæµå¼èŠå¤©
        """
        try:
            access_token = await self._get_access_token()
            
            model_name = request.model or "ernie-bot"
            endpoint = "completions"
            url = f"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/{endpoint}"
            
            messages = self._convert_messages(request.messages)
            
            payload = {
                "messages": messages,
                "temperature": request.temperature or 0.95,
                "max_output_tokens": request.max_tokens or 2048,
                "stream": True
            }
            
            # æµå¼è¯·æ±‚
            response = requests.post(
                url,
                params={"access_token": access_token},
                json=payload,
                headers={"Content-Type": "application/json"},
                stream=True
            )
            
            for line in response.iter_lines():
                if line:
                    line_text = line.decode('utf-8')
                    if line_text.startswith("data: "):
                        data = json.loads(line_text[6:])
                        
                        if "result" in data:
                            yield self._format_stream_chunk(
                                chunk_id=data.get("id", ""),
                                content=data["result"],
                                finish_reason=data.get("is_end", False) and "stop" or None
                            )
        
        except Exception as e:
            logger.error(f"Baidu stream error: {e}")
            raise AdapterError(f"Baidu stream failed: {str(e)}")
    
    async def embedding(self, request: EmbeddingRequest) -> EmbeddingResponse:
        """
        ç™¾åº¦æ–‡å¿ƒ Embedding
        """
        try:
            access_token = await self._get_access_token()
            
            url = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/embeddings/embedding-v1"
            
            # å¤„ç†è¾“å…¥
            texts = request.input if isinstance(request.input, list) else [request.input]
            
            payload = {"input": texts}
            
            response = requests.post(
                url,
                params={"access_token": access_token},
                json=payload,
                headers={"Content-Type": "application/json"}
            )
            
            result = response.json()
            
            if "error_code" in result:
                raise AdapterError(f"Baidu embedding error: {result}")
            
            return EmbeddingResponse(
                model="embedding-v1",
                data=[
                    EmbeddingData(
                        embedding=item["embedding"],
                        index=item["index"]
                    )
                    for item in result["data"]
                ],
                usage=Usage(
                    prompt_tokens=result["usage"]["prompt_tokens"],
                    total_tokens=result["usage"]["total_tokens"]
                )
            )
        
        except Exception as e:
            logger.error(f"Baidu embedding error: {e}")
            raise AdapterError(f"Baidu embedding failed: {str(e)}")
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] Chat æ¥å£å®Œæˆ
- [ ] æµå¼æ¥å£å®Œæˆ
- [ ] Embedding æ¥å£å®Œæˆ
- [ ] Token ç®¡ç†å®Œå–„

---

### ğŸ”„ P1 - é«˜çº§åŠŸèƒ½ï¼ˆè¿­ä»£2ï¼š1å‘¨ï¼‰

#### 1. æ¨¡å‹è·¯ç”±å¢å¼º

```python
# æ–‡ä»¶: algo/model-adapter/app/services/adapter_service.py

class AdapterService:
    """é€‚é…å™¨æœåŠ¡ - æ™ºèƒ½è·¯ç”±"""
    
    def __init__(self):
        self.adapters = {
            "openai": OpenAIAdapter(),
            "azure": AzureOpenAIAdapter(),
            "anthropic": AnthropicAdapter(),
            "zhipu": ZhipuAdapter(),
            "qwen": QwenAdapter(),
            "baidu": BaiduAdapter()
        }
        
        self.model_mapping = self._load_model_mapping()
    
    def _load_model_mapping(self) -> dict:
        """åŠ è½½æ¨¡å‹æ˜ å°„é…ç½®"""
        return {
            # OpenAI
            "gpt-4": "openai",
            "gpt-4-turbo": "openai",
            "gpt-3.5-turbo": "openai",
            
            # Anthropic
            "claude-3-opus": "anthropic",
            "claude-3-sonnet": "anthropic",
            
            # æ™ºè°±
            "glm-4": "zhipu",
            "glm-3-turbo": "zhipu",
            
            # åƒé—®
            "qwen-max": "qwen",
            "qwen-turbo": "qwen",
            
            # æ–‡å¿ƒ
            "ernie-bot-4": "baidu",
            "ernie-bot": "baidu"
        }
    
    async def route_chat(self, request: ChatRequest) -> ChatResponse:
        """
        æ™ºèƒ½è·¯ç”±èŠå¤©è¯·æ±‚
        """
        # 1. ç¡®å®šæä¾›å•†
        provider = self._determine_provider(request)
        
        # 2. è·å–é€‚é…å™¨
        adapter = self.adapters.get(provider)
        if not adapter:
            raise ValueError(f"Provider {provider} not supported")
        
        # 3. æ‰§è¡Œè¯·æ±‚ï¼ˆå¸¦é‡è¯•å’Œé™çº§ï¼‰
        try:
            return await self._execute_with_retry(
                adapter.chat,
                request
            )
        except Exception as e:
            # é™çº§åˆ°å¤‡ç”¨æ¨¡å‹
            if fallback := self._get_fallback_provider(provider):
                logger.warning(f"Fallback to {fallback} due to {e}")
                return await self.adapters[fallback].chat(request)
            raise
    
    def _determine_provider(self, request: ChatRequest) -> str:
        """ç¡®å®šæä¾›å•†"""
        # æ˜ç¡®æŒ‡å®š
        if request.provider:
            return request.provider
        
        # æ ¹æ®æ¨¡å‹åç§°
        if request.model in self.model_mapping:
            return self.model_mapping[request.model]
        
        # é»˜è®¤
        return "openai"
    
    async def _execute_with_retry(
        self,
        func,
        request,
        max_retries: int = 3
    ):
        """å¸¦é‡è¯•çš„æ‰§è¡Œ"""
        for attempt in range(max_retries):
            try:
                return await func(request)
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                
                wait_time = 2 ** attempt
                logger.warning(f"Retry {attempt + 1}/{max_retries} after {wait_time}s")
                await asyncio.sleep(wait_time)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ™ºèƒ½è·¯ç”±å®Œæˆ
- [ ] é‡è¯•æœºåˆ¶å®Œå–„
- [ ] é™çº§ç­–ç•¥å®ç°

#### 2. æ€§èƒ½ä¼˜åŒ–

```python
# æ–‡ä»¶: algo/model-adapter/app/services/cache_service.py

import hashlib
import redis
import json

class CacheService:
    """LLM å“åº”ç¼“å­˜æœåŠ¡"""
    
    def __init__(self):
        self.redis_client = redis.Redis(
            host=os.getenv("REDIS_HOST", "localhost"),
            port=int(os.getenv("REDIS_PORT", 6379)),
            db=1,
            decode_responses=True
        )
        self.ttl = 3600  # 1å°æ—¶
    
    def get_cache_key(self, request: ChatRequest) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # ä½¿ç”¨è¯·æ±‚å†…å®¹ç”Ÿæˆå”¯ä¸€é”®
        key_data = {
            "model": request.model,
            "messages": [
                {"role": m.role, "content": m.content}
                for m in request.messages
            ],
            "temperature": request.temperature,
            "max_tokens": request.max_tokens
        }
        
        key_str = json.dumps(key_data, sort_keys=True)
        hash_key = hashlib.md5(key_str.encode()).hexdigest()
        
        return f"llm_cache:{hash_key}"
    
    async def get_cached_response(
        self,
        request: ChatRequest
    ) -> Optional[ChatResponse]:
        """è·å–ç¼“å­˜çš„å“åº”"""
        cache_key = self.get_cache_key(request)
        
        cached = self.redis_client.get(cache_key)
        if cached:
            logger.info(f"Cache hit: {cache_key}")
            return ChatResponse.parse_raw(cached)
        
        return None
    
    async def cache_response(
        self,
        request: ChatRequest,
        response: ChatResponse
    ):
        """ç¼“å­˜å“åº”"""
        cache_key = self.get_cache_key(request)
        
        self.redis_client.setex(
            cache_key,
            self.ttl,
            response.json()
        )
        
        logger.info(f"Cached response: {cache_key}")
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] å“åº”ç¼“å­˜å®ç°
- [ ] ç¼“å­˜å‘½ä¸­ç‡ > 30%
- [ ] å»¶è¿Ÿé™ä½ 50%

#### 3. å¯è§‚æµ‹æ€§

```python
# æ–‡ä»¶: algo/model-adapter/app/middleware/metrics.py

from prometheus_client import Counter, Histogram, Gauge

# å®šä¹‰æŒ‡æ ‡
request_counter = Counter(
    'model_adapter_requests_total',
    'Total requests',
    ['provider', 'model', 'status']
)

request_duration = Histogram(
    'model_adapter_request_duration_seconds',
    'Request duration',
    ['provider', 'model']
)

token_usage = Counter(
    'model_adapter_tokens_total',
    'Token usage',
    ['provider', 'model', 'type']
)

active_requests = Gauge(
    'model_adapter_active_requests',
    'Active requests',
    ['provider']
)

@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    """æŒ‡æ ‡ä¸­é—´ä»¶"""
    start_time = time.time()
    
    # å¢åŠ æ´»è·ƒè¯·æ±‚
    provider = request.headers.get("X-Provider", "unknown")
    active_requests.labels(provider=provider).inc()
    
    try:
        response = await call_next(request)
        
        # è®°å½•æŒ‡æ ‡
        duration = time.time() - start_time
        request_duration.labels(
            provider=provider,
            model=request.headers.get("X-Model", "unknown")
        ).observe(duration)
        
        request_counter.labels(
            provider=provider,
            model=request.headers.get("X-Model", "unknown"),
            status="success"
        ).inc()
        
        return response
    
    except Exception as e:
        request_counter.labels(
            provider=provider,
            model=request.headers.get("X-Model", "unknown"),
            status="error"
        ).inc()
        raise
    
    finally:
        active_requests.labels(provider=provider).dec()
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] Prometheus æŒ‡æ ‡å®Œæˆ
- [ ] Grafana ä»ªè¡¨ç›˜
- [ ] å‘Šè­¦è§„åˆ™é…ç½®

---

## ä¸‰ã€è¯¦ç»†å®æ–½æ–¹æ¡ˆ

### é˜¶æ®µ1ï¼šå›½äº§æ¨¡å‹é€‚é…ï¼ˆWeek 1-2ï¼‰

#### Day 1-5: æ™ºè°± AI
1. å®ç° Chat æ¥å£
2. å®ç°æµå¼æ¥å£
3. å®ç° Embedding
4. æµ‹è¯•

#### Day 6-10: é€šä¹‰åƒé—®å’Œæ–‡å¿ƒ
1. å®ç°é€šä¹‰åƒé—®é€‚é…å™¨
2. å®ç°ç™¾åº¦æ–‡å¿ƒé€‚é…å™¨
3. é›†æˆæµ‹è¯•

#### Day 11-14: é›†æˆå’Œä¼˜åŒ–
1. æ¨¡å‹è·¯ç”±å¢å¼º
2. é”™è¯¯å¤„ç†
3. æ–‡æ¡£

### é˜¶æ®µ2ï¼šé«˜çº§åŠŸèƒ½ï¼ˆWeek 3ï¼‰

#### Day 15-17: ç¼“å­˜å’Œæ€§èƒ½
1. å®ç°å“åº”ç¼“å­˜
2. æ€§èƒ½ä¼˜åŒ–
3. æµ‹è¯•

#### Day 18-21: å¯è§‚æµ‹æ€§
1. Prometheus æŒ‡æ ‡
2. Grafana ä»ªè¡¨ç›˜
3. å‘Šè­¦é…ç½®

---

## å››ã€éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶
- [ ] æ‰€æœ‰é€‚é…å™¨å®Œæˆ
- [ ] æµå¼å“åº”æ­£å¸¸
- [ ] Embedding æ­£å¸¸
- [ ] ç¼“å­˜æ­£å¸¸å·¥ä½œ

### æ€§èƒ½éªŒæ”¶
- [ ] P95 å»¶è¿Ÿ < 2s
- [ ] ç¼“å­˜å‘½ä¸­ç‡ > 30%
- [ ] æ”¯æŒ 100 å¹¶å‘

### è´¨é‡éªŒæ”¶
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 70%
- [ ] æ‰€æœ‰ TODO æ¸…ç†
- [ ] API æ–‡æ¡£å®Œæ•´

---

## æ€»ç»“

Model Adapter çš„ä¸»è¦å¾…å®ŒæˆåŠŸèƒ½ï¼š

1. **æ™ºè°± AI é€‚é…å™¨**: å®Œæ•´å®ç° GLM-4
2. **é€šä¹‰åƒé—®é€‚é…å™¨**: å®Œæ•´å®ç° Qwen
3. **ç™¾åº¦æ–‡å¿ƒé€‚é…å™¨**: å®Œæ•´å®ç° ERNIE
4. **æ™ºèƒ½è·¯ç”±**: è‡ªåŠ¨é€‰æ‹©å’Œé™çº§
5. **æ€§èƒ½ä¼˜åŒ–**: ç¼“å­˜å’ŒæŒ‡æ ‡

å®Œæˆåå°†æ”¯æŒä¸»æµå›½äº§ LLMï¼Œæä¾›ç»Ÿä¸€æ¥å£ã€‚


